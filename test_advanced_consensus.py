#!/usr/bin/env python3
"""
Test script nÃ¢ng cao cho Enhanced Consensus Reranker
Kiá»ƒm tra trÆ°á»ng há»£p "scattered chunks" (3 chunks tá»« 3 documents khÃ¡c nhau)
"""

import sys
from pathlib import Path

# Add backend to Python path
backend_path = Path(__file__).parent / "backend"
sys.path.insert(0, str(backend_path))

from backend.app.services.result_reranker import RerankerService
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_scattered_test_documents():
    """Táº¡o test documents vá»›i 3 chunks tá»« 3 documents khÃ¡c nhau (scattered scenario)"""
    test_docs = [
        # Document A - 1 chunk
        {
            "id": "chunk_a1",
            "content": "Thá»§ tá»¥c Ä‘Äƒng kÃ½ káº¿t hÃ´n cáº§n cÃ³ giáº¥y tá»: CMND, há»™ kháº©u, giáº¥y xÃ¡c nháº­n Ä‘á»™c thÃ¢n.",
            "source": {"file_path": "document_A.json", "document_title": "HÆ°á»›ng dáº«n káº¿t hÃ´n"},
            "metadata": {"source": {"file_path": "document_A.json"}},
            "similarity": 0.85
        },
        # Document B - 1 chunk
        {
            "id": "chunk_b1",
            "content": "Quy trÃ¬nh ly hÃ´n theo quy Ä‘á»‹nh phÃ¡p luáº­t hiá»‡n hÃ nh cáº§n tuÃ¢n thá»§ nhiá»u bÆ°á»›c.",
            "source": {"file_path": "document_B.json", "document_title": "HÆ°á»›ng dáº«n ly hÃ´n"},
            "metadata": {"source": {"file_path": "document_B.json"}},
            "similarity": 0.82
        },
        # Document C - 1 chunk
        {
            "id": "chunk_c1",
            "content": "Thá»§ tá»¥c nuÃ´i con nuÃ´i yÃªu cáº§u há»“ sÆ¡ Ä‘áº§y Ä‘á»§ vÃ  kiá»ƒm tra Ä‘iá»u kiá»‡n gia Ä‘Ã¬nh.",
            "source": {"file_path": "document_C.json", "document_title": "HÆ°á»›ng dáº«n nuÃ´i con nuÃ´i"},
            "metadata": {"source": {"file_path": "document_C.json"}},
            "similarity": 0.80
        },
        # Document D - 1 chunk (lower score to test filtering)
        {
            "id": "chunk_d1",
            "content": "CÃ¡c loáº¡i hÃ¬nh doanh nghiá»‡p vÃ  cÃ¡ch thÃ nh láº­p cÃ´ng ty.",
            "source": {"file_path": "document_D.json", "document_title": "HÆ°á»›ng dáº«n thÃ nh láº­p doanh nghiá»‡p"},
            "metadata": {"source": {"file_path": "document_D.json"}},
            "similarity": 0.70
        },
        # Document E - 1 chunk (lower score)
        {
            "id": "chunk_e1",
            "content": "Quy Ä‘á»‹nh vá» thuáº¿ thu nháº­p cÃ¡ nhÃ¢n vÃ  cÃ¡c khoáº£n miá»…n giáº£m.",
            "source": {"file_path": "document_E.json", "document_title": "HÆ°á»›ng dáº«n thuáº¿ thu nháº­p"},
            "metadata": {"source": {"file_path": "document_E.json"}},
            "similarity": 0.68
        }
    ]
    
    return test_docs

def create_consensus_test_documents():
    """Táº¡o test documents vá»›i consensus rÃµ rÃ ng (3 chunks tá»« cÃ¹ng 1 document)"""
    test_docs = [
        # Document A - 3 chunks (should win consensus)
        {
            "id": "chunk_a1",
            "content": "Thá»§ tá»¥c Ä‘Äƒng kÃ½ káº¿t hÃ´n cáº§n cÃ³ giáº¥y tá»: CMND, há»™ kháº©u, giáº¥y xÃ¡c nháº­n Ä‘á»™c thÃ¢n.",
            "source": {"file_path": "document_A.json", "document_title": "HÆ°á»›ng dáº«n káº¿t hÃ´n"},
            "metadata": {"source": {"file_path": "document_A.json"}},
            "similarity": 0.85
        },
        {
            "id": "chunk_a2", 
            "content": "Lá»‡ phÃ­ Ä‘Äƒng kÃ½ káº¿t hÃ´n lÃ  20,000 VNÄ. Thá»i gian xá»­ lÃ½ lÃ  5 ngÃ y lÃ m viá»‡c.",
            "source": {"file_path": "document_A.json", "document_title": "HÆ°á»›ng dáº«n káº¿t hÃ´n"},
            "metadata": {"source": {"file_path": "document_A.json"}},
            "similarity": 0.83
        },
        {
            "id": "chunk_a3",
            "content": "Äá»‹a Ä‘iá»ƒm ná»™p há»“ sÆ¡: UBND phÆ°á»ng/xÃ£ nÆ¡i cÆ° trÃº cá»§a má»™t trong hai bÃªn.",
            "source": {"file_path": "document_A.json", "document_title": "HÆ°á»›ng dáº«n káº¿t hÃ´n"},
            "metadata": {"source": {"file_path": "document_A.json"}},
            "similarity": 0.81
        },
        # Document B - 2 chunks (should lose consensus)
        {
            "id": "chunk_b1",
            "content": "Quy trÃ¬nh ly hÃ´n theo quy Ä‘á»‹nh phÃ¡p luáº­t hiá»‡n hÃ nh cáº§n tuÃ¢n thá»§ nhiá»u bÆ°á»›c.",
            "source": {"file_path": "document_B.json", "document_title": "HÆ°á»›ng dáº«n ly hÃ´n"},
            "metadata": {"source": {"file_path": "document_B.json"}},
            "similarity": 0.82
        },
        {
            "id": "chunk_b2",
            "content": "Thá»§ tá»¥c ly hÃ´n Ä‘Æ¡n phÆ°Æ¡ng yÃªu cáº§u cÃ³ Ä‘Æ¡n khá»Ÿi kiá»‡n vÃ  báº±ng chá»©ng.",
            "source": {"file_path": "document_B.json", "document_title": "HÆ°á»›ng dáº«n ly hÃ´n"},
            "metadata": {"source": {"file_path": "document_B.json"}},
            "similarity": 0.80
        }
    ]
    
    return test_docs

def test_scattered_chunks_scenario():
    """Test trÆ°á»ng há»£p scattered chunks (3 chunks tá»« 3 documents khÃ¡c nhau)"""
    logger.info("ğŸ§ª TESTING Scattered Chunks Scenario")
    
    reranker = RerankerService()
    test_docs = create_scattered_test_documents()
    query = "thá»§ tá»¥c phÃ¡p luáº­t"
    
    logger.info(f"ğŸ“ Test query: '{query}'")
    logger.info(f"ğŸ“Š Test documents: {len(test_docs)} chunks from {len(test_docs)} different documents")
    
    # Test consensus method vá»›i scattered chunks
    logger.info("\nğŸ¯ Testing consensus method on scattered chunks:")
    try:
        consensus_doc = reranker.get_consensus_document(
            query=query,
            documents=test_docs,
            top_k=5,
            consensus_threshold=0.6,  # 3/5 = 60%
            min_rerank_score=-0.5  # Low threshold for legal documents
        )
        
        if consensus_doc:
            doc_source = consensus_doc.get('source', {}).get('file_path', 'unknown')
            logger.info(f"âœ… Consensus method selected: {doc_source}")
            logger.info(f"   Rerank score: {consensus_doc.get('rerank_score', 'N/A')}")
        else:
            logger.warning("âŒ Consensus method returned None")
            
    except Exception as e:
        logger.error(f"âŒ Consensus method failed: {e}")
    
    return True

def test_consensus_vs_scattered():
    """So sÃ¡nh káº¿t quáº£ giá»¯a consensus scenario vÃ  scattered scenario"""
    logger.info("\nğŸ”¬ COMPARATIVE TEST: Consensus vs Scattered")
    
    reranker = RerankerService()
    query = "thá»§ tá»¥c Ä‘Äƒng kÃ½ káº¿t hÃ´n"
    
    # Test 1: Consensus scenario
    logger.info("\nğŸ“Š SCENARIO 1: Strong Consensus (3 chunks from same document)")
    consensus_docs = create_consensus_test_documents()
    
    consensus_result = reranker.get_consensus_document(
        query=query,
        documents=consensus_docs,
        top_k=5,
        consensus_threshold=0.6,
        min_rerank_score=-0.5
    )
    
    if consensus_result:
        consensus_source = consensus_result.get('source', {}).get('file_path', 'unknown')
        logger.info(f"âœ… Consensus scenario selected: {consensus_source}")
    
    # Test 2: Scattered scenario  
    logger.info("\nğŸ“Š SCENARIO 2: Scattered Chunks (chunks from different documents)")
    scattered_docs = create_scattered_test_documents()
    
    scattered_result = reranker.get_consensus_document(
        query=query,
        documents=scattered_docs,
        top_k=5,
        consensus_threshold=0.6,
        min_rerank_score=-0.5
    )
    
    if scattered_result:
        scattered_source = scattered_result.get('source', {}).get('file_path', 'unknown')
        logger.info(f"âœ… Scattered scenario selected: {scattered_source}")
    
    return True

def test_threshold_sensitivity():
    """Test Ä‘á»™ nháº¡y cá»§a consensus threshold"""
    logger.info("\nğŸ”§ TESTING Threshold Sensitivity")
    
    reranker = RerankerService()
    test_docs = create_consensus_test_documents()
    query = "thá»§ tá»¥c Ä‘Äƒng kÃ½ káº¿t hÃ´n"
    
    thresholds = [0.4, 0.5, 0.6, 0.7, 0.8]
    
    for threshold in thresholds:
        try:
            result = reranker.get_consensus_document(
                query=query,
                documents=test_docs,
                top_k=5,
                consensus_threshold=threshold,
                min_rerank_score=-0.5
            )
            
            if result:
                doc_source = result.get('source', {}).get('file_path', 'unknown')
                logger.info(f"âœ… Threshold {threshold}: Selected {doc_source}")
            else:
                logger.info(f"âŒ Threshold {threshold}: No consensus found")
                
        except Exception as e:
            logger.error(f"âŒ Threshold {threshold} failed: {e}")
    
    return True

if __name__ == "__main__":
    logger.info("ğŸš€ Starting Advanced Consensus Reranker Tests")
    
    # Test scattered chunks scenario
    test_scattered_chunks_scenario()
    
    # Test consensus vs scattered comparison
    test_consensus_vs_scattered()
    
    # Test threshold sensitivity
    test_threshold_sensitivity()
    
    logger.info("\nâœ… All advanced tests completed!")
