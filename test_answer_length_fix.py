#!/usr/bin/env python3
"""
TEST MINIMUM RESPONSE TOKENS & ANSWER LENGTH FIX
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

from app.services.language_model import LLMService
from app.core.config import settings

def test_minimum_response_tokens():
    """Test ng∆∞·ª°ng t·ªëi thi·ªÉu ƒë·ªÉ sinh response"""
    
    print("üîß TESTING MINIMUM RESPONSE TOKENS")
    print("=" * 60)
    
    llm_service = LLMService()
    
    # Test case 1: Context l·ªõn nh∆∞ng v·∫´n ƒë·ªß space
    print("üìù Test 1: Adequate space for response")
    print("-" * 50)
    
    moderate_context = "Th√¥ng tin t√†i li·ªáu: " + "N·ªôi dung quan tr·ªçng. " * 100  # ~2200 chars
    
    formatted_prompt = llm_service._format_prompt(
        system_prompt="B·∫°n l√† tr·ª£ l√Ω AI ph√°p lu·∫≠t.",
        user_query="L·ªá ph√≠ ƒëƒÉng k√Ω k·∫øt h√¥n l√† bao nhi√™u?",
        context=moderate_context,
        chat_history=None
    )
    
    # Simulate calculation
    prompt_tokens = len(formatted_prompt) // 3
    total_window = settings.n_ctx
    safety_buffer = 256
    available_space = total_window - prompt_tokens - safety_buffer
    MINIMUM_RESPONSE_TOKENS = 64
    
    print(f"   Prompt tokens: ~{prompt_tokens}")
    print(f"   Available space: {available_space}")
    print(f"   Minimum required: {MINIMUM_RESPONSE_TOKENS}")
    print(f"   Status: {'‚úÖ ADEQUATE' if available_space > MINIMUM_RESPONSE_TOKENS else '‚ùå INSUFFICIENT'}")
    
    if available_space > MINIMUM_RESPONSE_TOKENS:
        max_tokens_used = min(settings.max_tokens, available_space)
        print(f"   Max tokens would be: {max_tokens_used}")
    
    # Test case 2: Context c·ª±c l·ªõn - kh√¥ng ƒë·ªß space
    print("\nüìù Test 2: Insufficient space (should return default message)")
    print("-" * 50)
    
    # T·∫°o prompt c·ª±c l·ªõn ƒë·ªÉ simulate insufficient space
    huge_context = "T√†i li·ªáu c·ª±c l·ªõn: " + "N·ªôi dung r·∫•t d√†i. " * 800  # ~14,400 chars
    huge_history = [
        {"role": "user", "content": "C√¢u h·ªèi " + "x" * 300},
        {"role": "assistant", "content": "Tr·∫£ l·ªùi " + "y" * 300}
    ] * 5  # 10 messages l·ªõn
    
    huge_prompt = llm_service._format_prompt(
        system_prompt="B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam v·ªõi quy t·∫Øc chi ti·∫øt...",
        user_query="C√¢u h·ªèi ph·ª©c t·∫°p v·ªÅ th·ªß t·ª•c?",
        context=huge_context,
        chat_history=huge_history
    )
    
    huge_prompt_tokens = len(huge_prompt) // 3
    huge_available_space = total_window - huge_prompt_tokens - safety_buffer
    
    print(f"   Huge prompt tokens: ~{huge_prompt_tokens}")
    print(f"   Available space: {huge_available_space}")
    print(f"   Minimum required: {MINIMUM_RESPONSE_TOKENS}")
    print(f"   Status: {'‚úÖ ADEQUATE' if huge_available_space > MINIMUM_RESPONSE_TOKENS else '‚ùå INSUFFICIENT'}")
    
    if huge_available_space <= MINIMUM_RESPONSE_TOKENS:
        print(f"   üîß Would return default message instead of crashing")
        print(f"   üìã Message: 'Xin l·ªói, ng·ªØ c·∫£nh qu√° ph·ª©c t·∫°p...'")

def test_stop_tokens_optimization():
    """Test vi·ªác ƒë∆°n gi·∫£n h√≥a stop tokens"""
    
    print("\nüõë TESTING STOP TOKENS OPTIMIZATION")
    print("=" * 60)
    
    print("üìù Stop tokens analysis:")
    print("-" * 50)
    
    old_stop_tokens = ["<|im_end|>", "<|im_start|>", "\n<|im_start|>"]
    new_stop_tokens = ["<|im_end|>"]
    
    print(f"   Old stop tokens: {old_stop_tokens}")
    print(f"   New stop tokens: {new_stop_tokens}")
    print(f"   Reduction: {len(old_stop_tokens)} ‚Üí {len(new_stop_tokens)} tokens")
    
    print(f"\nüìã Benefits:")
    print(f"   ‚úÖ Reduced chance of premature stopping")
    print(f"   ‚úÖ Less restrictive generation")
    print(f"   ‚úÖ Focus on main ChatML boundary")
    print(f"   ‚úÖ Better response completion")

def test_dynamic_adjustment_edge_cases():
    """Test c√°c edge cases c·ªßa dynamic adjustment"""
    
    print("\n‚öñÔ∏è TESTING DYNAMIC ADJUSTMENT EDGE CASES")
    print("=" * 60)
    
    test_cases = [
        {
            "name": "Very Small Available Space",
            "available_space": 30,
            "requested_max": 1200
        },
        {
            "name": "Exactly Minimum Space", 
            "available_space": 64,
            "requested_max": 1200
        },
        {
            "name": "Just Above Minimum",
            "available_space": 80,
            "requested_max": 1200
        },
        {
            "name": "Normal Case",
            "available_space": 2000,
            "requested_max": 1200
        }
    ]
    
    MINIMUM_RESPONSE_TOKENS = 64
    
    for case in test_cases:
        print(f"\nüìù {case['name']}:")
        print(f"   Available space: {case['available_space']}")
        print(f"   Requested max: {case['requested_max']}")
        
        if case['available_space'] <= MINIMUM_RESPONSE_TOKENS:
            print(f"   üö® Result: Return default message")
        else:
            dynamic_max = min(case['requested_max'], case['available_space'])
            if dynamic_max < MINIMUM_RESPONSE_TOKENS:
                dynamic_max = MINIMUM_RESPONSE_TOKENS
            print(f"   ‚úÖ Dynamic max tokens: {dynamic_max}")
            
            if dynamic_max != case['requested_max']:
                print(f"   ‚ö†Ô∏è Adjusted: {case['requested_max']} ‚Üí {dynamic_max}")
            else:
                print(f"   ‚úÖ No adjustment needed")

def main():
    """Ch·∫°y t·∫•t c·∫£ tests"""
    
    print("üöÄ TESTING ANSWER LENGTH FIX & MINIMUM TOKENS")
    print("=" * 70)
    print("M·ª•c ti√™u: Kh·∫Øc ph·ª•c Answer length: 0 issue")
    print("=" * 70)
    
    tests = [
        test_minimum_response_tokens,
        test_stop_tokens_optimization,
        test_dynamic_adjustment_edge_cases
    ]
    
    for i, test in enumerate(tests, 1):
        try:
            test()
            print(f"\n‚úÖ Test {i} completed")
        except Exception as e:
            print(f"\n‚ùå Test {i} failed: {e}")
    
    print("\n" + "=" * 70)
    print("üéØ OPTIMIZATION SUMMARY")
    print("=" * 70)
    print("‚úÖ Minimum response tokens: 64 (prevents empty answers)")
    print("‚úÖ Stop tokens simplified: Reduced premature stopping")
    print("‚úÖ Dynamic adjustment improved: Better edge case handling")
    print("‚úÖ Default message fallback: Graceful degradation")
    print("‚úÖ Context overflow protection: Enhanced")
    
    print(f"\nüéâ ANSWER LENGTH: 0 ISSUE SHOULD BE RESOLVED!")

if __name__ == "__main__":
    main()
