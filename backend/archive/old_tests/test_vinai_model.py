#!/usr/bin/env python3
"""
Test VinAI PhoGPT-4B-Chat Model vs Old Model
Compare performance and response quality improvements
"""

import asyncio
import json
import time
from app.services.rag_service import RAGService
from app.services.vectordb_service import VectorDBService
from app.services.llm_service import LLMService
from app.core.config import settings

async def test_vinai_model():
    """Test RAG system v·ªõi VinAI model ch√≠nh th·ª©c"""
    
    print("="*80)
    print("üÜö TESTING VINAI PHOGPT-4B-CHAT MODEL")
    print("="*80)
    print(f"üè¢ Model: VinAI Research (Official)")
    print(f"üì¶ File: {settings.llm_model_path.split('/')[-1]}")
    print(f"üîß Config:")
    print(f"   ‚Ä¢ GPU Layers: {settings.n_gpu_layers}")
    print(f"   ‚Ä¢ CPU Threads: {settings.n_threads}")
    print(f"   ‚Ä¢ Batch Size: {settings.n_batch}")
    print(f"   ‚Ä¢ Max Tokens: {settings.max_tokens}")
    print(f"   ‚Ä¢ Temperature: {settings.temperature}")
    print()
    
    try:
        # Kh·ªüi t·∫°o services v·ªõi timing
        print("üîß Initializing services...")
        init_start = time.time()
        
        vectordb_service = VectorDBService()
        print(f"   ‚úÖ VectorDB loaded ({time.time() - init_start:.2f}s)")
        
        llm_start = time.time()
        llm_service = LLMService()
        llm_init_time = time.time() - llm_start
        print(f"   ‚úÖ LLM loaded ({llm_init_time:.2f}s)")
        
        if llm_init_time > 30:
            print("   ‚ö†Ô∏è  LLM loading took a while - may need optimization")
        elif llm_init_time < 10:
            print("   üöÄ Fast LLM loading - GPU acceleration likely working")
        
        print()
        
        rag_service = RAGService(
            documents_dir=str(settings.documents_path),
            vectordb_service=vectordb_service,
            llm_service=llm_service
        )
        
        # Health check v·ªõi GPU detection
        health = rag_service.get_health_status()
        model_info = health.get('model_info', {})
        
        print(f"üè• System Health: {health['status']}")
        print(f"   ‚Ä¢ Collections: {health['total_collections']}")
        print(f"   ‚Ä¢ Documents: {health['total_documents']}")
        print(f"   ‚Ä¢ Model Size: {model_info.get('model_size_mb', 0):.0f} MB")
        print(f"   ‚Ä¢ GPU Layers: {model_info.get('model_kwargs', {}).get('n_gpu_layers', 'N/A')}")
        print()
        
        # Test queries focusing on previous issues
        test_cases = [
            {
                "query": "th·ªß t·ª•c nu√¥i con nu√¥i",
                "description": "Query g√¢y l·∫∑p response tr∆∞·ªõc ƒë√¢y",
                "expected_keywords": ["ƒëƒÉng k√Ω", "h·ªì s∆°", "th·ªß t·ª•c"]
            },
            {
                "query": "c√°ch ƒëƒÉng k√Ω khai sinh cho tr·∫ª em",
                "description": "Query ph·ª©c t·∫°p v·ªÅ th·ªß t·ª•c ph√°p l√Ω",
                "expected_keywords": ["gi·∫•y khai sinh", "UBND", "h·ªì s∆°"]
            },
            {
                "query": "quy tr√¨nh ch·ª©ng th·ª±c b·∫£n sao gi·∫•y t·ªù",
                "description": "Query v·ªÅ d·ªãch v·ª• c√¥ng",
                "expected_keywords": ["ch·ª©ng th·ª±c", "b·∫£n sao", "c√¥ng ch·ª©ng"]
            }
        ]
        
        results = []
        
        for i, test_case in enumerate(test_cases, 1):
            query = test_case["query"]
            description = test_case["description"]
            expected_keywords = test_case["expected_keywords"]
            
            print(f"üîç Test {i}/{len(test_cases)}: {description}")
            print(f"‚ùì Query: '{query}'")
            print("-" * 60)
            
            # Test v·ªõi parameters t·ªëi ∆∞u
            start_time = time.time()
            result = rag_service.query(
                question=query,
                max_tokens=300,    # Moderate length
                temperature=0.3    # Balance between accuracy and creativity
            )
            total_time = time.time() - start_time
            
            # Analyze response quality
            answer = result.get('answer', '')
            sources_count = len(result.get('sources', []))
            tokens_used = result.get('tokens_used', 0)
            llm_time = result.get('llm_processing_time', 0)
            
            # Quality metrics
            answer_length = len(answer)
            line_count = len([line for line in answer.split('\n') if line.strip()])
            
            # Check for repetition issues
            lines = answer.split('\n')
            unique_lines = set(line.strip() for line in lines if line.strip())
            repetition_ratio = (len(lines) - len(unique_lines)) / max(len(lines), 1)
            
            # Check keyword coverage
            answer_lower = answer.lower()
            keywords_found = [kw for kw in expected_keywords if kw.lower() in answer_lower]
            keyword_coverage = len(keywords_found) / len(expected_keywords)
            
            print(f"   ‚ö° Performance:")
            print(f"      ‚Ä¢ Total Time: {total_time:.2f}s")
            print(f"      ‚Ä¢ LLM Time: {llm_time:.2f}s")
            print(f"      ‚Ä¢ Tokens: {tokens_used}")
            print(f"      ‚Ä¢ Sources: {sources_count}")
            
            print(f"   üìä Quality Metrics:")
            print(f"      ‚Ä¢ Answer Length: {answer_length} chars")
            print(f"      ‚Ä¢ Lines: {line_count}")
            print(f"      ‚Ä¢ Repetition: {repetition_ratio:.1%}")
            print(f"      ‚Ä¢ Keyword Coverage: {keyword_coverage:.1%} ({len(keywords_found)}/{len(expected_keywords)})")
            
            # Quality assessment
            quality_score = 0
            if repetition_ratio < 0.2:  # Low repetition
                quality_score += 25
            if keyword_coverage >= 0.5:  # Good keyword coverage
                quality_score += 25
            if 100 <= answer_length <= 1000:  # Reasonable length
                quality_score += 25
            if llm_time < 5:  # Fast response
                quality_score += 25
                
            print(f"   üèÜ Quality Score: {quality_score}/100")
            
            print(f"   üí¨ Answer Preview:")
            preview = answer[:400].replace('\n', ' ')
            print(f"      {preview}{'...' if len(answer) > 400 else ''}")
            print()
            
            results.append({
                'query': query,
                'description': description,
                'total_time': total_time,
                'llm_time': llm_time,
                'tokens': tokens_used,
                'sources': sources_count,
                'answer_length': answer_length,
                'repetition_ratio': repetition_ratio,
                'keyword_coverage': keyword_coverage,
                'quality_score': quality_score,
                'answer': answer
            })
        
        # Final analysis
        print("="*80)
        print("üìä VINAI MODEL PERFORMANCE SUMMARY")
        print("="*80)
        
        avg_total_time = sum(r['total_time'] for r in results) / len(results)
        avg_llm_time = sum(r['llm_time'] for r in results) / len(results)
        avg_quality = sum(r['quality_score'] for r in results) / len(results)
        avg_repetition = sum(r['repetition_ratio'] for r in results) / len(results)
        avg_coverage = sum(r['keyword_coverage'] for r in results) / len(results)
        
        print(f"‚ö° Performance Metrics:")
        print(f"   ‚Ä¢ Average Total Time: {avg_total_time:.2f}s")
        print(f"   ‚Ä¢ Average LLM Time: {avg_llm_time:.2f}s")
        print(f"   ‚Ä¢ Processing Efficiency: {((avg_total_time - avg_llm_time) / avg_total_time)*100:.0f}% RAG / {(avg_llm_time/avg_total_time)*100:.0f}% LLM")
        
        print(f"\nüèÜ Quality Assessment:")
        print(f"   ‚Ä¢ Average Quality Score: {avg_quality:.1f}/100")
        print(f"   ‚Ä¢ Average Repetition: {avg_repetition:.1%}")
        print(f"   ‚Ä¢ Average Keyword Coverage: {avg_coverage:.1%}")
        
        # Overall verdict
        print(f"\nüéØ Overall Assessment:")
        
        if avg_quality >= 80:
            print("   ‚úÖ EXCELLENT - VinAI model working very well")
        elif avg_quality >= 60:
            print("   ‚úÖ GOOD - VinAI model is a significant improvement")
        elif avg_quality >= 40:
            print("   ‚ö†Ô∏è  FAIR - Some improvements but may need tuning")
        else:
            print("   ‚ùå POOR - May need further investigation")
            
        if avg_repetition < 0.1:
            print("   ‚úÖ Repetition issue RESOLVED")
        else:
            print("   ‚ö†Ô∏è  Still some repetition issues")
            
        if avg_llm_time < 3:
            print("   ‚úÖ GPU acceleration working well")
        elif avg_llm_time < 10:
            print("   ‚úÖ Good performance, GPU likely helping")
        else:
            print("   ‚ö†Ô∏è  Performance could be better - check GPU usage")
        
        print("\nüöÄ VinAI Model Integration: SUCCESS!")
        
        return results
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(test_vinai_model())
