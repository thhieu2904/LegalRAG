#!/usr/bin/env python3
"""
Enhanced Smart Router Generator with LLM - Version 2
=======================================================

Sinh tá»± Ä‘á»™ng cÃ¡c cÃ¢u há»i router examples báº±ng LLM thay vÃ¬ template cá»‘ Ä‘á»‹nh.
Tá»‘i Æ°u hÃ³a Ä‘á»ƒ sinh ra 10+ cÃ¢u há»i Ä‘a dáº¡ng tá»« metadata phong phÃº.

Sá»­a Ä‘á»•i: Tráº£ vá» dictionary thay vÃ¬ JSON string Ä‘á»ƒ trÃ¡nh lá»—i parsing.
"""

import json
import re
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import sys
import argparse

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Add app to Python path
sys.path.append(str(Path(__file__).parent.parent))

try:
    from app.services.language_model import LLMService
except ImportError as e:
    logger.error(f"âŒ Cannot import LLMService: {e}")
    logger.error("   Make sure you're in the backend directory and the service is available")
    sys.exit(1)

class SmartRouterLLMGenerator:
    """Enhanced router generator using LLM for question generation."""
    
    def __init__(self, documents_dir: str = None, output_dir: str = None):
        self.documents_dir = Path(documents_dir or "data/documents")
        self.output_dir = Path(output_dir or "data/router_examples_smart")
        
        # Initialize LLM service
        try:
            self.llm_service = LLMService()
            logger.info(f"âœ… LLM Service initialized successfully")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize LLM Service: {e}")
            raise

    def _detect_collection_from_path(self, file_path: str) -> str:
        """Detect collection name from file path."""
        path_lower = file_path.lower()
        
        if 'hanh_chinh' in path_lower:
            return 'administrative_procedures'
        elif 'kinh_doanh' in path_lower:
            return 'business_procedures'
        elif 'dat_dai' in path_lower:
            return 'land_procedures'
        elif 'xay_dung' in path_lower:
            return 'construction_procedures'
        elif 'tu_phap' in path_lower:
            return 'judicial_procedures'
        else:
            return 'general_procedures'

    def analyze_document_metadata(self, doc: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze document metadata for smart filtering."""
        metadata = doc.get('metadata', {})
        smart_filters = {}
        
        # Fee analysis
        fee_text = metadata.get('fee_text', '') or metadata.get('fee', '')
        if fee_text:
            fee_lower = fee_text.lower()
            if 'miá»…n phÃ­' in fee_lower or 'khÃ´ng phÃ­' in fee_lower:
                smart_filters['has_fee'] = False
            else:
                smart_filters['has_fee'] = True
        
        # Processing time
        processing_time = metadata.get('processing_time_text', '') or metadata.get('processing_time', '')
        if processing_time:
            time_normalized = processing_time.lower()
            if 'ngay' in time_normalized:
                smart_filters['processing_speed'] = 'fast'
            elif 'tuáº§n' in time_normalized or 'thÃ¡ng' in time_normalized:
                smart_filters['processing_speed'] = 'slow'
        
        return {
            'metadata': metadata,
            'smart_filters': smart_filters,
            'confidence_threshold': 0.75,
            'priority_score': 1.0
        }

    def generate_questions_with_llm(self, doc: Dict[str, Any]) -> Dict[str, Any]:
        """Sá»­ dá»¥ng LLM Ä‘á»ƒ sinh ra cÃ¢u há»i chÃ­nh vÃ  cÃ¡c biáº¿n thá»ƒ."""
        metadata = doc.get('metadata', {})
        document_title = metadata.get('title', 'Thá»§ tá»¥c chÆ°a xÃ¡c Ä‘á»‹nh')
        
        # Táº¡o má»™t chuá»—i tÃ³m táº¯t ná»™i dung tÃ i liá»‡u Ä‘á»ƒ Ä‘Æ°a vÃ o prompt
        document_content_summary = self._summarize_document_for_prompt(doc)

        # Prompt tá»‘i Æ°u hÃ³a - ngáº¯n gá»n vÃ  hiá»‡u quáº£
        user_query = f"""NHIá»†M Vá»¤: Táº¡o chÃ­nh xÃ¡c 10 cÃ¢u há»i vá» thá»§ tá»¥c "{document_title}"

THÃ”NG TIN: {document_content_summary}

YÃŠU Cáº¦U: Táº¡o 10 cÃ¢u há»i ngáº¯n gá»n, má»—i cÃ¢u má»™t dÃ²ng, Ä‘Ã¡nh sá»‘ tá»« 1-10. Má»—i cÃ¢u pháº£i khÃ¡c nhau hoÃ n toÃ n.

VÃ Dá»¤ FORMAT:
1. Thá»§ tá»¥c nÃ y lÃ  gÃ¬?
2. Ai cÃ³ thá»ƒ lÃ m?
3. Cáº§n giáº¥y tá» gÃ¬?
4. Chi phÃ­ bao nhiÃªu?
5. LÃ m á»Ÿ Ä‘Ã¢u?
6. Máº¥t bao lÃ¢u?
7. LÃ m online Ä‘Æ°á»£c khÃ´ng?
8. Nháº­n káº¿t quáº£ nhÆ° tháº¿ nÃ o?
9. CÃ³ Ä‘iá»u kiá»‡n gÃ¬ Ä‘áº·c biá»‡t?
10. LÆ°u Ã½ gÃ¬ khi lÃ m?

Báº®T Äáº¦U Táº O 10 CÃ‚U Há»I:"""

        # System prompt ngáº¯n gá»n vÃ  chá»‰ thá»‹ rÃµ rÃ ng
        system_prompt = "Táº¡o chÃ­nh xÃ¡c 10 cÃ¢u há»i ngáº¯n. ÄÃ¡nh sá»‘ 1-10. KhÃ´ng giáº£i thÃ­ch thÃªm."
        
        # ThÃªm retry logic Ä‘á»ƒ xá»­ lÃ½ LLM khÃ´ng á»•n Ä‘á»‹nh
        max_retries = 2
        for attempt in range(max_retries):
            try:
                logger.info(f"   ğŸ¤– Calling LLM (Attempt {attempt + 1}/{max_retries}) for: '{document_title}'")
                response_data = self.llm_service.generate_response(
                    user_query=user_query,
                    max_tokens=500,  # Giáº£m Ä‘á»ƒ trÃ¡nh repetition
                    temperature=0.3,  # TÄƒng má»™t chÃºt Ä‘á»ƒ cÃ³ Ä‘a dáº¡ng
                    system_prompt=system_prompt  # Sá»­ dá»¥ng system prompt riÃªng
                )
                
                # TrÃ­ch xuáº¥t vÃ  lÃ m sáº¡ch tá»« pháº£n há»“i cá»§a LLM
                response_text = response_data.get('response', '')
                logger.info(f"   ğŸ“ LLM Response (first 200 chars): {response_text[:200]}...")
                
                generated_data = self._extract_questions_from_llm_response(response_text, document_title)
                
                if generated_data:
                    # Clean up questions
                    generated_data['main_question'] = self._clean_question(generated_data['main_question'])
                    generated_data['question_variants'] = [
                        self._clean_question(q) for q in generated_data['question_variants'] if q.strip()
                    ]
                    
                    logger.info(f"   âœ… LLM generated {1 + len(generated_data['question_variants'])} questions.")
                    return generated_data  # ThÃ nh cÃ´ng - thoÃ¡t khá»i retry loop
                else:
                    raise ValueError("No valid questions found in LLM response")

            except Exception as e:
                logger.warning(f"   âš ï¸ Attempt {attempt + 1} failed for '{document_title}': {e}")
                if attempt + 1 == max_retries:
                    logger.error(f"   âŒ All attempts failed for '{document_title}'. Using fallback.")
                    # Fallback: Tráº£ vá» má»™t cÃ¢u há»i cÆ¡ báº£n náº¿u LLM tháº¥t báº¡i hoÃ n toÃ n
                    return {
                        "main_question": f"Thá»§ tá»¥c {document_title} cáº§n nhá»¯ng gÃ¬?",
                        "question_variants": [
                            f"Há»“ sÆ¡ {document_title} ra sao?",
                            f"Chi phÃ­ {document_title} lÃ  bao nhiÃªu?",
                            f"LÃ m {document_title} á»Ÿ Ä‘Ã¢u?"
                        ]
                    }
                time.sleep(1)  # Chá» 1 giÃ¢y trÆ°á»›c khi thá»­ láº¡i
        
        # Shouldn't reach here, but just in case
        return {
            "main_question": f"Thá»§ tá»¥c {document_title} cáº§n nhá»¯ng gÃ¬?",
            "question_variants": [
                f"Há»“ sÆ¡ {document_title} ra sao?",
                f"Chi phÃ­ {document_title} lÃ  bao nhiÃªu?"
            ]
        }

    def _clean_question(self, question: str) -> str:
        """Clean up generated questions"""
        if not question:
            return question
            
        # Remove numbering, bullet points, and extra whitespace
        question = re.sub(r'^[\d\-\*\+\.\)]\s*', '', question).strip()
        
        # Remove quotes and extra whitespace
        question = question.strip().strip('"').strip("'").strip()
        
        # Remove trailing underscores and parenthetical examples
        question = re.sub(r'\s*_+\s*$', '', question)  # Remove trailing ___
        question = re.sub(r'\s*\([^)]*\)\s*$', '', question)  # Remove (examples) at end
        
        # Ensure question ends with question mark
        if question and not question.endswith('?'):
            question += '?'
            
        # Limit question length to avoid overly long questions
        if len(question) > 100:
            question = question[:97] + "...?"
            
        return question

    def _summarize_document_for_prompt(self, doc: Dict[str, Any]) -> str:
        """Táº¡o má»™t báº£n tÃ³m táº¯t chi tiáº¿t tá»« file JSON Ä‘á»ƒ lÃ m input cho LLM."""
        parts = []
        metadata = doc.get("metadata", {})
        
        # Basic info vá»›i nhiá»u chi tiáº¿t hÆ¡n
        applicant_types = metadata.get('applicant_type', ['CÃ¡ nhÃ¢n'])
        if isinstance(applicant_types, list):
            parts.append(f"Äá»‘i tÆ°á»£ng: {', '.join(applicant_types)}")
        else:
            parts.append(f"Äá»‘i tÆ°á»£ng: {applicant_types}")
        
        parts.append(f"CÆ¡ quan thá»±c hiá»‡n: {metadata.get('executing_agency', 'N/A')}")
        parts.append(f"Thá»i gian xá»­ lÃ½: {metadata.get('processing_time_text', metadata.get('processing_time', 'N/A'))}")
        parts.append(f"Lá»‡ phÃ­: {metadata.get('fee_text', metadata.get('fee', 'N/A'))}")
        
        # ThÃªm thÃ´ng tin vá» submission method
        submission_method = metadata.get('submission_method', [])
        if submission_method:
            if isinstance(submission_method, list):
                parts.append(f"CÃ¡ch ná»™p há»“ sÆ¡: {', '.join(submission_method)}")
            else:
                parts.append(f"CÃ¡ch ná»™p há»“ sÆ¡: {submission_method}")
        
        # ThÃªm thÃ´ng tin vá» result delivery
        result_delivery = metadata.get('result_delivery', [])
        if result_delivery:
            if isinstance(result_delivery, list):
                parts.append(f"CÃ¡ch nháº­n káº¿t quáº£: {', '.join(result_delivery)}")
            else:
                parts.append(f"CÃ¡ch nháº­n káº¿t quáº£: {result_delivery}")
        
        # Content chunks - get key information tá»« nhiá»u sections
        content_chunks = doc.get("content_chunks", [])
        if content_chunks:
            # TÃ¬m section há»“ sÆ¡
            hoso_chunk = next((c for c in content_chunks if "há»“ sÆ¡" in c.get("section_title", "").lower()), None)
            if hoso_chunk:
                content = hoso_chunk.get('content', '')[:400]  # TÄƒng giá»›i háº¡n
                parts.append(f"ThÃ nh pháº§n há»“ sÆ¡: {content}...")
            
            # TÃ¬m section quy trÃ¬nh
            quytrÃ¬nh_chunk = next((c for c in content_chunks if any(keyword in c.get("section_title", "").lower() 
                                                                    for keyword in ["quy trÃ¬nh", "thá»§ tá»¥c", "cÃ¡ch thá»©c"])), None)
            if quytrÃ¬nh_chunk and quytrÃ¬nh_chunk != hoso_chunk:
                content = quytrÃ¬nh_chunk.get('content', '')[:300]
                parts.append(f"Quy trÃ¬nh thá»±c hiá»‡n: {content}...")
            
            # TÃ¬m section Ä‘iá»u kiá»‡n/yÃªu cáº§u
            dieukien_chunk = next((c for c in content_chunks if any(keyword in c.get("section_title", "").lower() 
                                                                   for keyword in ["Ä‘iá»u kiá»‡n", "yÃªu cáº§u", "Ä‘á»‘i tÆ°á»£ng"])), None)
            if dieukien_chunk and dieukien_chunk != hoso_chunk and dieukien_chunk != quytrÃ¬nh_chunk:
                content = dieukien_chunk.get('content', '')[:200]
                parts.append(f"Äiá»u kiá»‡n/YÃªu cáº§u: {content}...")

        return ". ".join(parts)
        
    def _extract_questions_from_llm_response(self, text: str, document_title: str) -> Optional[Dict]:
        """TrÃ­ch xuáº¥t vÃ  táº¡o dictionary tá»« text response cá»§a LLM."""
        if not text:
            return None
            
        # Strategy 1: TÃ¬m JSON sáºµn cÃ³
        json_match = re.search(r'\{.*?"main_question".*?"question_variants".*?\}', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except:
                pass
        
        # Strategy 2: Extract tá»« text response thÃ´ng thÆ°á»ng vá»›i deduplication
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        questions = []
        seen_questions = set()  # Äá»ƒ trÃ¡nh trÃ¹ng láº·p
        
        # Lá»c ra nhá»¯ng dÃ²ng chá»©a cÃ¢u há»i (cÃ³ Ä‘Ã¡nh sá»‘ hoáº·c káº¿t thÃºc báº±ng dáº¥u ?)
        for line in lines:
            # Remove numbering vÃ  bullet points
            cleaned_line = re.sub(r'^[\d\-\*\+\.\)]\s*', '', line).strip()
            
            # Lá»c cÃ¢u há»i dÃ i hÆ¡n 5 kÃ½ tá»± vÃ  cÃ³ dáº¥u ? hoáº·c cÃ³ tá»« khÃ³a cÃ¢u há»i
            if (cleaned_line.endswith('?') or any(keyword in cleaned_line.lower() 
                                                 for keyword in ['gÃ¬', 'nhÆ° tháº¿ nÃ o', 'bao nhiÃªu', 'á»Ÿ Ä‘Ã¢u', 'khi nÃ o', 'ai'])) and len(cleaned_line) > 5:
                # TrÃ¡nh trÃ¹ng láº·p báº±ng cÃ¡ch kiá»ƒm tra similarity
                is_duplicate = any(self._are_questions_similar(cleaned_line.lower(), seen) 
                                 for seen in seen_questions)
                if not is_duplicate:
                    if not cleaned_line.endswith('?'):
                        cleaned_line += '?'  # ThÃªm dáº¥u ? náº¿u chÆ°a cÃ³
                    questions.append(cleaned_line)
                    seen_questions.add(cleaned_line.lower())
        
        if len(questions) >= 1:
            # Chá»n cÃ¢u há»i chÃ­nh - Æ°u tiÃªn cÃ¢u há»i tá»•ng quan
            main_question = questions[0]
            
            # TÃ¬m cÃ¢u há»i tá»•ng quan tá»‘t hÆ¡n náº¿u cÃ³
            for q in questions[:3]:  # Chá»‰ xem 3 cÃ¢u Ä‘áº§u
                if any(keyword in q.lower() for keyword in ["lÃ  gÃ¬", "nhÆ° tháº¿ nÃ o", "thá»§ tá»¥c", "quy trÃ¬nh"]):
                    main_question = q
                    break
            
            # Láº¥y táº¥t cáº£ cÃ¢u há»i cÃ²n láº¡i lÃ m variants, tá»‘i Ä‘a 12 cÃ¢u
            question_variants = [q for q in questions if q != main_question][:12]
            
            return {
                "main_question": main_question,
                "question_variants": question_variants
            }
        
        # Strategy 3: Fallback vá»›i template questions
        return {
            "main_question": f"Thá»§ tá»¥c {document_title} lÃ  gÃ¬?",
            "question_variants": [
                f"LÃ m {document_title} cáº§n giáº¥y tá» gÃ¬?",
                f"Chi phÃ­ {document_title} bao nhiÃªu?",
                f"Thá»i gian xá»­ lÃ½ {document_title} lÃ  bao lÃ¢u?",
                f"LÃ m {document_title} á»Ÿ Ä‘Ã¢u?",
                f"CÃ³ thá»ƒ lÃ m {document_title} online khÃ´ng?",
                f"Nháº­n káº¿t quáº£ {document_title} tháº¿ nÃ o?"
            ]
        }
        
    def _are_questions_similar(self, q1: str, q2: str, threshold: float = 0.7) -> bool:
        """Kiá»ƒm tra 2 cÃ¢u há»i cÃ³ tÆ°Æ¡ng tá»± nhau khÃ´ng Ä‘á»ƒ trÃ¡nh trÃ¹ng láº·p."""
        # Simple similarity check dá»±a trÃªn tá»« khÃ³a chÃ­nh
        keywords1 = set(re.findall(r'\w+', q1.lower()))
        keywords2 = set(re.findall(r'\w+', q2.lower()))
        
        if not keywords1 or not keywords2:
            return False
        
        # Jaccard similarity
        intersection = len(keywords1.intersection(keywords2))
        union = len(keywords1.union(keywords2))
        
        return intersection / union > threshold if union > 0 else False

    def generate_all_smart_examples(self, force_rebuild: bool = False) -> int:
        """Sinh táº¥t cáº£ smart router examples báº±ng LLM."""
        logger.info("ğŸ¯ Generating smart router examples using LLM v2...")
        
        if not self.documents_dir.exists():
            logger.error(f"âŒ Documents directory not found: {self.documents_dir}")
            return 0
        
        json_files = sorted(list(self.documents_dir.rglob("*.json")))
        if not json_files:
            logger.error("âŒ No JSON documents found")
            return 0
        
        logger.info(f"   ğŸ“„ Found {len(json_files)} JSON files to process.")
        
        total_examples = 0
        processed_files = 0
        
        for i, json_file in enumerate(json_files):
            logger.info("-" * 60)
            logger.info(f"Processing file {i+1}/{len(json_files)}: {json_file.name}")
            
            relative_path = json_file.relative_to(self.documents_dir)
            output_file = self.output_dir / relative_path

            if not force_rebuild and output_file.exists():
                logger.info(f"   â© Skipping, router file already exists. Use --force to overwrite.")
                continue

            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    doc = json.load(f)
                
                # 1. PHÃ‚N TÃCH METADATA
                analysis = self.analyze_document_metadata(doc)
                collection_name = self._detect_collection_from_path(str(json_file))
                
                # 2. SINH CÃ‚U Há»I Báº°NG LLM
                questions = self.generate_questions_with_llm(doc)
                
                # 3. Táº O VÃ€ LÆ¯U FILE ROUTER
                router_data = {
                    'metadata': {
                        'title': doc.get('metadata', {}).get('title', ''),
                        'code': doc.get('metadata', {}).get('code', ''),
                        'collection': collection_name,
                        'source_document': str(relative_path),
                        'generated_by': 'llm_powered_generator_v2.0',
                        'generated_at': time.strftime('%Y-%m-%d %H:%M:%S')
                    },
                    'main_question': questions['main_question'],
                    'question_variants': questions.get('question_variants', []),
                    'smart_filters': analysis['smart_filters'],
                    'expected_collection': collection_name,
                    'confidence_threshold': analysis.get('confidence_threshold', 0.75),
                    'priority_score': analysis.get('priority_score', 1.0)
                }
                
                # Ensure output directory exists
                output_file.parent.mkdir(parents=True, exist_ok=True)
                
                # Save router data
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(router_data, f, ensure_ascii=False, indent=2)
                
                processed_files += 1
                num_questions = 1 + len(questions.get('question_variants', []))
                total_examples += num_questions
                logger.info(f"   ğŸ’¾ Saved {num_questions} examples to {output_file.name}")
                
                # ThÃªm má»™t khoáº£ng nghá»‰ nhá» Ä‘á»ƒ khÃ´ng lÃ m LLM quÃ¡ táº£i
                time.sleep(0.5) 

            except Exception as e:
                logger.error(f"   âš ï¸ Error processing {json_file.name}: {e}")
                continue
        
        # Táº¡o file tÃ³m táº¯t
        self._generate_summary(processed_files, total_examples, json_files)
        return processed_files

    def _generate_summary(self, processed_files: int, total_examples: int, json_files: List[Path]):
        """Táº¡o file tÃ³m táº¯t káº¿t quáº£."""
        collections = {}
        for json_file in json_files:
            collection = self._detect_collection_from_path(str(json_file))
            collections[collection] = collections.get(collection, 0) + 1
        
        summary = {
            'generator_info': {
                'version': 'llm_powered_v2.0',
                'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'llm_model': 'PhoGPT-4B-Chat'
            },
            'statistics': {
                'total_files_processed': processed_files,
                'total_source_files': len(json_files),
                'total_examples_generated': total_examples,
                'collections_distribution': collections
            },
            'quality_metrics': {
                'avg_variants_per_document': total_examples / processed_files if processed_files > 0 else 0,
                'success_rate': (processed_files / len(json_files)) * 100 if json_files else 0
            }
        }
        
        summary_file = self.output_dir / "llm_generation_summary_v2.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nğŸ“Š Summary saved to {summary_file}")

def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description='Generate smart router examples using LLM v2')
    parser.add_argument('--force', action='store_true', help='Force rebuild existing examples')
    parser.add_argument('--docs', type=str, default='data/documents', help='Documents directory')
    parser.add_argument('--output', type=str, default='data/router_examples_smart', help='Output directory')
    
    args = parser.parse_args()
    
    try:
        generator = SmartRouterLLMGenerator(args.docs, args.output)
        
        logger.info("ğŸš€ Starting LLM-powered Smart Router Generation v2...")
        start_time = time.time()
        
        processed_count = generator.generate_all_smart_examples(force_rebuild=args.force)
        
        end_time = time.time()
        duration = end_time - start_time
        
        logger.info("=" * 60)
        logger.info(f"âœ… Generation completed!")
        logger.info(f"   ğŸ“Š Processed: {processed_count} files")
        logger.info(f"   â±ï¸  Duration: {duration:.2f} seconds")
        logger.info(f"   ğŸ“ Output: {args.output}")
        logger.info("=" * 60)
        
    except KeyboardInterrupt:
        logger.info("\nâš ï¸ Process interrupted by user")
    except Exception as e:
        logger.error(f"âŒ Error: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
