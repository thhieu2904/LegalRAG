#!/usr/bin/env python3
"""
Enhanced Smart Router Generator with LLM - Version 2
=======================================================

Sinh t·ª± ƒë·ªông c√°c c√¢u h·ªèi router examples b·∫±ng LLM thay v√¨ template c·ªë ƒë·ªãnh.
T·ªëi ∆∞u h√≥a ƒë·ªÉ sinh ra 10+ c√¢u h·ªèi ƒëa d·∫°ng t·ª´ metadata phong ph√∫.

S·ª≠a ƒë·ªïi: Tr·∫£ v·ªÅ dictionary thay v√¨ JSON string ƒë·ªÉ tr√°nh l·ªói parsing.
"""

import json
import re
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import sys
import argparse

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Add app to Python path
sys.path.append(str(Path(__file__).parent.parent))

try:
    from app.services.language_model import LLMService
except ImportError as e:
    logger.error(f"‚ùå Cannot import LLMService: {e}")
    logger.error("   Make sure you're in the backend directory and the service is available")
    sys.exit(1)

class SmartRouterLLMGenerator:
    """Enhanced router generator using LLM for question generation."""
    
    def __init__(self, documents_dir: str = None, output_dir: str = None):
        self.documents_dir = Path(documents_dir or "data/documents")
        self.output_dir = Path(output_dir or "data/router_examples_smart")
        
        # Initialize LLM service
        try:
            self.llm_service = LLMService()
            logger.info(f"‚úÖ LLM Service initialized successfully")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize LLM Service: {e}")
            raise

    def _detect_collection_from_path(self, file_path: str) -> str:
        """Detect collection name from file path."""
        path_lower = file_path.lower()
        
        if 'hanh_chinh' in path_lower:
            return 'administrative_procedures'
        elif 'kinh_doanh' in path_lower:
            return 'business_procedures'
        elif 'dat_dai' in path_lower:
            return 'land_procedures'
        elif 'xay_dung' in path_lower:
            return 'construction_procedures'
        elif 'tu_phap' in path_lower:
            return 'judicial_procedures'
        else:
            return 'general_procedures'

    def analyze_document_metadata(self, doc: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze document metadata for smart filtering."""
        metadata = doc.get('metadata', {})
        smart_filters = {}
        
        # Fee analysis
        fee_text = metadata.get('fee_text', '') or metadata.get('fee', '')
        if fee_text:
            fee_lower = fee_text.lower()
            if 'mi·ªÖn ph√≠' in fee_lower or 'kh√¥ng ph√≠' in fee_lower:
                smart_filters['has_fee'] = False
            else:
                smart_filters['has_fee'] = True
        
        # Processing time
        processing_time = metadata.get('processing_time_text', '') or metadata.get('processing_time', '')
        if processing_time:
            time_normalized = processing_time.lower()
            if 'ngay' in time_normalized:
                smart_filters['processing_speed'] = 'fast'
            elif 'tu·∫ßn' in time_normalized or 'th√°ng' in time_normalized:
                smart_filters['processing_speed'] = 'slow'
        
        return {
            'metadata': metadata,
            'smart_filters': smart_filters,
            'confidence_threshold': 0.75,
            'priority_score': 1.0
        }

    def generate_questions_with_llm(self, doc: Dict[str, Any]) -> Dict[str, Any]:
        """S·ª≠ d·ª•ng LLM ƒë·ªÉ sinh ra c√¢u h·ªèi ch√≠nh v√† c√°c bi·∫øn th·ªÉ."""
        metadata = doc.get('metadata', {})
        document_title = metadata.get('title', 'Th·ªß t·ª•c ch∆∞a x√°c ƒë·ªãnh')
        
        # T·∫°o m·ªôt chu·ªói t√≥m t·∫Øt n·ªôi dung t√†i li·ªáu ƒë·ªÉ ƒë∆∞a v√†o prompt
        document_content_summary = self._summarize_document_for_prompt(doc)

        # Prompt t·ªëi ∆∞u h√≥a - ng·∫Øn g·ªçn v√† hi·ªáu qu·∫£
        user_query = f"""NHI·ªÜM V·ª§: T·∫°o ch√≠nh x√°c 10 c√¢u h·ªèi v·ªÅ th·ªß t·ª•c "{document_title}"

TH√îNG TIN: {document_content_summary}

Y√äU C·∫¶U: T·∫°o 10 c√¢u h·ªèi ng·∫Øn g·ªçn, m·ªói c√¢u m·ªôt d√≤ng, ƒë√°nh s·ªë t·ª´ 1-10. M·ªói c√¢u ph·∫£i kh√°c nhau ho√†n to√†n.

V√ç D·ª§ FORMAT:
1. Th·ªß t·ª•c n√†y l√† g√¨?
2. Ai c√≥ th·ªÉ l√†m?
3. C·∫ßn gi·∫•y t·ªù g√¨?
4. Chi ph√≠ bao nhi√™u?
5. L√†m ·ªü ƒë√¢u?
6. M·∫•t bao l√¢u?
7. L√†m online ƒë∆∞·ª£c kh√¥ng?
8. Nh·∫≠n k·∫øt qu·∫£ nh∆∞ th·∫ø n√†o?
9. C√≥ ƒëi·ªÅu ki·ªán g√¨ ƒë·∫∑c bi·ªát?
10. L∆∞u √Ω g√¨ khi l√†m?

B·∫ÆT ƒê·∫¶U T·∫†O 10 C√ÇU H·ªéI:"""

        # System prompt ng·∫Øn g·ªçn v√† ch·ªâ th·ªã r√µ r√†ng
        system_prompt = "T·∫°o ch√≠nh x√°c 10 c√¢u h·ªèi ng·∫Øn. ƒê√°nh s·ªë 1-10. Kh√¥ng gi·∫£i th√≠ch th√™m."
        
        # Th√™m retry logic ƒë·ªÉ x·ª≠ l√Ω LLM kh√¥ng ·ªïn ƒë·ªãnh
        max_retries = 2
        for attempt in range(max_retries):
            try:
                logger.info(f"   ü§ñ Calling LLM (Attempt {attempt + 1}/{max_retries}) for: '{document_title}'")
                response_data = self.llm_service.generate_response(
                    user_query=user_query,
                    max_tokens=500,  # Gi·∫£m ƒë·ªÉ tr√°nh repetition
                    temperature=0.3,  # TƒÉng m·ªôt ch√∫t ƒë·ªÉ c√≥ ƒëa d·∫°ng
                    system_prompt=system_prompt  # S·ª≠ d·ª•ng system prompt ri√™ng
                )
                
                # Tr√≠ch xu·∫•t v√† l√†m s·∫°ch t·ª´ ph·∫£n h·ªìi c·ªßa LLM
                response_text = response_data.get('response', '')
                logger.info(f"   üìù LLM Response (first 200 chars): {response_text[:200]}...")
                
                generated_data = self._extract_questions_from_llm_response(response_text, document_title)
                
                if generated_data:
                    # Clean up questions
                    generated_data['main_question'] = self._clean_question(generated_data['main_question'])
                    generated_data['question_variants'] = [
                        self._clean_question(q) for q in generated_data['question_variants'] if q.strip()
                    ]
                    
                    logger.info(f"   ‚úÖ LLM generated {1 + len(generated_data['question_variants'])} questions.")
                    return generated_data  # Th√†nh c√¥ng - tho√°t kh·ªèi retry loop
                else:
                    raise ValueError("No valid questions found in LLM response")

            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Attempt {attempt + 1} failed for '{document_title}': {e}")
                if attempt + 1 == max_retries:
                    logger.error(f"   ‚ùå All attempts failed for '{document_title}'. Using fallback.")
                    # Fallback: Tr·∫£ v·ªÅ m·ªôt c√¢u h·ªèi c∆° b·∫£n n·∫øu LLM th·∫•t b·∫°i ho√†n to√†n
                    return {
                        "main_question": f"Th·ªß t·ª•c {document_title} c·∫ßn nh·ªØng g√¨?",
                        "question_variants": [
                            f"H·ªì s∆° {document_title} ra sao?",
                            f"Chi ph√≠ {document_title} l√† bao nhi√™u?",
                            f"L√†m {document_title} ·ªü ƒë√¢u?"
                        ]
                    }
                time.sleep(1)  # Ch·ªù 1 gi√¢y tr∆∞·ªõc khi th·ª≠ l·∫°i
        
        # Shouldn't reach here, but just in case
        return {
            "main_question": f"Th·ªß t·ª•c {document_title} c·∫ßn nh·ªØng g√¨?",
            "question_variants": [
                f"H·ªì s∆° {document_title} ra sao?",
                f"Chi ph√≠ {document_title} l√† bao nhi√™u?"
            ]
        }

    def _clean_question(self, question: str) -> str:
        """Clean up generated questions"""
        if not question:
            return question
            
        # Remove numbering, bullet points, and extra whitespace
        question = re.sub(r'^[\d\-\*\+\.\)]\s*', '', question).strip()
        
        # Remove quotes and extra whitespace
        question = question.strip().strip('"').strip("'").strip()
        
        # Remove trailing underscores and parenthetical examples
        question = re.sub(r'\s*_+\s*$', '', question)  # Remove trailing ___
        question = re.sub(r'\s*\([^)]*\)\s*$', '', question)  # Remove (examples) at end
        
        # Ensure question ends with question mark
        if question and not question.endswith('?'):
            question += '?'
            
        # Limit question length to avoid overly long questions
        if len(question) > 100:
            question = question[:97] + "...?"
            
        return question

    def _summarize_document_for_prompt(self, doc: Dict[str, Any]) -> str:
        """T·∫°o m·ªôt b·∫£n t√≥m t·∫Øt chi ti·∫øt t·ª´ file JSON ƒë·ªÉ l√†m input cho LLM."""
        parts = []
        metadata = doc.get("metadata", {})
        
        # Basic info v·ªõi nhi·ªÅu chi ti·∫øt h∆°n
        applicant_types = metadata.get('applicant_type', ['C√° nh√¢n'])
        if isinstance(applicant_types, list):
            parts.append(f"ƒê·ªëi t∆∞·ª£ng: {', '.join(applicant_types)}")
        else:
            parts.append(f"ƒê·ªëi t∆∞·ª£ng: {applicant_types}")
        
        parts.append(f"C∆° quan th·ª±c hi·ªán: {metadata.get('executing_agency', 'N/A')}")
        parts.append(f"Th·ªùi gian x·ª≠ l√Ω: {metadata.get('processing_time_text', metadata.get('processing_time', 'N/A'))}")
        parts.append(f"L·ªá ph√≠: {metadata.get('fee_text', metadata.get('fee', 'N/A'))}")
        
        # Th√™m th√¥ng tin v·ªÅ submission method
        submission_method = metadata.get('submission_method', [])
        if submission_method:
            if isinstance(submission_method, list):
                parts.append(f"C√°ch n·ªôp h·ªì s∆°: {', '.join(submission_method)}")
            else:
                parts.append(f"C√°ch n·ªôp h·ªì s∆°: {submission_method}")
        
        # Th√™m th√¥ng tin v·ªÅ result delivery
        result_delivery = metadata.get('result_delivery', [])
        if result_delivery:
            if isinstance(result_delivery, list):
                parts.append(f"C√°ch nh·∫≠n k·∫øt qu·∫£: {', '.join(result_delivery)}")
            else:
                parts.append(f"C√°ch nh·∫≠n k·∫øt qu·∫£: {result_delivery}")
        
        # Content chunks - get key information t·ª´ nhi·ªÅu sections
        content_chunks = doc.get("content_chunks", [])
        if content_chunks:
            # T√¨m section h·ªì s∆°
            hoso_chunk = next((c for c in content_chunks if "h·ªì s∆°" in c.get("section_title", "").lower()), None)
            if hoso_chunk:
                content = hoso_chunk.get('content', '')[:400]  # TƒÉng gi·ªõi h·∫°n
                parts.append(f"Th√†nh ph·∫ßn h·ªì s∆°: {content}...")
            
            # T√¨m section quy tr√¨nh
            quytr√¨nh_chunk = next((c for c in content_chunks if any(keyword in c.get("section_title", "").lower() 
                                                                    for keyword in ["quy tr√¨nh", "th·ªß t·ª•c", "c√°ch th·ª©c"])), None)
            if quytr√¨nh_chunk and quytr√¨nh_chunk != hoso_chunk:
                content = quytr√¨nh_chunk.get('content', '')[:300]
                parts.append(f"Quy tr√¨nh th·ª±c hi·ªán: {content}...")
            
            # T√¨m section ƒëi·ªÅu ki·ªán/y√™u c·∫ßu
            dieukien_chunk = next((c for c in content_chunks if any(keyword in c.get("section_title", "").lower() 
                                                                   for keyword in ["ƒëi·ªÅu ki·ªán", "y√™u c·∫ßu", "ƒë·ªëi t∆∞·ª£ng"])), None)
            if dieukien_chunk and dieukien_chunk != hoso_chunk and dieukien_chunk != quytr√¨nh_chunk:
                content = dieukien_chunk.get('content', '')[:200]
                parts.append(f"ƒêi·ªÅu ki·ªán/Y√™u c·∫ßu: {content}...")

        return ". ".join(parts)
        
    def _extract_questions_from_llm_response(self, text: str, document_title: str) -> Optional[Dict]:
        """Tr√≠ch xu·∫•t v√† t·∫°o dictionary t·ª´ text response c·ªßa LLM."""
        if not text:
            return None
            
        # Strategy 1: T√¨m JSON s·∫µn c√≥
        json_match = re.search(r'\{.*?"main_question".*?"question_variants".*?\}', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except:
                pass
        
        # Strategy 2: Extract t·ª´ text response th√¥ng th∆∞·ªùng v·ªõi deduplication
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        questions = []
        seen_questions = set()  # ƒê·ªÉ tr√°nh tr√πng l·∫∑p
        
        # L·ªçc ra nh·ªØng d√≤ng ch·ª©a c√¢u h·ªèi (c√≥ ƒë√°nh s·ªë ho·∫∑c k·∫øt th√∫c b·∫±ng d·∫•u ?)
        for line in lines:
            # Remove numbering v√† bullet points
            cleaned_line = re.sub(r'^[\d\-\*\+\.\)]\s*', '', line).strip()
            
            # L·ªçc c√¢u h·ªèi d√†i h∆°n 5 k√Ω t·ª± v√† c√≥ d·∫•u ? ho·∫∑c c√≥ t·ª´ kh√≥a c√¢u h·ªèi
            if (cleaned_line.endswith('?') or any(keyword in cleaned_line.lower() 
                                                 for keyword in ['g√¨', 'nh∆∞ th·∫ø n√†o', 'bao nhi√™u', '·ªü ƒë√¢u', 'khi n√†o', 'ai'])) and len(cleaned_line) > 5:
                # Tr√°nh tr√πng l·∫∑p b·∫±ng c√°ch ki·ªÉm tra similarity
                is_duplicate = any(self._are_questions_similar(cleaned_line.lower(), seen) 
                                 for seen in seen_questions)
                if not is_duplicate:
                    if not cleaned_line.endswith('?'):
                        cleaned_line += '?'  # Th√™m d·∫•u ? n·∫øu ch∆∞a c√≥
                    questions.append(cleaned_line)
                    seen_questions.add(cleaned_line.lower())
        
        if len(questions) >= 1:
            # Ch·ªçn c√¢u h·ªèi ch√≠nh - ∆∞u ti√™n c√¢u h·ªèi t·ªïng quan
            main_question = questions[0]
            
            # T√¨m c√¢u h·ªèi t·ªïng quan t·ªët h∆°n n·∫øu c√≥
            for q in questions[:3]:  # Ch·ªâ xem 3 c√¢u ƒë·∫ßu
                if any(keyword in q.lower() for keyword in ["l√† g√¨", "nh∆∞ th·∫ø n√†o", "th·ªß t·ª•c", "quy tr√¨nh"]):
                    main_question = q
                    break
            
            # L·∫•y t·∫•t c·∫£ c√¢u h·ªèi c√≤n l·∫°i l√†m variants, t·ªëi ƒëa 12 c√¢u
            question_variants = [q for q in questions if q != main_question][:12]
            
            return {
                "main_question": main_question,
                "question_variants": question_variants
            }
        
        # Strategy 3: Fallback v·ªõi template questions
        return {
            "main_question": f"Th·ªß t·ª•c {document_title} l√† g√¨?",
            "question_variants": [
                f"L√†m {document_title} c·∫ßn gi·∫•y t·ªù g√¨?",
                f"Chi ph√≠ {document_title} bao nhi√™u?",
                f"Th·ªùi gian x·ª≠ l√Ω {document_title} l√† bao l√¢u?",
                f"L√†m {document_title} ·ªü ƒë√¢u?",
                f"C√≥ th·ªÉ l√†m {document_title} online kh√¥ng?",
                f"Nh·∫≠n k·∫øt qu·∫£ {document_title} th·∫ø n√†o?"
            ]
        }
        
    def _are_questions_similar(self, q1: str, q2: str, threshold: float = 0.7) -> bool:
        """Ki·ªÉm tra 2 c√¢u h·ªèi c√≥ t∆∞∆°ng t·ª± nhau kh√¥ng ƒë·ªÉ tr√°nh tr√πng l·∫∑p."""
        # Simple similarity check d·ª±a tr√™n t·ª´ kh√≥a ch√≠nh
        keywords1 = set(re.findall(r'\w+', q1.lower()))
        keywords2 = set(re.findall(r'\w+', q2.lower()))
        
        if not keywords1 or not keywords2:
            return False
        
        # Jaccard similarity
        intersection = len(keywords1.intersection(keywords2))
        union = len(keywords1.union(keywords2))
        
        return intersection / union > threshold if union > 0 else False

    def generate_all_smart_examples(self, force_rebuild: bool = False) -> int:
        """Sinh t·∫•t c·∫£ smart router examples b·∫±ng LLM."""
        logger.info("üéØ Generating smart router examples using LLM v2...")
        
        if not self.documents_dir.exists():
            logger.error(f"‚ùå Documents directory not found: {self.documents_dir}")
            return 0
        
        json_files = sorted(list(self.documents_dir.rglob("*.json")))
        if not json_files:
            logger.error("‚ùå No JSON documents found")
            return 0
        
        logger.info(f"   üìÑ Found {len(json_files)} JSON files to process.")
        
        total_examples = 0
        processed_files = 0
        
        for i, json_file in enumerate(json_files):
            logger.info("-" * 60)
            logger.info(f"Processing file {i+1}/{len(json_files)}: {json_file.name}")
            
            relative_path = json_file.relative_to(self.documents_dir)
            output_file = self.output_dir / relative_path

            if not force_rebuild and output_file.exists():
                logger.info(f"   ‚è© Skipping, router file already exists. Use --force to overwrite.")
                continue

            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    doc = json.load(f)
                
                # 1. PH√ÇN T√çCH METADATA
                analysis = self.analyze_document_metadata(doc)
                collection_name = self._detect_collection_from_path(str(json_file))
                
                # 2. SINH C√ÇU H·ªéI B·∫∞NG LLM
                questions = self.generate_questions_with_llm(doc)
                
                # 3. T·∫†O V√Ä L∆ØU FILE ROUTER
                router_data = {
                    'metadata': {
                        'title': doc.get('metadata', {}).get('title', ''),
                        'code': doc.get('metadata', {}).get('code', ''),
                        'collection': collection_name,
                        'source_document': str(relative_path),
                        'generated_by': 'llm_powered_generator_v2.0',
                        'generated_at': time.strftime('%Y-%m-%d %H:%M:%S')
                    },
                    'main_question': questions['main_question'],
                    'question_variants': questions.get('question_variants', []),
                    'smart_filters': analysis['smart_filters'],
                    'expected_collection': collection_name,
                    'confidence_threshold': analysis.get('confidence_threshold', 0.75),
                    'priority_score': analysis.get('priority_score', 1.0)
                }
                
                # Ensure output directory exists
                output_file.parent.mkdir(parents=True, exist_ok=True)
                
                # Save router data
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(router_data, f, ensure_ascii=False, indent=2)
                
                processed_files += 1
                num_questions = 1 + len(questions.get('question_variants', []))
                total_examples += num_questions
                logger.info(f"   üíæ Saved {num_questions} examples to {output_file.name}")
                
                # Th√™m m·ªôt kho·∫£ng ngh·ªâ nh·ªè ƒë·ªÉ kh√¥ng l√†m LLM qu√° t·∫£i
                time.sleep(0.5) 

            except Exception as e:
                logger.error(f"   ‚ö†Ô∏è Error processing {json_file.name}: {e}")
                continue
        
        # T·∫°o file t√≥m t·∫Øt
        self._generate_summary(processed_files, total_examples, json_files)
        return processed_files

    def _generate_summary(self, processed_files: int, total_examples: int, json_files: List[Path]):
        """T·∫°o file t√≥m t·∫Øt k·∫øt qu·∫£."""
        collections = {}
        for json_file in json_files:
            collection = self._detect_collection_from_path(str(json_file))
            collections[collection] = collections.get(collection, 0) + 1
        
        summary = {
            'generator_info': {
                'version': 'llm_powered_v2.0',
                'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'llm_model': 'PhoGPT-4B-Chat'
            },
            'statistics': {
                'total_files_processed': processed_files,
                'total_source_files': len(json_files),
                'total_examples_generated': total_examples,
                'collections_distribution': collections
            },
            'quality_metrics': {
                'avg_variants_per_document': total_examples / processed_files if processed_files > 0 else 0,
                'success_rate': (processed_files / len(json_files)) * 100 if json_files else 0
            }
        }
        
        summary_file = self.output_dir / "llm_generation_summary_v2.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nüìä Summary saved to {summary_file}")

def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description='Generate smart router examples using LLM v2')
    parser.add_argument('--force', action='store_true', help='Force rebuild existing examples')
    parser.add_argument('--docs', type=str, default='data/documents', help='Documents directory')
    parser.add_argument('--output', type=str, default='data/router_examples_smart', help='Output directory')
    
    args = parser.parse_args()
    
    try:
        generator = SmartRouterLLMGenerator(args.docs, args.output)
        
        logger.info("üöÄ Starting LLM-powered Smart Router Generation v2...")
        start_time = time.time()
        
        processed_count = generator.generate_all_smart_examples(force_rebuild=args.force)
        
        end_time = time.time()
        duration = end_time - start_time
        
        logger.info("=" * 60)
        logger.info(f"‚úÖ Generation completed!")
        logger.info(f"   üìä Processed: {processed_count} files")
        logger.info(f"   ‚è±Ô∏è  Duration: {duration:.2f} seconds")
        logger.info(f"   üìÅ Output: {args.output}")
        logger.info("=" * 60)
        
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Process interrupted by user")
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
