#!/usr/bin/env python3
"""
Multi-Aspect Multi-Persona Smart Router Generator - Version 4
===========================================================

Sinh c√¢u h·ªèi ƒëa kh√≠a c·∫°nh v√† ƒëa vai tr√≤ ƒë·ªÉ ƒë·∫°t m·ª•c ti√™u 30+ c√¢u h·ªèi/vƒÉn b·∫£n.
Ph∆∞∆°ng ph√°p: K·∫øt h·ª£p t·ª´ng content_chunk v·ªõi t·ª´ng persona ng∆∞·ªùi d√πng ph√π h·ª£p.
"""

import json
import re
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import sys
import argparse
from collections import defaultdict

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Add app to Python path
sys.path.append(str(Path(__file__).parent.parent))

try:
    from app.services.language_model import LLMService
except ImportError as e:
    logger.error(f"‚ùå Cannot import LLMService: {e}")
    logger.error("   This could be due to missing dependencies (llama_cpp, etc.)")
    logger.error("   Make sure you're in the backend directory and all requirements are installed")
    logger.error("   You can install missing dependencies with: pip install llama-cpp-python")
    sys.exit(1)

class MultiAspectPersonaRouter:
    """Multi-aspect multi-persona router generator for comprehensive question generation."""
    
    def __init__(self, documents_dir: Optional[str] = None, output_dir: Optional[str] = None):
        self.documents_dir = Path(documents_dir or "data/documents")
        self.output_dir = Path(output_dir or "data/router_examples_smart_v3")
        
        # Initialize LLM service
        try:
            self.llm_service = LLMService()
            logger.info(f"‚úÖ LLM Service initialized successfully")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize LLM Service: {e}")
            raise
        
        # Define 5 distinct user personas
        self.personas = {
            "nguoi_lan_dau": {
                "name": "Ng∆∞·ªùi L·∫ßn ƒê·∫ßu",
                "description": "Ng∆∞·ªùi ch∆∞a t·ª´ng l√†m th·ªß t·ª•c, c·∫ßn h∆∞·ªõng d·∫´n c∆° b·∫£n",
                "question_style": "C√¢u h·ªèi c∆° b·∫£n, t·ª´ng b∆∞·ªõc, d·ªÖ hi·ªÉu",
                "concerns": ["th·ªß t·ª•c c∆° b·∫£n", "gi·∫•y t·ªù c·∫ßn thi·∫øt", "b∆∞·ªõc ƒë·∫ßu ti√™n"]
            },
            "nguoi_ban_ron": {
                "name": "Ng∆∞·ªùi B·∫≠n R·ªôn", 
                "description": "Ng∆∞·ªùi b·∫≠n vi·ªác, mu·ªën gi·∫£i quy·∫øt nhanh, quan t√¢m th·ªùi gian",
                "question_style": "C√¢u h·ªèi v·ªÅ th·ªùi gian, t·ªëc ƒë·ªô, hi·ªáu qu·∫£",
                "concerns": ["th·ªùi gian x·ª≠ l√Ω", "c√°ch nhanh nh·∫•t", "online/offline"]
            },
            "nguoi_can_than": {
                "name": "Ng∆∞·ªùi C·∫©n Th·∫≠n",
                "description": "Ng∆∞·ªùi mu·ªën bi·∫øt chi ti·∫øt, ƒëi·ªÅu ki·ªán, l∆∞u √Ω ƒë·∫∑c bi·ªát",
                "question_style": "C√¢u h·ªèi chi ti·∫øt v·ªÅ ƒëi·ªÅu ki·ªán, r·ªßi ro, l∆∞u √Ω",
                "concerns": ["ƒëi·ªÅu ki·ªán c·ª• th·ªÉ", "tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát", "l∆∞u √Ω quan tr·ªçng"]
            },
            "nguoi_lam_ho": {
                "name": "Ng∆∞·ªùi L√†m H·ªô",
                "description": "Ng∆∞·ªùi ƒë·∫°i di·ªán, l√†m h·ªô cho ng∆∞·ªùi kh√°c",
                "question_style": "C√¢u h·ªèi v·ªÅ ·ªßy quy·ªÅn, ƒë·∫°i di·ªán, l√†m h·ªô",
                "concerns": ["·ªßy quy·ªÅn", "gi·∫•y t·ªù ƒë·∫°i di·ªán", "quy·ªÅn h·∫°n"]
            },
            "nguoi_gap_van_de": {
                "name": "Ng∆∞·ªùi G·∫∑p V·∫•n ƒê·ªÅ", 
                "description": "Ng∆∞·ªùi g·∫∑p kh√≥ khƒÉn, c·∫ßn gi·∫£i quy·∫øt t√¨nh hu·ªëng ƒë·∫∑c bi·ªát",
                "question_style": "C√¢u h·ªèi v·ªÅ x·ª≠ l√Ω s·ª± c·ªë, tr∆∞·ªùng h·ª£p kh√≥ khƒÉn",
                "concerns": ["thi·∫øu gi·∫•y t·ªù", "tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát", "gi·∫£i quy·∫øt kh√≥ khƒÉn"]
            }
        }
        
        # Define aspect-persona mapping (which personas are most relevant for each content type)
        self.aspect_persona_mapping = {
            "h·ªì s∆°": ["nguoi_lan_dau", "nguoi_can_than", "nguoi_gap_van_de"],
            "th·ªùi gian": ["nguoi_ban_ron", "nguoi_lan_dau"],
            "chi ph√≠": ["nguoi_ban_ron", "nguoi_can_than"],
            "ƒëi·ªÅu ki·ªán": ["nguoi_can_than", "nguoi_lan_dau"],
            "quy tr√¨nh": ["nguoi_lan_dau", "nguoi_can_than"],
            "·ªßy quy·ªÅn": ["nguoi_lam_ho", "nguoi_can_than"],
            "n∆°i th·ª±c hi·ªán": ["nguoi_ban_ron", "nguoi_lan_dau"],
            "k·∫øt qu·∫£": ["nguoi_ban_ron", "nguoi_can_than"],
            "l∆∞u √Ω": ["nguoi_can_than", "nguoi_gap_van_de"],
            "tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát": ["nguoi_gap_van_de", "nguoi_can_than"],
            "gi·∫•y t·ªù": ["nguoi_lan_dau", "nguoi_can_than", "nguoi_gap_van_de"]
        }

    def _classify_chunk_aspects(self, chunk: Dict[str, Any]) -> List[str]:
        """Ph√¢n lo·∫°i chunk thu·ªôc kh√≠a c·∫°nh n√†o."""
        content = (chunk.get('content', '') + ' ' + chunk.get('section_title', '')).lower()
        keywords = chunk.get('keywords', [])
        
        aspects = []
        
        # Check for specific aspects based on content and keywords
        if any(word in content for word in ['h·ªì s∆°', 'gi·∫•y t·ªù', 't√†i li·ªáu', 'ch·ª©ng t·ª´']):
            aspects.append('h·ªì s∆°')
            aspects.append('gi·∫•y t·ªù')
            
        if any(word in content for word in ['th·ªùi gian', 'th·ªùi h·∫°n', 'ng√†y', 'gi·ªù']):
            aspects.append('th·ªùi gian')
            
        if any(word in content for word in ['l·ªá ph√≠', 'chi ph√≠', 'ph√≠', 'ti·ªÅn']):
            aspects.append('chi ph√≠')
            
        if any(word in content for word in ['ƒëi·ªÅu ki·ªán', 'y√™u c·∫ßu', 'quy ƒë·ªãnh']):
            aspects.append('ƒëi·ªÅu ki·ªán')
            
        if any(word in content for word in ['quy tr√¨nh', 'tr√¨nh t·ª±', 'b∆∞·ªõc', 'th·ªß t·ª•c']):
            aspects.append('quy tr√¨nh')
            
        if any(word in content for word in ['·ªßy quy·ªÅn', 'ƒë·∫°i di·ªán', 'thay m·∫∑t']):
            aspects.append('·ªßy quy·ªÅn')
            
        if any(word in content for word in ['n∆°i', 'ƒë·ªãa ƒëi·ªÉm', 'c∆° quan', 'ph√≤ng']):
            aspects.append('n∆°i th·ª±c hi·ªán')
            
        if any(word in content for word in ['k·∫øt qu·∫£', 'gi·∫•y', 'ch·ª©ng', 'b·∫±ng']):
            aspects.append('k·∫øt qu·∫£')
            
        if any(word in content for word in ['l∆∞u √Ω', 'ch√∫ √Ω', 'quan tr·ªçng', 'ƒë·∫∑c bi·ªát']):
            aspects.append('l∆∞u √Ω')
            
        if any(word in content for word in ['tr∆∞·ªùng h·ª£p', 'ngo·∫°i l·ªá', 'ƒë·∫∑c bi·ªát', 'ri√™ng']):
            aspects.append('tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát')
            
        # Default fallback
        if not aspects:
            aspects = ['quy tr√¨nh', 'ƒëi·ªÅu ki·ªán']
            
        return list(set(aspects))  # Remove duplicates

    def _get_relevant_personas(self, aspects: List[str]) -> List[str]:
        """L·∫•y danh s√°ch personas ph√π h·ª£p v·ªõi c√°c aspects."""
        relevant_personas = set()
        
        for aspect in aspects:
            if aspect in self.aspect_persona_mapping:
                relevant_personas.update(self.aspect_persona_mapping[aspect])
        
        # Ensure we always have at least 2-3 personas
        if len(relevant_personas) < 2:
            relevant_personas.update(['nguoi_lan_dau', 'nguoi_can_than'])
            
        return list(relevant_personas)

    def _generate_questions_for_chunk_persona(self, chunk: Dict[str, Any], persona_key: str, 
                                            document_title: str, max_questions: int = 2) -> List[str]:
        """
        Sinh c√¢u h·ªèi theo quy tr√¨nh 2 b∆∞·ªõc: 1. Tr√≠ch xu·∫•t th√¥ng tin, 2. Sinh c√¢u h·ªèi.
        """
        persona = self.personas[persona_key]
        chunk_content = chunk.get('content', '')
        section_title = chunk.get('section_title', '')

        try:
            # === B∆Ø·ªöC 1: TR√çCH XU·∫§T TH√îNG TIN C·ªêT L√ïI ===
            # M·ª•c ti√™u: B·∫Øt LLM ƒë·ªçc v√† t√≥m t·∫Øt c√°c √Ω ch√≠nh m·ªôt c√°ch ng·∫Øn g·ªçn, ch√≠nh x√°c.
            # Ch√∫ng ta d√πng temperature=0.0 ƒë·ªÉ y√™u c·∫ßu s·ª± ch√≠nh x√°c tuy·ªát ƒë·ªëi, kh√¥ng s√°ng t·∫°o.
            extraction_system_prompt = "B·∫°n l√† tr·ª£ l√Ω AI ch·ªâ chuy√™n tr√≠ch xu·∫•t th√¥ng tin. Li·ªát k√™ c√°c ƒëi·ªÉm th√¥ng tin ch√≠nh t·ª´ vƒÉn b·∫£n sau th√†nh c√°c g·∫°ch ƒë·∫ßu d√≤ng ng·∫Øn g·ªçn. Kh√¥ng gi·∫£i th√≠ch, kh√¥ng b√¨nh lu·∫≠n."
            extraction_user_query = f"VƒÉn b·∫£n v·ªÅ '{section_title}' c·ªßa th·ªß t·ª•c '{document_title}':\n\n{chunk_content[:1500]}\n\nC√°c ƒëi·ªÉm th√¥ng tin ch√≠nh:"
            
            extraction_response = self.llm_service.generate_response(
                user_query=extraction_user_query,
                system_prompt=extraction_system_prompt,
                max_tokens=250,
                temperature=0.0  # Y√™u c·∫ßu s·ª± ch√≠nh x√°c, kh√¥ng "·∫£o gi√°c"
            )
            extracted_topics = extraction_response.get('response', '').strip()

            # N·∫øu b∆∞·ªõc 1 th·∫•t b·∫°i, kh√¥ng c√≥ th√¥ng tin ƒë·ªÉ l√†m b∆∞·ªõc 2 -> d·ª´ng l·∫°i
            if not extracted_topics or len(extracted_topics) < 10:
                logger.warning(f"      ‚ö†Ô∏è B∆∞·ªõc 1 th·∫•t b·∫°i: Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c th√¥ng tin t·ª´ chunk cho persona {persona_key}.")
                return []

            logger.info(f"      ‚úÖ B∆∞·ªõc 1 ({persona_key}): Tr√≠ch xu·∫•t th√†nh c√¥ng ch·ªß ƒë·ªÅ.")

            # === B∆Ø·ªöC 2: SINH C√ÇU H·ªéI T·ª™ TH√îNG TIN ƒê√É TR√çCH XU·∫§T ===
            # M·ª•c ti√™u: D·ª±a tr√™n c√°c √Ω ch√≠nh s·∫°ch s·∫Ω t·ª´ B∆∞·ªõc 1, LLM s·∫Ω ƒë√≥ng vai v√† ƒë·∫∑t c√¢u h·ªèi.
            # Y√™u c·∫ßu n√†y ƒë∆°n gi·∫£n h∆°n nhi·ªÅu, n√™n ch·∫•t l∆∞·ª£ng s·∫Ω cao h∆°n.
            # Ch√∫ng ta d√πng temperature=0.7 ƒë·ªÉ cho ph√©p m·ªôt ch√∫t s√°ng t·∫°o trong c√°ch ƒë·∫∑t c√¢u h·ªèi.
            generation_system_prompt = f"""ƒê√≥ng vai m·ªôt '{persona['name']}' ({persona['description']}). D·ª±a v√†o c√°c th√¥ng tin d∆∞·ªõi ƒë√¢y, h√£y ƒë·∫∑t {max_questions} c√¢u h·ªèi th·ª±c t·∫ø m√† b·∫°n quan t√¢m.

Y√äU C·∫¶U:
- Ch·ªâ tr·∫£ l·ªùi b·∫±ng c√¢u h·ªèi.
- M·ªói c√¢u h·ªèi tr√™n m·ªôt d√≤ng.
- Kh√¥ng l·∫∑p l·∫°i th√¥ng tin, kh√¥ng gi·∫£i th√≠ch, kh√¥ng ƒë√°nh s·ªë.
"""
            generation_user_query = f"Th√¥ng tin v·ªÅ th·ªß t·ª•c '{document_title}':\n{extracted_topics}\n\nH√£y ƒë·∫∑t c√¢u h·ªèi:"

            generation_response = self.llm_service.generate_response(
                user_query=generation_user_query,
                system_prompt=generation_system_prompt,
                max_tokens=200,
                temperature=0.7  # Cho ph√©p s√°ng t·∫°o trong c√°ch h·ªèi
            )
            response_text = generation_response.get('response', '').strip()
            
            # S·ª≠ d·ª•ng l·∫°i c√°c h√†m l√†m s·∫°ch v√† x√°c th·ª±c m·∫°nh m·∫Ω c·ªßa b·∫°n
            questions = []
            for line in response_text.split('\n'):
                cleaned_question = self._clean_question_v4(line)
                if cleaned_question and self._is_valid_question(cleaned_question):
                    questions.append(cleaned_question)
            
            if questions:
                 logger.info(f"         ‚úÖ B∆∞·ªõc 2 ({persona_key}): Sinh ra {len(questions)} c√¢u h·ªèi ch·∫•t l∆∞·ª£ng.")
            else:
                 logger.warning(f"      ‚ö†Ô∏è B∆∞·ªõc 2 ({persona_key}): Kh√¥ng sinh ƒë∆∞·ª£c c√¢u h·ªèi n√†o.")

            return questions[:max_questions] if questions else []

        except Exception as e:
            logger.error(f"      ‚ùå L·ªói trong quy tr√¨nh 2 b∆∞·ªõc cho persona {persona_key}: {e}")
            return []

    def _clean_question_v4(self, question: str) -> str:
        """Clean and standardize questions with enhanced cleaning."""
        # Filter out answer-like responses first (NEW)
        answer_keywords = [
            'tr·∫£ l·ªùi:', 'theo th√¥ng tin', 'd·ª±a tr√™n n·ªôi dung', 'trong t√†i li·ªáu',
            'sau khi nh·∫≠n ƒë∆∞·ª£c', 'c√°n b·ªô ti·∫øp nh·∫≠n s·∫Ω', 'gi·∫•y t·ªù n∆∞·ªõc ngo√†i',
            'y√™u c·∫ßu v·ªÅ gi·∫•y t·ªù', 'tuy nhi√™n v·ªõi', 'v√≠ d·ª•:', 'theo quy ƒë·ªãnh t·∫°i'
        ]
        if any(keyword in question.lower() for keyword in answer_keywords):
            return ""
        
        # Filter out statements that are clearly answers (not questions)  
        if question.strip().startswith(('Theo', 'D·ª±a v√†o', 'Nh∆∞ v·∫≠y', 'Do ƒë√≥', 'V√¨ v·∫≠y')):
            return ""
            
        # Remove common prefixes and numbering
        question = re.sub(r'^(\d+[\.\)]?\s*)', '', question)
        question = re.sub(r'^[‚Ä¢\-\*]\s*', '', question)
        
        # Remove "C√¢u h·ªèi" prefixes (case insensitive)
        question = re.sub(r'^(c√¢u h·ªèi\s*\d*[:\.]?\s*)', '', question, flags=re.IGNORECASE)
        
        # Remove meta-questions about question format
        if any(phrase in question.lower() for phrase in ['c√¢u h·ªèi k·∫øt th√∫c', 'd·∫•u ?', 't·∫°i sao']):
            return ""
        
        # Ensure proper punctuation
        question = question.strip()
        if question and not question.endswith('?'):
            question += '?'
            
        # Filter out too short or generic questions
        if len(question) < 10:
            return ""
            
        return question

    def _is_valid_question(self, question: str) -> bool:
        """Validate if a string is actually a question, not a statement with '?' added."""
        # Remove the trailing '?' for analysis
        content = question.rstrip('?').strip()
        
        # Check if it starts with question words (Vietnamese)
        question_starters = [
            'ai', 'g√¨', 'ƒë√¢u', 'khi n√†o', 'bao gi·ªù', 'nh∆∞ th·∫ø n√†o', 'sao', 't·∫°i sao',
            'c√≥', 'c√≥ th·ªÉ', 'c√≥ c·∫ßn', 'c√≥ ƒë∆∞·ª£c', 'c√≥ ph·∫£i', 'l√†m', 'l√†m sao', 'l√†m th·∫ø n√†o',
            'c·∫ßn', 'c·∫ßn g√¨', 'c·∫ßn l√†m', 'ph·∫£i', 'ph·∫£i l√†m', 'gi·∫•y t·ªù n√†o', 'ƒëi·ªÅu ki·ªán',
            'th·ªß t·ª•c', 'quy tr√¨nh', 'chi ph√≠', 'th·ªùi gian', 'h·ªì s∆°', 'ƒë·ªãa ƒëi·ªÉm'
        ]
        
        content_lower = content.lower()
        
        # If it starts with typical question words, it's likely a valid question
        if any(content_lower.startswith(word) for word in question_starters):
            return True
            
        # If it contains question patterns in the middle
        question_patterns = ['c√≥ th·ªÉ', 'c√≥ c·∫ßn', 'bao nhi√™u', 'm·∫•t bao l√¢u', '·ªü ƒë√¢u', 'nh∆∞ th·∫ø n√†o']
        if any(pattern in content_lower for pattern in question_patterns):
            return True
            
        # If it's too declarative (starts with statements), reject it
        declarative_starters = [
            'sau khi', 'c√°n b·ªô', 'gi·∫•y t·ªù', 'theo quy ƒë·ªãnh', 'd·ª±a tr√™n', 'trong tr∆∞·ªùng h·ª£p',
            'vi·ªác', 'ng∆∞·ªùi', 'h·ªì s∆° bao g·ªìm', 'y√™u c·∫ßu', 'quy ƒë·ªãnh', 'tr∆∞·ªùng h·ª£p'
        ]
        
        if any(content_lower.startswith(word) for word in declarative_starters):
            return False
            
        return True  # Default to accepting if unclear

    def _deduplicate_questions(self, questions: List[str]) -> List[str]:
        """Lo·∫°i b·ªè c√¢u h·ªèi tr√πng l·∫∑p ho·∫∑c t∆∞∆°ng t·ª± v·ªõi thu·∫≠t to√°n c·∫£i ti·∫øn."""
        if not questions:
            return []
        
        # For small lists, use the detailed similarity check
        if len(questions) <= 50:
            return self._deduplicate_questions_similarity(questions)
        else:
            # For large lists, use the faster normalized check
            return self._deduplicate_questions_fast(questions)

    def _deduplicate_questions_similarity(self, questions: List[str]) -> List[str]:
        """Detailed similarity-based deduplication for smaller lists."""
        deduplicated = []
        seen_questions = set()
        
        for question in questions:
            # Create a normalized version for comparison
            normalized = re.sub(r'[^\w\s]', '', question.lower())
            normalized = ' '.join(normalized.split())
            
            # Check if this question is similar to any existing one
            is_duplicate = False
            for seen in seen_questions:
                # Jaccard similarity check based on common words
                seen_words = set(seen.split())
                new_words = set(normalized.split())
                
                if len(seen_words & new_words) / len(seen_words | new_words) > 0.7:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                deduplicated.append(question)
                seen_questions.add(normalized)
                
        return deduplicated

    def _deduplicate_questions_fast(self, questions: List[str]) -> List[str]:
        """Fast normalized deduplication for larger lists."""
        deduplicated = []
        seen_normalized = set()
        
        for question in questions:
            # Normalize: remove punctuation, lowercase, remove extra spaces
            normalized = re.sub(r'[^\w\s]', '', question.lower())
            normalized = ' '.join(normalized.split())
            
            if normalized not in seen_normalized:
                deduplicated.append(question)
                seen_normalized.add(normalized)
                
        return deduplicated

    def generate_comprehensive_questions(self, doc: Dict[str, Any]) -> Dict[str, Any]:
        """
        Sinh c√¢u h·ªèi ƒëa kh√≠a c·∫°nh cho m·ªôt document.
        ƒê√¢y l√† method ch√≠nh th·ª±c hi·ªán thu·∫≠t to√°n ma tr·∫≠n chunk x persona.
        """
        metadata = doc.get('metadata', {})
        title = metadata.get('title', 'Th·ªß t·ª•c')
        content_chunks = doc.get('content_chunks', [])
        
        if not content_chunks:
            logger.warning(f"‚ö†Ô∏è No content_chunks found for document: {title}")
            return self._generate_fallback_questions(title, metadata)
        
        logger.info(f"üìä Analyzing {len(content_chunks)} chunks for: {title}")
        
        all_questions = []
        chunk_persona_stats = defaultdict(int)
        
        # B∆Ø·ªöC 1: L·∫∑p qua t·ª´ng content_chunk
        for i, chunk in enumerate(content_chunks):
            logger.info(f"   üîç Processing chunk {i+1}: {chunk.get('section_title', 'Untitled')}")
            
            # B∆Ø·ªöC 2: Ph√¢n lo·∫°i chunk thu·ªôc aspects n√†o
            aspects = self._classify_chunk_aspects(chunk)
            logger.info(f"      Aspects: {aspects}")
            
            # B∆Ø·ªöC 3: X√°c ƒë·ªãnh personas ph√π h·ª£p
            relevant_personas = self._get_relevant_personas(aspects)
            logger.info(f"      Relevant personas: {relevant_personas}")
            
            # B∆Ø·ªöC 4: Sinh c√¢u h·ªèi cho t·ª´ng c·∫∑p (chunk, persona)
            chunk_questions = []
            personas_success = 0
            for persona_key in relevant_personas:
                persona_questions = self._generate_questions_for_chunk_persona(
                    chunk, persona_key, title, max_questions=2
                )
                
                if persona_questions:
                    chunk_questions.extend(persona_questions)
                    chunk_persona_stats[f"{chunk.get('section_title', 'Unknown')} x {persona_key}"] = len(persona_questions)
                    personas_success += 1
                    logger.info(f"         ‚úÖ Generated {len(persona_questions)} questions from {persona_key}")
                else:
                    logger.info(f"         ‚ö†Ô∏è No questions generated from {persona_key}")
                
                # Small delay to avoid overwhelming the LLM
                time.sleep(0.2)
            
            success_rate = (personas_success / len(relevant_personas)) * 100 if relevant_personas else 0
            all_questions.extend(chunk_questions)
            logger.info(f"      üìù Chunk total: {len(chunk_questions)} questions (Success: {success_rate:.1f}%)")
        
        # B∆Ø·ªöC 5: Lo·∫°i b·ªè tr√πng l·∫∑p v√† t·ªïng h·ª£p
        logger.info(f"üîÑ Deduplicating from {len(all_questions)} total questions...")
        deduplicated_questions = self._deduplicate_questions(all_questions)
        
        deduplication_effectiveness = (len(all_questions) - len(deduplicated_questions)) / len(all_questions) * 100 if all_questions else 0
        logger.info(f"   üìä Deduplication: {len(all_questions)} ‚Üí {len(deduplicated_questions)} ({deduplication_effectiveness:.1f}% duplicates removed)")
        
        # Generate main question
        main_question = self._generate_main_question_enhanced(title, metadata, deduplicated_questions)
        
        logger.info(f"‚úÖ Final result: 1 main + {len(deduplicated_questions)} variants = {1 + len(deduplicated_questions)} total")
        
        return {
            "main_question": main_question,
            "question_variants": deduplicated_questions,
            "generation_stats": {
                "total_chunks_processed": len(content_chunks),
                "total_questions_generated": len(all_questions),
                "questions_after_deduplication": len(deduplicated_questions),
                "deduplication_effectiveness_percent": round(deduplication_effectiveness, 1),
                "chunk_persona_breakdown": dict(chunk_persona_stats),
                "personas_activated": len(set(persona_key.split(' x ')[1] for persona_key in chunk_persona_stats.keys() if ' x ' in persona_key)),
                "aspects_covered": len(set().union(*[self._classify_chunk_aspects(chunk) for chunk in content_chunks])),
                "generation_method": "chunk_x_persona_matrix_v4_improved"
            }
        }

    def _generate_main_question_enhanced(self, title: str, metadata: Dict, variants: List[str]) -> str:
        """Generate a comprehensive main question based on variants."""
        if variants:
            # Use the first high-quality variant as inspiration for main question
            first_variant = variants[0] if variants else ""
            if title.lower() in first_variant.lower():
                return first_variant
        
        # Fallback to a generic but proper main question
        return f"Th·ªß t·ª•c {title} ƒë∆∞·ª£c th·ª±c hi·ªán nh∆∞ th·∫ø n√†o?"

    def _generate_fallback_questions(self, title: str, metadata: Dict) -> Dict[str, Any]:
        """Fallback when no content_chunks available."""
        logger.warning(f"Using fallback question generation for: {title}")
        
        fallback_variants = [
            f"L√†m {title} c·∫ßn gi·∫•y t·ªù g√¨?",
            f"Chi ph√≠ {title} l√† bao nhi√™u?",
            f"Th·ªùi gian x·ª≠ l√Ω {title} m·∫•t bao l√¢u?", 
            f"L√†m {title} ·ªü ƒë√¢u?",
            f"ƒêi·ªÅu ki·ªán ƒë·ªÉ ƒë∆∞·ª£c {title}?",
            f"Quy tr√¨nh {title} nh∆∞ th·∫ø n√†o?",
            f"C√≥ th·ªÉ l√†m {title} online kh√¥ng?",
            f"L∆∞u √Ω khi l√†m {title}?"
        ]
        
        return {
            "main_question": f"Th·ªß t·ª•c {title} ƒë∆∞·ª£c th·ª±c hi·ªán nh∆∞ th·∫ø n√†o?",
            "question_variants": fallback_variants,
            "generation_stats": {
                "fallback_mode": True,
                "reason": "No content_chunks available"
            }
        }

    # Additional utility methods from the original script (simplified versions)
    
    def _detect_collection_from_path(self, file_path: str) -> str:
        """Detect collection name from file path."""
        path_lower = file_path.lower()
        
        if 'ho_tich_cap_xa' in path_lower:
            return 'ho_tich_cap_xa'
        elif 'chung_thuc' in path_lower:
            return 'chung_thuc'
        elif 'nuoi_con_nuoi' in path_lower:
            return 'nuoi_con_nuoi'
        elif 'hanh_chinh' in path_lower:
            return 'administrative_procedures'
        elif 'kinh_doanh' in path_lower:
            return 'business_procedures'
        elif 'dat_dai' in path_lower:
            return 'land_procedures'
        elif 'xay_dung' in path_lower:
            return 'construction_procedures'
        elif 'tu_phap' in path_lower:
            return 'judicial_procedures'
        else:
            return 'general_procedures'

    def analyze_document_metadata_enhanced(self, doc: Dict[str, Any], file_path: str) -> Dict[str, Any]:
        """Enhanced metadata analysis to create rich smart_filters with detailed logic."""
        metadata = doc.get('metadata', {})
        title = metadata.get('title', '')
        code = metadata.get('code', '')
        
        # Create comprehensive smart_filters with detailed analysis
        smart_filters = {
            "exact_title": [title] if title else [],
            "title_keywords": self._extract_title_keywords(title),
            "procedure_code": [code] if code else [],
            "agency": self._extract_agency(metadata),
            "agency_level": self._determine_agency_level(metadata, file_path),
            "cost_type": self._determine_cost_type(metadata),
            "processing_speed": self._determine_processing_speed(metadata),
            "applicant_type": metadata.get('applicant_type', ['C√° nh√¢n'])
        }
        
        # Create key_attributes with proper mapping
        key_attributes = {
            "speed": self._map_processing_speed(smart_filters["processing_speed"]),
            "cost": self._map_cost_type(smart_filters["cost_type"]),
            "level": self._map_agency_level(smart_filters["agency_level"]),
            "applicant_scope": smart_filters["applicant_type"]
        }
        
        return {
            'metadata': metadata,
            'smart_filters': smart_filters,
            'key_attributes': key_attributes,
            'confidence_threshold': 0.8,
            'priority_score': 1.2  # Higher priority for multi-aspect generated content
        }

    def _extract_agency(self, metadata: Dict) -> List[str]:
        """Extract agency from metadata with detailed parsing."""
        agency = metadata.get('executing_agency', '')
        if not agency:
            return []
        return [agency]

    def _determine_agency_level(self, metadata: Dict, file_path: str) -> List[str]:
        """Determine agency level from metadata and path with detailed logic."""
        agency = metadata.get('executing_agency', '').lower()
        path_lower = file_path.lower()
        
        if 'c·∫•p x√£' in agency or 'cap_xa' in path_lower:
            return ['commune']
        elif 'c·∫•p huy·ªán' in agency:
            return ['district'] 
        elif 'c·∫•p t·ªânh' in agency:
            return ['province']
        else:
            return ['commune']  # Default

    def _determine_cost_type(self, metadata: Dict) -> List[str]:
        """Determine cost type from metadata with detailed analysis."""
        fee_text = metadata.get('fee_text', '') or metadata.get('fee', '')
        if not fee_text:
            return []
        
        fee_lower = fee_text.lower()
        if 'mi·ªÖn ph√≠' in fee_lower or 'kh√¥ng ph√≠' in fee_lower:
            return ['free']
        else:
            return ['paid']

    def _determine_processing_speed(self, metadata: Dict) -> List[str]:
        """Determine processing speed from metadata with detailed parsing."""
        processing_time = metadata.get('processing_time_text', '') or metadata.get('processing_time', '')
        if not processing_time:
            return []
        
        time_lower = processing_time.lower()
        if 'ngay' in time_lower and ('nh·∫≠n' in time_lower or 'khi' in time_lower):
            return ['immediate']
        elif 'ng√†y' in time_lower or 'tu·∫ßn' in time_lower:
            return ['multiple_days']  
        elif 'th√°ng' in time_lower:
            return ['slow']
        else:
            return ['multiple_days']

    def _map_processing_speed(self, speed_list: List[str]) -> str:
        """Map processing speed to key_attributes format."""
        if not speed_list:
            return "unknown"
        speed = speed_list[0]
        mapping = {
            'immediate': 'immediate',
            'multiple_days': 'multi_day', 
            'slow': 'slow'
        }
        return mapping.get(speed, 'multi_day')

    def _map_cost_type(self, cost_list: List[str]) -> str:
        """Map cost type to key_attributes format."""
        if not cost_list:
            return "unknown"
        return cost_list[0]

    def _map_agency_level(self, level_list: List[str]) -> str:
        """Map agency level to key_attributes format."""
        if not level_list:
            return "commune"
        return level_list[0]

    def _extract_title_keywords(self, title: str) -> List[str]:
        """Extract keywords from title."""
        if not title:
            return []
        
        # Simple keyword extraction
        words = title.lower().split()
        keywords = [word for word in words if len(word) > 2 and word not in ['th·ªß', 't·ª•c', 'c·ªßa', 'cho', 'v·ªõi', 'theo']]
        return keywords

    def generate_all_smart_examples(self, force_rebuild: bool = False) -> int:
        """
        Main method to process all documents and generate comprehensive router examples.
        """
        if not self.documents_dir.exists():
            logger.error(f"‚ùå Documents directory not found: {self.documents_dir}")
            return 0

        # Find all JSON files
        json_files = list(self.documents_dir.rglob("*.json"))
        if not json_files:
            logger.error(f"‚ùå No JSON files found in {self.documents_dir}")
            return 0

        logger.info(f"üîç Found {len(json_files)} JSON files to process")
        
        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        processed_files = 0
        total_examples = 0
        
        for i, json_file in enumerate(json_files):
            logger.info("=" * 80)
            logger.info(f"Processing file {i+1}/{len(json_files)}: {json_file.name}")
            logger.info("=" * 80)
            
            # Create mirrored directory structure like V3
            relative_path = json_file.relative_to(self.documents_dir)
            output_file = self.output_dir / relative_path
            
            # Ensure output maintains the same directory structure as source
            output_file.parent.mkdir(parents=True, exist_ok=True)

            if not force_rebuild and output_file.exists():
                logger.info(f"   ‚è© Skipping, router file already exists. Use --force to overwrite.")
                continue

            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    doc = json.load(f)
                
                # STEP 1: Enhanced metadata analysis
                analysis = self.analyze_document_metadata_enhanced(doc, str(json_file))
                collection_name = self._detect_collection_from_path(str(json_file))
                
                # STEP 2: Multi-aspect comprehensive question generation
                questions = self.generate_comprehensive_questions(doc)
                
                # STEP 3: Create structured router data
                router_data = {
                    'metadata': {
                        'title': doc.get('metadata', {}).get('title', ''),
                        'code': doc.get('metadata', {}).get('code', ''),
                        'collection': collection_name,
                        'source_document': str(relative_path),
                        'generated_by': 'multi_aspect_persona_generator_v4.0',
                        'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                        'generation_method': 'chunk_x_persona_matrix'
                    },
                    'main_question': questions['main_question'],
                    'question_variants': questions.get('question_variants', []),
                    'smart_filters': analysis['smart_filters'],
                    'key_attributes': analysis['key_attributes'],
                    'expected_collection': collection_name,
                    'confidence_threshold': analysis.get('confidence_threshold', 0.8),
                    'priority_score': analysis.get('priority_score', 1.2),
                    'generation_stats': questions.get('generation_stats', {})
                }
                
                # Ensure output directory exists
                output_file.parent.mkdir(parents=True, exist_ok=True)
                
                # Save router data
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(router_data, f, ensure_ascii=False, indent=2)
                
                processed_files += 1
                num_questions = 1 + len(questions.get('question_variants', []))
                total_examples += num_questions
                
                logger.info("üìä GENERATION RESULTS:")
                logger.info(f"   üìù Questions generated: {num_questions} total (1 main + {len(questions.get('question_variants', []))} variants)")
                logger.info(f"   üíæ Saved to: {output_file.name}")
                if 'generation_stats' in questions:
                    stats = questions['generation_stats']
                    logger.info(f"   üîç Chunks processed: {stats.get('total_chunks_processed', 0)}")
                    logger.info(f"   üéØ Success rate: {stats.get('overall_success_rate', 'N/A')}")
                    logger.info(f"   üìà Questions: {stats.get('questions_after_deduplication', 0)}/{stats.get('total_questions_generated', 0)} after deduplication")
                
                # Brief pause between files
                time.sleep(0.5)

            except Exception as e:
                logger.error(f"   ‚ùå Error processing {json_file.name}: {e}")
                continue
        
        # Generate comprehensive summary
        self._generate_comprehensive_summary(processed_files, total_examples, json_files)
        return processed_files

    def _generate_comprehensive_summary(self, processed_files: int, total_examples: int, json_files: List[Path]):
        """Generate comprehensive summary report."""
        collections = {}
        for json_file in json_files:
            collection = self._detect_collection_from_path(str(json_file))
            collections[collection] = collections.get(collection, 0) + 1
        
        summary = {
            'generator_info': {
                'version': 'multi_aspect_persona_v4.0',
                'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'method': 'chunk_x_persona_matrix_generation',
                'llm_model': 'PhoGPT-4B-Chat',
                'personas_used': list(self.personas.keys()),
                'target_questions_per_document': '30+',
                'approach': 'comprehensive_multi_perspective_generation'
            },
            'statistics': {
                'total_files_processed': processed_files,
                'total_source_files': len(json_files),
                'total_examples_generated': total_examples,
                'avg_questions_per_document': round(total_examples / processed_files, 2) if processed_files > 0 else 0,
                'collections_distribution': collections,
                'success_rate_percent': round((processed_files / len(json_files)) * 100, 2) if json_files else 0
            },
            'quality_metrics': {
                'generation_approach': 'multi_aspect_multi_persona',
                'deduplication': 'advanced_similarity_based',
                'context_coverage': 'full_content_chunks_analysis',
                'persona_diversity': len(self.personas),
                'aspect_coverage': list(self.aspect_persona_mapping.keys()),
                'target_achievement': 'optimized_for_30plus_questions'
            },
            'personas_definition': self.personas,
            'aspect_persona_mapping': self.aspect_persona_mapping
        }
        
        summary_file = self.output_dir / "multi_aspect_generation_summary_v4.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info("\n" + "=" * 80)
        logger.info("üìä COMPREHENSIVE GENERATION SUMMARY")
        logger.info("=" * 80)
        logger.info(f"üéØ Target: 30+ questions per document")
        logger.info(f"üìà Achieved: {summary['statistics']['avg_questions_per_document']} questions per document on average")
        logger.info(f"üìÅ Summary saved to: {summary_file}")
        logger.info("=" * 80)


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description='Generate comprehensive multi-aspect router examples V4')
    parser.add_argument('--force', action='store_true', help='Force rebuild existing examples')
    parser.add_argument('--docs', type=str, default='data/documents', help='Documents directory')
    parser.add_argument('--output', type=str, default='data/router_examples_smart_v3', help='Output directory')
    
    args = parser.parse_args()
    
    try:
        generator = MultiAspectPersonaRouter(args.docs, args.output)
        
        logger.info("üöÄ Starting Multi-Aspect Multi-Persona Router Generation V4...")
        logger.info("üéØ Target: 30+ comprehensive questions per document")
        logger.info("üîß Method: Chunk √ó Persona Matrix Generation")
        
        start_time = time.time()
        
        processed_count = generator.generate_all_smart_examples(force_rebuild=args.force)
        
        end_time = time.time()
        duration = end_time - start_time
        
        logger.info("\n" + "=" * 80)
        logger.info("‚úÖ MULTI-ASPECT GENERATION COMPLETED!")
        logger.info("=" * 80)
        logger.info(f"üìä Files processed: {processed_count}")
        logger.info(f"‚è±Ô∏è Total duration: {duration:.2f} seconds")
        logger.info(f"üìÅ Output directory: {args.output}")
        logger.info(f"üéØ Achievement: Comprehensive multi-perspective question generation")
        logger.info("=" * 80)
        
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Process interrupted by user")
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
