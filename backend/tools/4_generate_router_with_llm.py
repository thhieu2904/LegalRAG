#!/usr/bin/env python3
"""
LLM-Powered Smart Router Examples Generator for LegalRAG
=========================================================

NÃ¢ng cáº¥p tool táº¡o router examples báº±ng cÃ¡ch sá»­ dá»¥ng LLM (PhoGPT-4B) Ä‘á»ƒ
sinh ra cÃ¡c cÃ¢u há»i Ä‘a dáº¡ng vÃ  cÃ³ chiá»u sÃ¢u ngá»¯ nghÄ©a hÆ¡n, thay tháº¿
cho cÃ¡c template cá»©ng.

Logic cá»‘t lÃµi:
1.  Giá»¯ láº¡i toÃ n bá»™ pháº§n phÃ¢n tÃ­ch metadata vÃ  smart filters tá»« tool cÅ©.
2.  Sá»­ dá»¥ng LLMService Ä‘á»ƒ Ä‘á»c ná»™i dung tÃ i liá»‡u vÃ  sinh ra bá»™ cÃ¢u há»i
    phong phÃº dá»±a trÃªn ká»¹ thuáº­t prompt "Ä‘Ã³ng vai".
3.  Tá»± Ä‘á»™ng hÃ³a viá»‡c táº¡o dá»¯ liá»‡u cháº¥t lÆ°á»£ng cao cho Router.

Usage:
    cd backend
    python tools/4_generate_router_with_llm.py
    python tools/4_generate_router_with_llm.py --force  # Rebuild existing
"""

import sys
import os
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
import argparse
import time

# --- SETUP PATH ---
backend_dir = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(backend_dir))
# --- END SETUP PATH ---

# --- IMPORT CÃC THÃ€NH PHáº¦N Tá»ª Dá»° ÃN ---
from app.services.language_model import LLMService
from app.core.config import settings

# --- SETUP LOGGING ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SmartRouterLLMGenerator:
    """
    LLM-powered generator cho smart router examples
    """
    def __init__(self, documents_dir: str, output_dir: str, llm_service: LLMService):
        self.documents_dir = Path(documents_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.llm_service = llm_service
        
        # Collection mapping
        self.collection_mapping = {
            'chung_thuc': ['chung_thuc', 'chung-thuc', 'cÃ´ng chá»©ng', 'chung thá»±c'],
            'ho_tich_cap_xa': ['ho_tich', 'há»™ tá»‹ch', 'khai sinh', 'cap_xa', 'cáº¥p xÃ£', 'á»§y ban'],
            'nuoi_con_nuoi': ['nuoi_con_nuoi', 'nuÃ´i con nuÃ´i', 'nháº­n nuÃ´i', 'con nuÃ´i']
        }
        
        logger.info("ðŸ§  LLM-Powered Generator initialized.")

    def _detect_collection_from_path(self, file_path: str) -> str:
        """Detect collection from file path"""
        path_lower = file_path.lower()
        
        # Check file path for collection indicators
        for collection, keywords in self.collection_mapping.items():
            for keyword in keywords:
                if keyword.replace('_', ' ') in path_lower or keyword.replace(' ', '_') in path_lower:
                    return collection
        
        # Default fallback based on path components
        if 'chung' in path_lower:
            return 'chung_thuc'
        elif 'ho_tich' in path_lower or 'khai_sinh' in path_lower:
            return 'ho_tich_cap_xa'
        elif 'nuoi' in path_lower:
            return 'nuoi_con_nuoi'
        
        return 'chung_thuc'  # Default

    def analyze_document_metadata(self, doc: Dict[str, Any]) -> Dict[str, Any]:
        """PhÃ¢n tÃ­ch metadata cá»§a document Ä‘á»ƒ táº¡o smart filters"""
        metadata = doc.get('metadata', {})
        
        # Basic metadata extraction
        title = metadata.get('title', '')
        code = metadata.get('code', '')
        applicant_type = metadata.get('applicant_type', [])
        executing_agency = metadata.get('executing_agency', '')
        
        # Smart filters based on metadata
        smart_filters = {}
        
        # Applicant type filter
        if applicant_type:
            smart_filters['applicant_type'] = applicant_type
        
        # Agency filter
        if executing_agency:
            agency_normalized = executing_agency.lower()
            if 'ubnd' in agency_normalized or 'á»§y ban' in agency_normalized:
                smart_filters['agency_level'] = 'local'
            elif 'bá»™' in agency_normalized or 'trung Æ°Æ¡ng' in agency_normalized:
                smart_filters['agency_level'] = 'central'
        
        # Fee detection
        fee_info = metadata.get('fee_text', '') or metadata.get('fee', '')
        if fee_info:
            fee_normalized = fee_info.lower()
            if 'miá»…n' in fee_normalized or 'khÃ´ng' in fee_normalized:
                smart_filters['has_fee'] = False
            elif 'Ä‘á»“ng' in fee_normalized or 'vnÄ‘' in fee_normalized:
                smart_filters['has_fee'] = True
        
        # Processing time
        processing_time = metadata.get('processing_time_text', '') or metadata.get('processing_time', '')
        if processing_time:
            time_normalized = processing_time.lower()
            if 'ngay' in time_normalized:
                smart_filters['processing_speed'] = 'fast'
            elif 'tuáº§n' in time_normalized or 'thÃ¡ng' in time_normalized:
                smart_filters['processing_speed'] = 'slow'
        
        return {
            'metadata': metadata,
            'smart_filters': smart_filters,
            'confidence_threshold': 0.75,
            'priority_score': 1.0
        }

    def generate_questions_with_llm(self, doc: Dict[str, Any]) -> Dict[str, Any]:
        """
        Sá»­ dá»¥ng LLM Ä‘á»ƒ sinh ra cÃ¢u há»i chÃ­nh vÃ  cÃ¡c biáº¿n thá»ƒ.
        """
        metadata = doc.get('metadata', {})
        document_title = metadata.get('title', 'Thá»§ tá»¥c chÆ°a xÃ¡c Ä‘á»‹nh')
        
        # Táº¡o má»™t chuá»—i tÃ³m táº¯t ná»™i dung tÃ i liá»‡u Ä‘á»ƒ Ä‘Æ°a vÃ o prompt
        document_content_summary = self._summarize_document_for_prompt(doc)

        # Prompt tá»‘i Æ°u hÃ³a - ngáº¯n gá»n vÃ  hiá»‡u quáº£
        user_query = f"""Táº¡o 10 cÃ¢u há»i khÃ¡c nhau vá» thá»§ tá»¥c "{document_title}":

THÃ”NG TIN: {document_content_summary}

CÃC LOáº I CÃ‚U Há»ŽI Cáº¦N Táº O:
- Thá»§ tá»¥c lÃ  gÃ¬? Ai lÃ m Ä‘Æ°á»£c?
- Cáº§n giáº¥y tá» gÃ¬?
- LÃ m á»Ÿ Ä‘Ã¢u? 
- Chi phÃ­ bao nhiÃªu?
- Máº¥t bao lÃ¢u?
- LÃ m online Ä‘Æ°á»£c khÃ´ng?
- Nháº­n káº¿t quáº£ tháº¿ nÃ o?
- Äiá»u kiá»‡n gÃ¬?
- Quy trÃ¬nh ra sao?
- LÆ°u Ã½ gÃ¬?

Táº O 10 CÃ‚U Há»ŽI NGáº®N Gá»ŒN:"""

        # System prompt ngáº¯n gá»n
        system_prompt = "Táº¡o cÃ¢u há»i ngáº¯n. Má»—i cÃ¢u 1 dÃ²ng, Ä‘Ã¡nh sá»‘. KhÃ´ng láº·p láº¡i."
        
        # ThÃªm retry logic Ä‘á»ƒ xá»­ lÃ½ LLM khÃ´ng á»•n Ä‘á»‹nh
        max_retries = 2
        for attempt in range(max_retries):
            try:
                logger.info(f"   ðŸ¤– Calling LLM (Attempt {attempt + 1}/{max_retries}) for: '{document_title}'")
                response_data = self.llm_service.generate_response(
                    user_query=user_query,
                    max_tokens=500,  # Giáº£m Ä‘á»ƒ trÃ¡nh repetition
                    temperature=0.3,  # TÄƒng má»™t chÃºt Ä‘á»ƒ cÃ³ Ä‘a dáº¡ng
                    system_prompt=system_prompt  # Sá»­ dá»¥ng system prompt riÃªng
                )
                
                # TrÃ­ch xuáº¥t vÃ  lÃ m sáº¡ch chuá»—i JSON tá»« pháº£n há»“i cá»§a LLM
                response_text = response_data.get('response', '{}')
                logger.info(f"   ðŸ“ LLM Response (first 200 chars): {response_text[:200]}...")
                
                generated_data = self._extract_json_from_llm_response(response_text, document_title)
                
                if generated_data:
                    # Äáº£m báº£o cÃ³ Ä‘á»§ key vÃ  value lÃ  list
                    if 'question_variants' not in generated_data or not isinstance(generated_data['question_variants'], list):
                        generated_data['question_variants'] = []
                    if 'main_question' not in generated_data:
                         generated_data['main_question'] = generated_data['question_variants'][0] if generated_data['question_variants'] else document_title
                    
                    # Clean up questions
                    generated_data['main_question'] = self._clean_question(generated_data['main_question'])
                    generated_data['question_variants'] = [
                        self._clean_question(q) for q in generated_data['question_variants'] if q.strip()
                    ]
                    
                    logger.info(f"   âœ… LLM generated {1 + len(generated_data['question_variants'])} questions.")
                    return generated_data  # ThÃ nh cÃ´ng - thoÃ¡t khá»i retry loop
                else:
                    raise ValueError("No valid questions found in LLM response")

            except Exception as e:
                logger.warning(f"   âš ï¸ Attempt {attempt + 1} failed for '{document_title}': {e}")
                if attempt + 1 == max_retries:
                    logger.error(f"   âŒ All attempts failed for '{document_title}'. Using fallback.")
                    # Fallback: Tráº£ vá» má»™t cÃ¢u há»i cÆ¡ báº£n náº¿u LLM tháº¥t báº¡i hoÃ n toÃ n
                    return {
                        "main_question": f"Thá»§ tá»¥c {document_title} cáº§n nhá»¯ng gÃ¬?",
                        "question_variants": [
                            f"Há»“ sÆ¡ {document_title} ra sao?",
                            f"Chi phÃ­ {document_title} lÃ  bao nhiÃªu?",
                            f"LÃ m {document_title} á»Ÿ Ä‘Ã¢u?"
                        ]
                    }
                time.sleep(1)  # Chá» 1 giÃ¢y trÆ°á»›c khi thá»­ láº¡i
        
        # Shouldn't reach here, but just in case
        return {
            "main_question": f"Thá»§ tá»¥c {document_title} cáº§n nhá»¯ng gÃ¬?",
            "question_variants": [
                f"Há»“ sÆ¡ {document_title} ra sao?",
                f"Chi phÃ­ {document_title} lÃ  bao nhiÃªu?"
            ]
        }

    def _clean_question(self, question: str) -> str:
        """Clean up generated questions"""
        if not question:
            return question
            
        # Remove numbering, bullet points, and extra whitespace
        question = re.sub(r'^[\d\-\*\+\.\)]\s*', '', question).strip()
        
        # Remove quotes and extra whitespace
        question = question.strip().strip('"').strip("'").strip()
        
        # Ensure question ends with question mark
        if question and not question.endswith('?'):
            question += '?'
            
        return question

    def _summarize_document_for_prompt(self, doc: Dict[str, Any]) -> str:
        """Táº¡o má»™t báº£n tÃ³m táº¯t chi tiáº¿t tá»« file JSON Ä‘á»ƒ lÃ m input cho LLM."""
        parts = []
        metadata = doc.get("metadata", {})
        
        # Basic info vá»›i nhiá»u chi tiáº¿t hÆ¡n
        applicant_types = metadata.get('applicant_type', ['CÃ¡ nhÃ¢n'])
        if isinstance(applicant_types, list):
            parts.append(f"Äá»‘i tÆ°á»£ng: {', '.join(applicant_types)}")
        else:
            parts.append(f"Äá»‘i tÆ°á»£ng: {applicant_types}")
        
        parts.append(f"CÆ¡ quan thá»±c hiá»‡n: {metadata.get('executing_agency', 'N/A')}")
        parts.append(f"Thá»i gian xá»­ lÃ½: {metadata.get('processing_time_text', metadata.get('processing_time', 'N/A'))}")
        parts.append(f"Lá»‡ phÃ­: {metadata.get('fee_text', metadata.get('fee', 'N/A'))}")
        
        # ThÃªm thÃ´ng tin vá» submission method
        submission_method = metadata.get('submission_method', [])
        if submission_method:
            if isinstance(submission_method, list):
                parts.append(f"CÃ¡ch ná»™p há»“ sÆ¡: {', '.join(submission_method)}")
            else:
                parts.append(f"CÃ¡ch ná»™p há»“ sÆ¡: {submission_method}")
        
        # ThÃªm thÃ´ng tin vá» result delivery
        result_delivery = metadata.get('result_delivery', [])
        if result_delivery:
            if isinstance(result_delivery, list):
                parts.append(f"CÃ¡ch nháº­n káº¿t quáº£: {', '.join(result_delivery)}")
            else:
                parts.append(f"CÃ¡ch nháº­n káº¿t quáº£: {result_delivery}")
        
        # Content chunks - get key information tá»« nhiá»u sections
        content_chunks = doc.get("content_chunks", [])
        if content_chunks:
            # TÃ¬m section há»“ sÆ¡
            hoso_chunk = next((c for c in content_chunks if "há»“ sÆ¡" in c.get("section_title", "").lower()), None)
            if hoso_chunk:
                content = hoso_chunk.get('content', '')[:400]  # TÄƒng giá»›i háº¡n
                parts.append(f"ThÃ nh pháº§n há»“ sÆ¡: {content}...")
            
            # TÃ¬m section quy trÃ¬nh
            quytrÃ¬nh_chunk = next((c for c in content_chunks if any(keyword in c.get("section_title", "").lower() 
                                                                    for keyword in ["quy trÃ¬nh", "thá»§ tá»¥c", "cÃ¡ch thá»©c"])), None)
            if quytrÃ¬nh_chunk and quytrÃ¬nh_chunk != hoso_chunk:
                content = quytrÃ¬nh_chunk.get('content', '')[:300]
                parts.append(f"Quy trÃ¬nh thá»±c hiá»‡n: {content}...")
            
            # TÃ¬m section Ä‘iá»u kiá»‡n/yÃªu cáº§u
            dieukien_chunk = next((c for c in content_chunks if any(keyword in c.get("section_title", "").lower() 
                                                                   for keyword in ["Ä‘iá»u kiá»‡n", "yÃªu cáº§u", "Ä‘á»‘i tÆ°á»£ng"])), None)
            if dieukien_chunk and dieukien_chunk != hoso_chunk and dieukien_chunk != quytrÃ¬nh_chunk:
                content = dieukien_chunk.get('content', '')[:200]
                parts.append(f"Äiá»u kiá»‡n/YÃªu cáº§u: {content}...")
            
            # Náº¿u khÃ´ng tÃ¬m Ä‘Æ°á»£c sections cá»¥ thá»ƒ, láº¥y chunk Ä‘áº§u tiÃªn
            if not hoso_chunk and not quytrÃ¬nh_chunk and not dieukien_chunk and content_chunks:
                first_chunk = content_chunks[0]
                content = first_chunk.get('content', '')[:300]
                parts.append(f"ThÃ´ng tin chÃ­nh: {content}...")

        return ". ".join(parts)
        
    def _extract_json_from_llm_response(self, text: str) -> Optional[str]:
        """TrÃ­ch xuáº¥t vÃ  tÃ¡i táº¡o JSON tá»« text response cá»§a LLM."""
        if not text:
            return None
            
        # Strategy 1: TÃ¬m JSON sáºµn cÃ³
        json_match = re.search(r'\{.*?"main_question".*?"question_variants".*?\}', text, re.DOTALL)
        if json_match:
            return json_match.group(0)
        
        # Strategy 2: Extract tá»« text response thÃ´ng thÆ°á»ng
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        questions = []
        seen_questions = set()  # Äá»ƒ trÃ¡nh trÃ¹ng láº·p
        
        # Lá»c ra nhá»¯ng dÃ²ng chá»©a cÃ¢u há»i (káº¿t thÃºc báº±ng dáº¥u ?)
        for line in lines:
            # Remove numbering vÃ  bullet points
            cleaned_line = re.sub(r'^[\d\-\*\+\.\)]\s*', '', line).strip()
            if cleaned_line.endswith('?') and len(cleaned_line) > 5:
                # TrÃ¡nh trÃ¹ng láº·p
                if cleaned_line.lower() not in seen_questions:
                    questions.append(cleaned_line)
                    seen_questions.add(cleaned_line.lower())
        
        if len(questions) >= 1:
            # Chá»n cÃ¢u há»i chÃ­nh - Æ°u tiÃªn cÃ¢u há»i tá»•ng quan
            main_question = questions[0]
            
            # TÃ¬m cÃ¢u há»i tá»•ng quan tá»‘t hÆ¡n náº¿u cÃ³
            for q in questions[:3]:  # Chá»‰ xem 3 cÃ¢u Ä‘áº§u
                if any(keyword in q.lower() for keyword in ["lÃ  gÃ¬", "nhÆ° tháº¿ nÃ o", "thá»§ tá»¥c", "quy trÃ¬nh"]):
                    main_question = q
                    break
            
            # Láº¥y táº¥t cáº£ cÃ¢u há»i cÃ²n láº¡i lÃ m variants, tá»‘i Ä‘a 10 cÃ¢u
            question_variants = [q for q in questions if q != main_question][:10]
            
            # Táº¡o JSON object
            result = {
                "main_question": main_question,
                "question_variants": question_variants
            }
            return json.dumps(result, ensure_ascii=False)
        
        # Strategy 3: TÃ¬m patterns phá»• biáº¿n (fallback)
        main_q_patterns = [
            r'cÃ¢u há»i chÃ­nh[:\-\s]*(.+\?)',
            r'main[_\s]question[:\-\s]*(.+\?)',
            r'(?:^|\n)(.+\?)',  # First question mark
        ]
        
        for pattern in main_q_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                main_question = match.group(1).strip()
                
                # TÃ¬m thÃªm cÃ¢u há»i khÃ¡c
                remaining_text = text[match.end():]
                variant_questions = re.findall(r'(.+\?)', remaining_text)
                
                # Clean up variants vÃ  trÃ¡nh trÃ¹ng láº·p
                variants = []
                seen = set()
                for q in variant_questions[:8]:  # Láº¥y tá»‘i Ä‘a 8 variants
                    cleaned = re.sub(r'^[\d\-\*\+\.\)]\s*', '', q).strip()
                    if cleaned and cleaned != main_question and cleaned.lower() not in seen:
                        variants.append(cleaned)
                        seen.add(cleaned.lower())
                
                result = {
                    "main_question": main_question,
                    "question_variants": variants
                }
                return json.dumps(result, ensure_ascii=False)
        
        return None

    def generate_all_smart_examples(self, force_rebuild: bool = False) -> int:
        """
        Sinh táº¥t cáº£ smart router examples báº±ng LLM.
        """
        logger.info("ðŸŽ¯ Generating smart router examples using LLM...")
        
        if not self.documents_dir.exists():
            logger.error(f"âŒ Documents directory not found: {self.documents_dir}")
            return 0
        
        json_files = sorted(list(self.documents_dir.rglob("*.json")))
        if not json_files:
            logger.error("âŒ No JSON documents found")
            return 0
        
        logger.info(f"   ðŸ“„ Found {len(json_files)} JSON files to process.")
        
        total_examples = 0
        processed_files = 0
        
        for i, json_file in enumerate(json_files):
            logger.info("-" * 60)
            logger.info(f"Processing file {i+1}/{len(json_files)}: {json_file.name}")
            
            relative_path = json_file.relative_to(self.documents_dir)
            output_file = self.output_dir / relative_path

            if not force_rebuild and output_file.exists():
                logger.info(f"   â© Skipping, router file already exists. Use --force to overwrite.")
                continue

            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    doc = json.load(f)
                
                # 1. PHÃ‚N TÃCH METADATA
                analysis = self.analyze_document_metadata(doc)
                collection_name = self._detect_collection_from_path(str(json_file))
                
                # 2. SINH CÃ‚U Há»ŽI Báº°NG LLM
                questions = self.generate_questions_with_llm(doc)
                
                # 3. Táº O VÃ€ LÆ¯U FILE ROUTER
                router_data = {
                    'metadata': {
                        'title': doc.get('metadata', {}).get('title', ''),
                        'code': doc.get('metadata', {}).get('code', ''),
                        'collection': collection_name,
                        'source_document': str(relative_path),
                        'generated_by': 'llm_powered_generator_v1.0',
                        'generated_at': time.strftime('%Y-%m-%d %H:%M:%S')
                    },
                    'main_question': questions['main_question'],
                    'question_variants': questions.get('question_variants', []),
                    'smart_filters': analysis['smart_filters'],
                    'expected_collection': collection_name,
                    'confidence_threshold': analysis.get('confidence_threshold', 0.75),
                    'priority_score': analysis.get('priority_score', 1.0)
                }
                
                # Ensure output directory exists
                output_file.parent.mkdir(parents=True, exist_ok=True)
                
                # Save router data
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(router_data, f, ensure_ascii=False, indent=2)
                
                processed_files += 1
                num_questions = 1 + len(questions.get('question_variants', []))
                total_examples += num_questions
                logger.info(f"   ðŸ’¾ Saved {num_questions} examples to {output_file.name}")
                
                # ThÃªm má»™t khoáº£ng nghá»‰ nhá» Ä‘á»ƒ khÃ´ng lÃ m LLM quÃ¡ táº£i
                time.sleep(0.5) 

            except Exception as e:
                logger.error(f"   âš ï¸ Error processing {json_file.name}: {e}")
                continue
        
        # Táº¡o file tÃ³m táº¯t
        self._generate_summary(processed_files, total_examples, json_files)
        return processed_files

    def _generate_summary(self, processed_files: int, total_examples: int, json_files: List[Path]):
        """Táº¡o file tÃ³m táº¯t káº¿t quáº£."""
        collections = {}
        for json_file in json_files:
            collection = self._detect_collection_from_path(str(json_file))
            collections[collection] = collections.get(collection, 0) + 1
        
        summary = {
            'generator_info': {
                'version': 'llm_powered_v1.0',
                'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'llm_model': 'PhoGPT-4B-Chat'
            },
            'statistics': {
                'total_files_processed': processed_files,
                'total_source_files': len(json_files),
                'total_examples_generated': total_examples,
                'collections_distribution': collections
            },
            'quality_metrics': {
                'avg_variants_per_document': total_examples / processed_files if processed_files > 0 else 0,
                'success_rate': (processed_files / len(json_files)) * 100 if json_files else 0
            }
        }
        
        summary_file = self.output_dir / "llm_generation_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nðŸ“Š Summary saved to {summary_file}")
        logger.info(f"   âœ… Processed: {processed_files}/{len(json_files)} files")
        logger.info(f"   ðŸ“ Generated: {total_examples} total examples")
        logger.info(f"   ðŸ“Š Success rate: {summary['quality_metrics']['success_rate']:.1f}%")


def main():
    parser = argparse.ArgumentParser(
        description='Generate smart router examples for LegalRAG using LLM.',
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        '--force',
        action='store_true',
        help='Force rebuild - overwrite existing router examples'
    )
    parser.add_argument(
        '--verify-llm',
        action='store_true',
        help='Only verify LLM service and exit'
    )
    args = parser.parse_args()

    # --- KHá»žI Táº O CÃC THÃ€NH PHáº¦N Cáº¦N THIáº¾T ---
    documents_dir = backend_dir / "data" / "documents"
    router_examples_dir = backend_dir / "data" / "router_examples_smart"
    
    if not documents_dir.exists() and not args.verify_llm:
        logger.error(f"âŒ Documents directory not found: {documents_dir}")
        return 1

    logger.info("ðŸ§  LLM-Powered Smart Router Examples Generator")
    logger.info("=" * 60)
    
    try:
        # Táº£i LLMService Ä‘á»ƒ sá»­ dá»¥ng cho viá»‡c sinh cÃ¢u há»i
        logger.info("ðŸš€ Initializing LLMService (this may take a moment)...")
        llm = LLMService()
        
        if not llm.is_loaded():
            raise RuntimeError("LLM failed to load. Please check config and model path.")
        
        logger.info("âœ… LLMService initialized successfully.")
        
        # Show model info
        model_info = llm.get_model_info()
        logger.info(f"   ðŸ“Š Model size: {model_info['model_size_mb']:.1f}MB")
        logger.info(f"   ðŸ”§ GPU layers: {model_info['model_kwargs'].get('n_gpu_layers', 0)}")
        
        if args.verify_llm:
            logger.info("ðŸ§ª Testing LLM with sample query...")
            test_response = llm.generate_response(
                user_query="Thá»§ tá»¥c khai sinh cáº§n gÃ¬?",
                max_tokens=100,
                temperature=0.1
            )
            logger.info(f"   âœ… Test response: {test_response['response'][:100]}...")
            logger.info("âœ… LLM verification successful!")
            return 0
            
    except Exception as e:
        logger.error(f"âŒ Failed to initialize LLMService: {e}")
        logger.error("   Please ensure your LLM model is properly installed.")
        logger.error(f"   Expected model path: {settings.llm_model_path}")
        return 1

    # Khá»Ÿi táº¡o vÃ  cháº¡y generator
    generator = SmartRouterLLMGenerator(str(documents_dir), str(router_examples_dir), llm)
    count = generator.generate_all_smart_examples(force_rebuild=args.force)
    
    if count > 0:
        logger.info(f"\nðŸŽ‰ SUCCESS! Generated router examples for {count} documents.")
        logger.info(f"   ðŸ“‚ Output directory: {router_examples_dir}")
        logger.info(f"   ðŸ’¡ Next step: Run 'python tools/4_build_router_cache.py' to build cache")
    else:
        logger.error("\nâŒ No new router examples were generated.")
        if not args.force:
            logger.info("   ðŸ’¡ Try using --force flag to rebuild existing examples.")
    
    return 0 if count > 0 else 1

if __name__ == "__main__":
    exit(main())
