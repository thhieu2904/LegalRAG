#!/usr/bin/env python3
"""
Models Setup Tool for LegalRAG
===============================

Tool setup models d·ª±a tr√™n fresh_install_setup.py v√† config.py:
- T·∫°o directory structure theo config t·ª´ fresh_install_setup.py
- Check v√† load models t·ª´ config settings
- Download missing models n·∫øu c·∫ßn (nh∆∞ fresh_install_setup.py)
- Test model functionality

Usage:
    cd backend
    python tools/1_setup_models.py
    python tools/1_setup_models.py --verify-only  # Ch·ªâ ki·ªÉm tra
"""

import sys
import os
from pathlib import Path
import logging
import argparse

# Add backend to Python path
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def setup_directory_structure():
    """T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c c·∫ßn thi·∫øt - t·ª´ fresh_install_setup.py"""
    logger.info("üìÅ SETTING UP DIRECTORY STRUCTURE")
    logger.info("-" * 40)
    
    required_dirs = [
        'data',
        'data/documents',
        'data/models',
        'data/models/hf_cache',
        'data/models/llm_dir', 
        'data/vectordb',
        'data/cache',
        'data/router_examples'
    ]
    
    created_count = 0
    for dir_path in required_dirs:
        full_path = Path(dir_path)
        if not full_path.exists():
            full_path.mkdir(parents=True, exist_ok=True)
            logger.info(f"   ‚úÖ Created: {dir_path}")
            created_count += 1
        else:
            logger.info(f"   ‚úÖ Exists: {dir_path}")
    
    if created_count > 0:
        logger.info(f"üìÅ Created {created_count} new directories")
    else:
        logger.info("üìÅ All required directories already exist")
    
    logger.info("")

def check_and_setup_models(verify_only: bool = False):
    """Ki·ªÉm tra v√† setup AI models - t·ª´ fresh_install_setup.py"""
    logger.info("ü§ñ CHECKING AI MODELS")
    logger.info("-" * 40)
    
    try:
        from app.core.config import settings
        settings.setup_environment()
        
        # Ensure models directory exists
        models_cache_dir = settings.hf_cache_path
        models_cache_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"   üìÅ Models cache directory: {models_cache_dir}")
        
        # Check embedding model
        logger.info("   üìä Embedding Model:")
        try:
            from sentence_transformers import SentenceTransformer
            # Use cache directory for download/load
            embedding_model = SentenceTransformer(
                settings.embedding_model_name,
                cache_folder=str(models_cache_dir)
            )
            logger.info(f"      ‚úÖ {settings.embedding_model_name} loaded successfully")
            
            # Test v·ªõi m·ªôt c√¢u
            test_embedding = embedding_model.encode("test sentence")
            logger.info(f"      üìä Embedding dimension: {len(test_embedding)}")
        except Exception as e:
            logger.error(f"      ‚ùå Error loading embedding model: {e}")
            if not verify_only:
                logger.info("      üí° Attempting to download...")
                try:
                    from sentence_transformers import SentenceTransformer
                    SentenceTransformer(
                        settings.embedding_model_name,
                        cache_folder=str(models_cache_dir)
                    )
                    logger.info("      ‚úÖ Download successful!")
                except Exception as e2:
                    logger.error(f"      ‚ùå Download failed: {e2}")
                    return False
            else:
                return False
        
        # Check reranker model  
        logger.info("   üéØ Reranker Model:")
        try:
            from sentence_transformers import CrossEncoder
            # Use cache directory for download/load
            reranker_model = CrossEncoder(
                settings.reranker_model_name,
                cache_folder=str(models_cache_dir)
            )
            logger.info(f"      ‚úÖ {settings.reranker_model_name} loaded successfully")
            
            # Test v·ªõi m·ªôt c·∫∑p c√¢u
            test_scores = reranker_model.predict([("query test", "document test")])
            logger.info(f"      üìä Test score: {test_scores[0]:.3f}")
        except Exception as e:
            logger.error(f"      ‚ùå Error loading reranker model: {e}")
            if not verify_only:
                logger.info("      üí° Attempting to download...")
                try:
                    from sentence_transformers import CrossEncoder
                    CrossEncoder(
                        settings.reranker_model_name,
                        cache_folder=str(models_cache_dir)
                    )
                    logger.info("      ‚úÖ Download successful!")
                except Exception as e2:
                    logger.error(f"      ‚ùå Download failed: {e2}")
                    return False
            else:
                return False
        
        # Check LLM model
        logger.info("   üß† LLM Model:")
        llm_path = Path(settings.llm_model_path)
        llm_dir = llm_path.parent
        
        if llm_path.exists():
            file_size_mb = llm_path.stat().st_size / (1024 * 1024)
            logger.info(f"      ‚úÖ LLM model found: {llm_path}")
            logger.info(f"      üìä File size: {file_size_mb:.1f}MB")
        else:
            logger.error(f"      ‚ùå LLM model not found: {llm_path}")
            if not verify_only and settings.llm_model_url:
                logger.info("      üí° Attempting to download LLM model...")
                try:
                    # Create LLM directory if needed
                    llm_dir.mkdir(parents=True, exist_ok=True)
                    
                    # Download using requests or wget
                    import requests
                    import shutil
                    
                    logger.info(f"      üì• Downloading from: {settings.llm_model_url}")
                    response = requests.get(settings.llm_model_url, stream=True)
                    response.raise_for_status()
                    
                    with open(llm_path, 'wb') as f:
                        shutil.copyfileobj(response.raw, f)
                    
                    file_size_mb = llm_path.stat().st_size / (1024 * 1024)
                    logger.info(f"      ‚úÖ Download successful! Size: {file_size_mb:.1f}MB")
                    
                except Exception as e:
                    logger.error(f"      ‚ùå Download failed: {e}")
                    logger.info("      üí° Please manually download the LLM model")
                    return False
            else:
                logger.info("      üí° Please set LLM_MODEL_URL in .env or download manually")
                return False
        
        logger.info("   ‚úÖ All models available!")
        logger.info("")
        return True
        
    except Exception as e:
        logger.error(f"   ‚ùå Error checking models: {e}")
        return False

def verify_model_environment():
    """Verify environment variables ƒë∆∞·ª£c set ƒë√∫ng"""
    logger.info("üîß VERIFYING MODEL ENVIRONMENT")
    logger.info("-" * 40)
    
    env_vars = {
        'HF_HOME': os.environ.get('HF_HOME'),
        'HF_HUB_CACHE': os.environ.get('HF_HUB_CACHE'),
        'HF_HUB_OFFLINE': os.environ.get('HF_HUB_OFFLINE'),
        'TRANSFORMERS_OFFLINE': os.environ.get('TRANSFORMERS_OFFLINE')
    }
    
    for var, value in env_vars.items():
        if value:
            logger.info(f"   ‚úÖ {var}: {value}")
        else:
            logger.warning(f"   ‚ö†Ô∏è  {var}: Not set")
    
    # Check if HF cache directory exists and has content
    hf_home = env_vars.get('HF_HOME')
    if hf_home and Path(hf_home).exists():
        hf_cache = Path(hf_home)
        cached_models = list(hf_cache.rglob('**/config.json'))
        logger.info(f"   üìä Cached models found: {len(cached_models)}")
    else:
        logger.warning("   ‚ö†Ô∏è  HF cache directory not found")
    
    logger.info("")

def main():
    parser = argparse.ArgumentParser(
        description='Complete Models Setup for LegalRAG',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python tools/1_setup_models.py             # Full setup with downloads
  python tools/1_setup_models.py --verify    # Verify only, no downloads

This tool will:
1. Create required directory structure from fresh_install_setup.py
2. Setup HuggingFace environment variables  
3. Check and download Vietnamese embedding models
4. Verify LLM model availability
5. Test model functionality
        """
    )
    
    parser.add_argument(
        '--verify-only',
        action='store_true',
        help='Only verify existing models, do not download'
    )
    
    args = parser.parse_args()
    
    logger.info("üöÄ LEGALRAG MODELS SETUP")
    logger.info("=" * 60)
    logger.info("This tool will setup the complete model environment")
    logger.info("")
    
    # Step 1: Directory structure
    setup_directory_structure()
    
    # Step 2: Verify environment
    verify_model_environment()
    
    # Step 3: Check models
    if not check_and_setup_models(verify_only=args.verify_only):
        logger.error("‚ùå SETUP FAILED: Models not available!")
        logger.info("üí° Please check your internet connection and try again")
        return False
    
    logger.info("üéâ SETUP COMPLETED SUCCESSFULLY!")
    logger.info("=" * 60)
    logger.info("‚úÖ Directory structure created")
    logger.info("‚úÖ Models loaded and tested")
    logger.info("‚úÖ Environment verified")
    logger.info("")
    logger.info("üöÄ Your model environment is ready!")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
