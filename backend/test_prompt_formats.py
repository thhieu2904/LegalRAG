"""
Test ƒë·ªÉ ki·ªÉm tra prompt format c·ªßa PhoGPT model
"""

import os
import sys
import logging
from pathlib import Path

# Add backend path
backend_path = Path(__file__).parent
sys.path.append(str(backend_path))

from app.services.language_model import LLMService

def test_prompt_formats():
    """Test c√°c prompt format kh√°c nhau ƒë·ªÉ t√¨m format ƒë√∫ng"""
    
    llm_service = LLMService()
    
    query = "mu·ªën ƒëƒÉng k√Ω khai sinh th√¨ c√≥ c·∫ßn ƒë√≥ng ti·ªÅn kh√¥ng"
    context = "Th·ªß t·ª•c ƒëƒÉng k√Ω khai sinh l√† MI·ªÑN PH√ç theo quy ƒë·ªãnh."
    system_prompt = "Tr·∫£ l·ªùi ng·∫Øn g·ªçn d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p."
    
    # Test format hi·ªán t·∫°i
    print("üß™ TESTING DIFFERENT PROMPT FORMATS")
    print("=" * 60)
    
    # Format 1: Current format
    print("\nüìù FORMAT 1 (Current):")
    current_format = f"### C√¢u h·ªèi: {system_prompt}\n\nNg·ªØ c·∫£nh th√¥ng tin:\n{context}\n\nC√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {query}\n### Tr·∫£ l·ªùi:"
    print(current_format[:200] + "...")
    
    try:
        result = llm_service.model(
            current_format,
            max_tokens=100,
            temperature=0.1,
            stop=["### C√¢u h·ªèi:", "### Tr·∫£ l·ªùi:", "\n\n###"],
            echo=False
        )
        response = result['choices'][0]['text'].strip()
        print(f"ü§ñ Response: {response[:150]}...")
    except Exception as e:
        print(f"‚ùå Error: {e}")
    
    # Format 2: Simpler format
    print("\nüìù FORMAT 2 (Simpler):")
    simple_format = f"{system_prompt}\n\nContext: {context}\n\nQuestion: {query}\n\nAnswer:"
    print(simple_format[:200] + "...")
    
    try:
        result = llm_service.model(
            simple_format,
            max_tokens=100,
            temperature=0.1,
            stop=["\n\nQuestion:", "\n\nContext:", "Question:", "Answer:"],
            echo=False
        )
        response = result['choices'][0]['text'].strip()
        print(f"ü§ñ Response: {response[:150]}...")
    except Exception as e:
        print(f"‚ùå Error: {e}")
    
    # Format 3: Chat format
    print("\nüìù FORMAT 3 (Chat):")
    chat_format = f"<|system|>\n{system_prompt}\n\n{context}<|end|>\n<|user|>\n{query}<|end|>\n<|assistant|>\n"
    print(chat_format[:200] + "...")
    
    try:
        result = llm_service.model(
            chat_format,
            max_tokens=100,
            temperature=0.1,
            stop=["<|user|>", "<|system|>", "<|end|>"],
            echo=False
        )
        response = result['choices'][0]['text'].strip()
        print(f"ü§ñ Response: {response[:150]}...")
    except Exception as e:
        print(f"‚ùå Error: {e}")

    # Format 4: Plain format (no special tokens)
    print("\nüìù FORMAT 4 (Plain):")
    plain_format = f"Instruction: {system_prompt}\n\nContext: {context}\n\nUser: {query}\n\nAssistant:"
    print(plain_format[:200] + "...")
    
    try:
        result = llm_service.model(
            plain_format,
            max_tokens=100,
            temperature=0.1,
            stop=["User:", "Instruction:", "Context:", "Assistant:"],
            echo=False
        )
        response = result['choices'][0]['text'].strip()
        print(f"ü§ñ Response: {response[:150]}...")
    except Exception as e:
        print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    test_prompt_formats()
