"""
Script Ä‘Æ¡n giáº£n Ä‘á»ƒ kiá»ƒm tra system prompt vÃ  LLM behavior
"""

import os
import sys
import logging
from pathlib import Path

# Add backend path
backend_path = Path(__file__).parent
sys.path.append(str(backend_path))

from app.services.language_model import LLMService
from app.core.config import settings

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_llm_with_simple_context():
    """Test LLM vá»›i context Ä‘Æ¡n giáº£n Ä‘á»ƒ kiá»ƒm tra hallucination"""
    
    print("ğŸ§ª TESTING LLM WITH SIMPLE CONTEXT")
    print("=" * 60)
    
    # Khá»Ÿi táº¡o LLM service
    llm_service = LLMService()
    
    # Test case 1: Context rÃµ rÃ ng vá» lá»‡ phÃ­
    test_context = """
ğŸ“‹ TIÃŠU Äá»€: ÄÄƒng kÃ½ khai sinh
ğŸ¢ CÆ  QUAN THá»°C HIá»†N: UBND cáº¥p phÆ°á»ng/xÃ£
ğŸ’° Lá»† PHÃ: MIá»„N PHÃ (khÃ´ng pháº£i Ä‘Ã³ng tiá»n)
â° THá»œI GIAN Xá»¬ LÃ: Ngay khi nháº­n Ä‘á»§ há»“ sÆ¡ há»£p lá»‡

ğŸ“„ Ná»˜I DUNG 1:
Thá»§ tá»¥c Ä‘Äƒng kÃ½ khai sinh lÃ  MIá»„N PHÃ theo quy Ä‘á»‹nh cá»§a phÃ¡p luáº­t Viá»‡t Nam.
NgÆ°á»i dÃ¢n khÃ´ng pháº£i Ä‘Ã³ng báº¥t ká»³ khoáº£n phÃ­ nÃ o khi lÃ m thá»§ tá»¥c nÃ y.

ğŸ“„ Ná»˜I DUNG 2:
Há»“ sÆ¡ bao gá»“m:
- Giáº¥y chá»©ng sinh (do bá»‡nh viá»‡n cáº¥p)
- Giáº¥y tá» tÃ¹y thÃ¢n cá»§a cha máº¹
"""

    queries = [
        "muá»‘n Ä‘Äƒng kÃ½ khai sinh thÃ¬ cÃ³ cáº§n Ä‘Ã³ng tiá»n khÃ´ng",
        "mÃ¬nh muá»‘n há»i phÃ­ khi mÃ  Ä‘Äƒng kÃ½ khai sinh Ã¡",
        "Ä‘Äƒng kÃ½ khai sinh cÃ³ tá»‘n phÃ­ gÃ¬ khÃ´ng"
    ]
    
    # Test system prompt hiá»‡n táº¡i
    current_system_prompt = """Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn vá» phÃ¡p luáº­t Viá»‡t Nam. HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p.

HÆ°á»›ng dáº«n tráº£ lá»i:
1. Tráº£ lá»i chÃ­nh xÃ¡c dá»±a trÃªn thÃ´ng tin trong tÃ i liá»‡u
2. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, hÃ£y nÃ³i rÃµ vÃ  cung cáº¥p trÃ­ch dáº«n Ä‘á»ƒ há»— trá»£ cho cÃ¢u tráº£ lá»i cá»§a báº¡n
3. Sá»­ dá»¥ng ngá»¯ Ä‘iá»‡u thÃ¢n thiá»‡n vÃ  chuyÃªn nghiá»‡p khi giao tiáº¿p vá»›i ngÆ°á»i dÃ¹ng khÃ¡c vá» cÃ¡c váº¥n Ä‘á» phÃ¡p lÃ½ liÃªn quan Ä‘áº¿n Viá»‡t Nam"""
    
    print(f"ğŸ“ Current System Prompt:")
    print(current_system_prompt)
    print("\n" + "=" * 60)
    
    for i, query in enumerate(queries, 1):
        print(f"\nğŸ” Test {i}: {query}")
        print("-" * 40)
        
        try:
            result = llm_service.generate_response(
                user_query=query,
                context=test_context,
                system_prompt=current_system_prompt,
                max_tokens=200,
                temperature=0.3
            )
            
            print(f"ğŸ¤– LLM Response:")
            response_text = result.get('response', 'No response')
            print(f"   {response_text}")
            
            # PhÃ¢n tÃ­ch response
            if "miá»…n phÃ­" in response_text.lower() or "khÃ´ng pháº£i Ä‘Ã³ng" in response_text.lower():
                print("âœ… Correct: LLM hiá»ƒu Ä‘Ãºng vá» miá»…n phÃ­")
            else:
                print("âŒ Wrong: LLM khÃ´ng hiá»ƒu Ä‘Ãºng vá» lá»‡ phÃ­")
                print("ğŸš¨ POTENTIAL HALLUCINATION DETECTED!")
            
        except Exception as e:
            print(f"âŒ Error: {e}")
        
        print()

def test_improved_system_prompt():
    """Test vá»›i system prompt cáº£i tiáº¿n Ä‘á»ƒ cháº·n hallucination"""
    
    print("\n" + "=" * 80)
    print("ğŸ”§ TESTING IMPROVED SYSTEM PROMPT")
    print("=" * 80)
    
    # LLM service
    llm_service = LLMService()
    
    # Context nhÆ° trÃªn
    test_context = """
ğŸ“‹ TIÃŠU Äá»€: ÄÄƒng kÃ½ khai sinh
ğŸ’° Lá»† PHÃ: MIá»„N PHÃ (khÃ´ng pháº£i Ä‘Ã³ng tiá»n)

ğŸ“„ Ná»˜I DUNG:
Thá»§ tá»¥c Ä‘Äƒng kÃ½ khai sinh lÃ  MIá»„N PHÃ theo quy Ä‘á»‹nh cá»§a phÃ¡p luáº­t Viá»‡t Nam.
"""
    
    # IMPROVED system prompt - cháº·t cháº½ hÆ¡n
    improved_system_prompt = """Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn vá» phÃ¡p luáº­t Viá»‡t Nam. 

QUAN TRá»ŒNG - QUY Táº®C Báº®T BUá»˜C:
1. CHá»ˆ tráº£ lá»i dá»±a trÃªn thÃ´ng tin CÃ“ TRONG tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p
2. KHÃ”NG tá»± táº¡o thÃ´ng tin khÃ´ng cÃ³ trong tÃ i liá»‡u
3. Náº¿u cÃ¢u há»i vá» "phÃ­" hoáº·c "tiá»n" - hÃ£y TÃŒM CHÃNH XÃC thÃ´ng tin "Lá»† PHÃ" trong tÃ i liá»‡u
4. Tráº£ lá»i NGáº®N Gá»ŒN, TRá»°C TIáº¾P vÃ o váº¥n Ä‘á» Ä‘Æ°á»£c há»i
5. KHÃ”NG nÃªu thÃ´ng tin vá» cÃ¡c trÆ°á»ng há»£p khÃ¡c náº¿u khÃ´ng Ä‘Æ°á»£c há»i

Cáº¥u trÃºc tráº£ lá»i:
- Tráº£ lá»i trá»±c tiáº¿p cÃ¢u há»i (1-2 cÃ¢u)
- TrÃ­ch dáº«n thÃ´ng tin tá»« tÃ i liá»‡u"""
    
    query = "muá»‘n Ä‘Äƒng kÃ½ khai sinh thÃ¬ cÃ³ cáº§n Ä‘Ã³ng tiá»n khÃ´ng"
    
    print(f"ğŸ“ Improved System Prompt:")
    print(improved_system_prompt[:200] + "...")
    print(f"\nğŸ” Test Query: {query}")
    print("-" * 50)
    
    try:
        result = llm_service.generate_response(
            user_query=query,
            context=test_context,
            system_prompt=improved_system_prompt,
            max_tokens=150,
            temperature=0.1  # Giáº£m temperature Ä‘á»ƒ Ã­t hallucination
        )
        
        print(f"ğŸ¤– Improved Response:")
        response_text = result.get('response', 'No response')
        print(f"   {response_text}")
        
        # PhÃ¢n tÃ­ch
        if len(response_text.split()) < 50:  # Kiá»ƒm tra Ä‘á»™ dÃ i
            print("âœ… Good: Response ngáº¯n gá»n")
        else:
            print("âš ï¸  Warning: Response quÃ¡ dÃ i")
            
        if "miá»…n phÃ­" in response_text.lower():
            print("âœ… Correct: Tráº£ lá»i Ä‘Ãºng vá» miá»…n phÃ­")
        else:
            print("âŒ Wrong: Váº«n khÃ´ng tráº£ lá»i Ä‘Ãºng")
            
    except Exception as e:
        print(f"âŒ Error: {e}")

if __name__ == "__main__":
    test_llm_with_simple_context()
    test_improved_system_prompt()
