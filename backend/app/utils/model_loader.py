"""
Auto Model Loader - T·ª± ƒë·ªông t·∫£i models khi kh·ªüi ƒë·ªông
Kh√¥ng c·∫ßn can thi·ªáp th·ªß c√¥ng, models s·∫Ω ƒë∆∞·ª£c t·∫£i t·ª± ƒë·ªông n·∫øu ch∆∞a c√≥
"""

import os
import sys
import requests
import logging
from pathlib import Path
from typing import Optional

# Setup logging
logger = logging.getLogger(__name__)

def ensure_model_exists(model_path: Path, download_url: str, model_name: str) -> bool:
    """ƒê·∫£m b·∫£o model t·ªìn t·∫°i, t·ª± ƒë·ªông download n·∫øu ch∆∞a c√≥"""
    
    if model_path.exists():
        file_size = model_path.stat().st_size / (1024 * 1024)  # MB
        logger.info(f"‚úÖ Model {model_name} already exists ({file_size:.1f}MB): {model_path}")
        return True
    
    logger.info(f"üì• Downloading {model_name} to {model_path}")
    
    try:
        # T·∫°o th∆∞ m·ª•c n·∫øu c·∫ßn
        model_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Download v·ªõi progress
        response = requests.get(download_url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(model_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    
                    # Log progress every 100MB
                    if downloaded % (100 * 1024 * 1024) == 0:
                        progress = (downloaded / total_size * 100) if total_size > 0 else 0
                        logger.info(f"   Progress: {progress:.1f}% ({downloaded/(1024*1024):.1f}MB)")
        
        file_size = model_path.stat().st_size / (1024 * 1024)
        logger.info(f"‚úÖ Downloaded {model_name} successfully ({file_size:.1f}MB)")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Failed to download {model_name}: {e}")
        if model_path.exists():
            model_path.unlink()
        return False

def ensure_huggingface_model(model_name: str) -> bool:
    """ƒê·∫£m b·∫£o HuggingFace model t·ªìn t·∫°i, t·ª± ƒë·ªông download n·∫øu ch∆∞a c√≥"""
    
    try:
        if "Vietnamese_Embedding" in model_name:
            logger.info(f"üì• Loading embedding model: {model_name}")
            from sentence_transformers import SentenceTransformer
            
            # Ki·ªÉm tra xem model ƒë√£ t·∫£i ch∆∞a
            try:
                model = SentenceTransformer(model_name, device='cpu')  # Load CPU first
                logger.info(f"‚úÖ Embedding model loaded: {model_name}")
                return True
            except Exception as e:
                logger.error(f"‚ùå Failed to load embedding model {model_name}: {e}")
                return False
                
        elif "Vietnamese_Reranker" in model_name:
            logger.info(f"üì• Loading reranker model: {model_name}")
            from transformers import AutoTokenizer, AutoModel
            
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model = AutoModel.from_pretrained(model_name)
                logger.info(f"‚úÖ Reranker model loaded: {model_name}")
                return True
            except Exception as e:
                logger.error(f"‚ùå Failed to load reranker model {model_name}: {e}")
                return False
                
    except ImportError as e:
        logger.error(f"‚ùå Missing dependencies for {model_name}: {e}")
        return False
    except Exception as e:
        logger.error(f"‚ùå Unexpected error loading {model_name}: {e}")
        return False
    
    return False

def auto_setup_models(settings) -> dict:
    """
    T·ª± ƒë·ªông setup t·∫•t c·∫£ models c·∫ßn thi·∫øt
    Returns: Dictionary v·ªõi status c·ªßa t·ª´ng model
    """
    
    logger.info("üöÄ Starting automatic models setup...")
    
    results = {
        'llm_model': False,
        'embedding_model': False, 
        'reranker_model': False,
        'all_success': False
    }
    
    # 1. LLM Model (.gguf file) - use correct settings properties
    llm_path = Path(settings.llm_model_path)
    results['llm_model'] = ensure_model_exists(
        llm_path, 
        settings.llm_model_url, 
        f"PhoGPT LLM ({llm_path.name})"
    )
    
    # 2. Embedding Model (HuggingFace)
    results['embedding_model'] = ensure_huggingface_model(settings.embedding_model_name)
    
    # 3. Reranker Model (HuggingFace) - Optional for now
    results['reranker_model'] = ensure_huggingface_model(settings.reranker_model_name)
    
    # Check overall success (LLM + Embedding required, Reranker optional)
    required_models = results['llm_model'] and results['embedding_model']
    results['all_success'] = required_models
    
    if results['all_success']:
        logger.info("üéâ All required models are ready!")
    else:
        logger.warning("‚ö†Ô∏è  Some required models failed to load. System may not work properly.")
    
    return results
