"""
Optimized Enhanced RAG Service v·ªõi VRAM-optimized architecture
T√≠ch h·ª£p:
1. Ambiguous Query Detection & Processing
2. Enhanced Context Expansion v·ªõi Nucleus Strategy  
3. VRAM-optimized model placement (CPU embedding, GPU LLM/Reranker)
4. Session management
"""

import logging
import time
import uuid
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from pathlib import Path

from .vectordb_service import VectorDBService
from .llm_service import LLMService
from .reranker_service import RerankerService
from .query_router import QueryRouter
from .enhanced_smart_query_router import EnhancedSmartQueryRouter, RouterBasedAmbiguousQueryService
from .enhanced_context_expansion_service import EnhancedContextExpansionService
from ..core.config import settings

logger = logging.getLogger(__name__)

@dataclass
class OptimizedChatSession:
    """Session chat v·ªõi th√¥ng tin t·ªëi ∆∞u"""
    session_id: str
    created_at: float
    last_accessed: float
    query_history: List[Dict[str, Any]] = field(default_factory=list)
    context_cache: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

class OptimizedEnhancedRAGService:
    """
    Enhanced RAG Service ƒë∆∞·ª£c t·ªëi ∆∞u VRAM v√† performance
    
    Ki·∫øn tr√∫c:
    - Embedding Model: CPU (ti·∫øt ki·ªám VRAM cho query ng·∫Øn)
    - LLM: GPU (c·∫ßn song song h√≥a cho context d√†i)  
    - Reranker: GPU (c·∫ßn song song h√≥a cho multiple comparisons)
    """
    
    def __init__(
        self,
        documents_dir: str,
        vectordb_service: VectorDBService,
        llm_service: LLMService
    ):
        self.documents_dir = documents_dir
        self.vectordb_service = vectordb_service
        self.llm_service = llm_service
        
        # Initialize supporting services
        self._initialize_services()
        
        # Chat sessions management
        self.chat_sessions: Dict[str, OptimizedChatSession] = {}
        
        # Performance metrics
        self.metrics = {
            "total_queries": 0,
            "ambiguous_detected": 0,
            "context_expansions": 0,
            "avg_response_time": 0.0
        }
        
        logger.info("‚úÖ Optimized Enhanced RAG Service initialized")
        
    def _initialize_services(self):
        """Kh·ªüi t·∫°o c√°c service h·ªó tr·ª£ v·ªõi Enhanced Smart Router"""
        try:
            # Enhanced Smart Query Router v·ªõi Example Questions Database
            self.smart_router = EnhancedSmartQueryRouter(embedding_model=self.vectordb_service.embedding_model)
            logger.info("‚úÖ Enhanced Smart Query Router initialized")
            
            # Reranker Service (GPU)
            self.reranker_service = RerankerService()
            logger.info("‚úÖ Reranker Service initialized (GPU)")
            
            # Router-based Ambiguous Query Service (CPU)
            self.ambiguous_service = RouterBasedAmbiguousQueryService(
                router=self.smart_router
            )
            logger.info("‚úÖ Router-based Ambiguous Query Service initialized (CPU)")
            
            # Enhanced Context Expansion Service  
            self.context_expansion_service = EnhancedContextExpansionService(
                vectordb_service=self.vectordb_service,
                documents_dir=self.documents_dir
            )
            logger.info("‚úÖ Enhanced Context Expansion Service initialized")
            
        except Exception as e:
            logger.error(f"Error initializing services: {e}")
            raise
            
    def create_session(self, metadata: Optional[Dict[str, Any]] = None) -> str:
        """T·∫°o session chat m·ªõi"""
        session_id = str(uuid.uuid4())
        
        session = OptimizedChatSession(
            session_id=session_id,
            created_at=time.time(),
            last_accessed=time.time(),
            metadata=metadata or {}
        )
        
        self.chat_sessions[session_id] = session
        logger.info(f"Created new chat session: {session_id}")
        
        return session_id
        
    def get_session(self, session_id: str) -> Optional[OptimizedChatSession]:
        """L·∫•y session theo ID"""
        session = self.chat_sessions.get(session_id)
        if session:
            session.last_accessed = time.time()
        return session
        
    def enhanced_query(
        self,
        query: str,
        session_id: Optional[str] = None,
        max_context_length: int = 3000,
        use_ambiguous_detection: bool = True,
        use_full_document_expansion: bool = True
    ) -> Dict[str, Any]:
        """
        Query ch√≠nh v·ªõi t·∫•t c·∫£ t·ªëi ∆∞u h√≥a
        
        Flow:
        1. Detect ambiguous query (CPU embedding)
        2. Route query n·∫øu clear
        3. Broad search (CPU embedding) 
        4. Rerank (GPU reranker)
        5. Context expansion v·ªõi nucleus strategy
        6. Generate answer (GPU LLM)
        """
        start_time = time.time()
        self.metrics["total_queries"] += 1
        
        try:
            # Get or create session
            if session_id:
                session = self.get_session(session_id)
                if not session:
                    return {"error": f"Session {session_id} not found"}
            else:
                session_id = self.create_session()
                session = self.get_session(session_id)
                
            logger.info(f"Processing query in session {session_id}: {query[:50]}...")
            
            # Step 1: Enhanced Smart Query Routing v·ªõi Ambiguous Detection  
            if use_ambiguous_detection:
                is_ambiguous, routing_result = self.ambiguous_service.is_ambiguous(query)
                
                if is_ambiguous:
                    self.metrics["ambiguous_detected"] += 1
                    logger.info(f"Ambiguous query detected: {routing_result['status']} (confidence: {routing_result['confidence']:.2f})")
                    
                    clarification_response = self.ambiguous_service.generate_clarification_response(routing_result)
                    
                    return {
                        "type": "clarification_needed",
                        "status": routing_result['status'],
                        "confidence": routing_result['confidence'],
                        "clarification": clarification_response,
                        "matched_example": routing_result.get('matched_example'),
                        "source_procedure": routing_result.get('source_procedure'),
                        "session_id": session_id,
                        "processing_time": time.time() - start_time
                    }
                else:
                    # Query kh√¥ng m∆° h·ªì - c√≥ target collection r·ªìi
                    target_collection = routing_result['target_collection']
                    best_collections = [target_collection] if target_collection else [settings.chroma_collection_name]
                    logger.info(f"Smart routed to collection: {target_collection} (confidence: {routing_result['confidence']:.2f})")
            else:
                # Fallback: s·ª≠ d·ª•ng default collection
                best_collections = [settings.chroma_collection_name]
            
            # Step 2: Focused Search trong target collection
            broad_search_results = []
            for collection_name in best_collections[:2]:  # Limit to top 2 collections
                try:
                    results = self.vectordb_service.search_in_collection(
                        collection_name=collection_name,
                        query=query,
                        top_k=settings.broad_search_k,
                        similarity_threshold=settings.similarity_threshold
                    )
                    
                    for result in results:
                        result["collection"] = collection_name
                        
                    broad_search_results.extend(results)
                    
                except Exception as e:
                    logger.warning(f"Error searching in collection {collection_name}: {e}")
            
            if not broad_search_results:
                return {
                    "type": "no_results",
                    "message": "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n.",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            logger.info(f"Found {len(broad_search_results)} candidate chunks")
            
            # Step 4: Reranking (GPU - high precision) - L·∫§Y 1 CHUNK CAO NH·∫§T SAU RERANK
            if settings.use_reranker and len(broad_search_results) > 1:
                # ‚úÖ FIX CRITICAL BUG: Rerank T·∫§T C·∫¢ documents thay v√¨ ch·ªâ top 10
                # ƒê√¢y l√† l·ªói logic nghi√™m tr·ªçng - kh√¥ng ƒë∆∞·ª£c v·ª©t b·ªè documents ti·ªÅm nƒÉng!
                docs_to_rerank = broad_search_results  # RERANK ALL DOCUMENTS
                logger.info(f"Reranking ALL {len(broad_search_results)} candidate documents (FIXED BUG)")
                
                nucleus_chunks = self.reranker_service.rerank_documents(
                    query=query,
                    documents=docs_to_rerank,
                    top_k=1  # CH·ªà 1 nucleus chunk cao nh·∫•t - s·∫Ω expand to√†n b·ªô document ch·ª©a chunk n√†y
                )
                
                # ‚úÖ VALIDATION: Ki·ªÉm tra rerank score ƒë·ªÉ tr√°nh ch·ªçn t√†i li·ªáu kh√¥ng li√™n quan
                if nucleus_chunks and len(nucleus_chunks) > 0:
                    best_score = nucleus_chunks[0].get('rerank_score', 0)
                    logger.info(f"Best rerank score: {best_score:.4f}")
                    
                    # N·∫øu ƒëi·ªÉm qu√° th·∫•p (< 0.1), c·∫£nh b√°o nh∆∞ng v·∫´n ti·∫øp t·ª•c
                    if best_score < 0.1:
                        logger.warning(f"‚ö†Ô∏è  LOW RERANK SCORE ({best_score:.4f}) - T√†i li·ªáu c√≥ th·ªÉ kh√¥ng li√™n quan!")
                        logger.warning("üí° Suggestion: Ki·ªÉm tra l·∫°i query ho·∫∑c database content")
                
                logger.info(f"Selected {len(nucleus_chunks)} nucleus chunk with highest rerank score")
            else:
                nucleus_chunks = broad_search_results[:1]  # Fallback: l·∫•y chunk t·ªët nh·∫•t theo vector similarity
                
            # Step 5: Full Document Expansion - QUAN TR·ªåNG: L·∫•y to√†n b·ªô document thay v√¨ ch·ªâ 1 chunk
            expanded_context = None
            if use_full_document_expansion:
                self.metrics["context_expansions"] += 1
                expanded_context = self.context_expansion_service.expand_context_with_nucleus(
                    nucleus_chunks=nucleus_chunks,
                    max_context_length=max_context_length,
                    include_full_document=True  # KEY: L·∫•y to√†n b·ªô document t·ª´ file JSON g·ªëc
                )
                
                context_text = self._build_context_from_expanded(expanded_context)
                
                logger.info(f"Context expanded: {expanded_context['total_length']} chars from {len(expanded_context.get('source_documents', []))} documents")
            else:
                # Fallback: simple concatenation
                context_text = "\n\n".join([
                    f"T√†i li·ªáu: {chunk.get('metadata', {}).get('source', 'N/A')}\n{chunk['content']}"
                    for chunk in nucleus_chunks
                ])
            
            # Step 6: Generate Answer (GPU LLM)
            if not session:
                return {
                    "type": "error",
                    "error": "Session not found",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            answer = self._generate_answer_with_context(
                query=query,
                context=context_text,
                session=session
            )
            
            # Update session history
            session.query_history.append({
                "query": query,
                "answer": answer,
                "timestamp": time.time(),
                "nucleus_chunks_count": len(nucleus_chunks),
                "context_length": len(context_text)
            })
            
            # Keep only last 10 queries in session
            if len(session.query_history) > 10:
                session.query_history = session.query_history[-10:]
                
            processing_time = time.time() - start_time
            self.metrics["avg_response_time"] = (
                (self.metrics["avg_response_time"] * (self.metrics["total_queries"] - 1) + processing_time) 
                / self.metrics["total_queries"]
            )
            
            return {
                "type": "answer",
                "answer": answer,
                "context_info": {
                    "nucleus_chunks": len(nucleus_chunks),
                    "context_length": len(context_text),
                    "source_collections": list(set(chunk.get("collection", "") for chunk in nucleus_chunks)),
                    "source_documents": list(expanded_context.get("source_documents", [])) if expanded_context else []
                },
                "session_id": session_id,
                "processing_time": processing_time,
                "routing_info": {"best_collections": best_collections}
            }
            
        except Exception as e:
            logger.error(f"Error in enhanced query: {e}")
            return {
                "type": "error",
                "error": str(e),
                "session_id": session_id,
                "processing_time": time.time() - start_time
            }
            
    def handle_clarification(
        self,
        session_id: str,
        selected_option: str,
        original_query: str
    ) -> Dict[str, Any]:
        """X·ª≠ l√Ω ph·∫£n h·ªìi clarification"""
        session = self.get_session(session_id)
        if not session:
            return {"error": f"Session {session_id} not found"}
            
        # T·∫°o refined query
        refined_query = f"{original_query} - {selected_option}"
        
        # Process refined query (s·∫Ω kh√¥ng ambiguous n·ªØa)
        return self.enhanced_query(
            query=refined_query,
            session_id=session_id,
            use_ambiguous_detection=False  # Skip ambiguous detection for refined query
        )
        
    def _build_context_from_expanded(self, expanded_context: Dict[str, Any]) -> str:
        """Build context string t·ª´ expanded context"""
        context_parts = []
        
        for doc_content in expanded_context.get("expanded_content", []):
            source = doc_content.get("source", "N/A")
            text = doc_content.get("text", "")
            chunk_count = doc_content.get("chunk_count", 0)
            
            context_parts.append(f"=== T√†i li·ªáu: {source} ({chunk_count} ƒëo·∫°n) ===\n{text}")
            
        return "\n\n".join(context_parts)
        
    def _generate_answer_with_context(
        self,
        query: str,
        context: str,
        session: OptimizedChatSession
    ) -> str:
        """Generate answer v·ªõi context v√† session history"""
        
        # Build conversation context if needed
        conversation_context = ""
        if len(session.query_history) > 0:
            recent_queries = session.query_history[-3:]  # Last 3 queries
            conversation_context = "L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y:\n" + "\n".join([
                f"Q: {item['query']}\nA: {item['answer'][:200]}..." 
                for item in recent_queries
            ]) + "\n\n"
            
        # Build system prompt
        system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
1. Tr·∫£ l·ªùi ch√≠nh x√°c d·ª±a tr√™n th√¥ng tin trong t√†i li·ªáu
2. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r√µ v√† cung c·∫•p tr√≠ch d·∫´n ƒë·ªÉ h·ªó tr·ª£ cho c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n
3. S·ª≠ d·ª•ng ng·ªØ ƒëi·ªáu th√¢n thi·ªán v√† chuy√™n nghi·ªáp khi giao ti·∫øp v·ªõi ng∆∞·ªùi d√πng kh√°c v·ªÅ c√°c v·∫•n ƒë·ªÅ ph√°p l√Ω li√™n quan ƒë·∫øn Vi·ªát Nam"""
        
        # Build enhanced context v·ªõi conversation history
        enhanced_context = conversation_context + context

        try:
            response_data = self.llm_service.generate_response(
                user_query=query,
                context=enhanced_context,
                max_tokens=settings.max_tokens,
                temperature=settings.temperature,
                system_prompt=system_prompt
            )
            
            # Extract response text from dict
            if isinstance(response_data, dict) and "response" in response_data:
                return response_data["response"].strip()
            elif isinstance(response_data, str):
                return response_data.strip()
            else:
                return str(response_data).strip()
            
        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            return f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t·∫°o c√¢u tr·∫£ l·ªùi: {e}"
            
    def get_health_status(self) -> Dict[str, Any]:
        """Tr·∫°ng th√°i health c·ªßa service"""
        try:
            collections = self.vectordb_service.list_collections()
            total_documents = 0
            
            for collection_info in collections:
                try:
                    collection = self.vectordb_service.get_collection(collection_info["name"])
                    count = collection.count()
                    total_documents += count
                except:
                    continue
                    
            return {
                "status": "healthy",
                "total_collections": len(collections),
                "total_documents": total_documents,
                "llm_loaded": self.llm_service.model is not None,
                "reranker_loaded": self.reranker_service.model is not None,
                "embedding_device": "CPU (VRAM optimized)",
                "llm_device": "GPU",
                "reranker_device": "GPU",
                "active_sessions": len(self.chat_sessions),
                "metrics": self.metrics,
                "router_stats": self.smart_router.get_collection_info(),
                "context_expansion": {
                    "total_chunks_cached": len(self.context_expansion_service.document_metadata_cache),
                    **self.context_expansion_service.get_stats()
                }
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
            
    def cleanup_old_sessions(self, max_age_hours: int = 24):
        """D·ªçn d·∫πp sessions c≈©"""
        current_time = time.time()
        cutoff_time = current_time - (max_age_hours * 3600)
        
        old_sessions = [
            session_id for session_id, session in self.chat_sessions.items()
            if session.last_accessed < cutoff_time
        ]
        
        for session_id in old_sessions:
            del self.chat_sessions[session_id]
            
        if old_sessions:
            logger.info(f"Cleaned up {len(old_sessions)} old sessions")
            
        return len(old_sessions)
