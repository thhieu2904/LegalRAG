import logging
import os
import requests
from pathlib import Path
from typing import Optional, List, Dict, Any
from llama_cpp import Llama
import time
from ..core.config import settings

logger = logging.getLogger(__name__)

class LLMService:
    """Service qu·∫£n l√Ω PhoGPT model t·ª´ HuggingFace v·ªõi VRAM Optimization"""
    
    def __init__(self, model_path: Optional[str] = None, model_url: Optional[str] = None, **kwargs):
        # Use absolute path to avoid issues when running from different directories
        if model_path:
            self.model_path = Path(model_path)
        else:
            # Use the computed property from settings which gives absolute path
            self.model_path = settings.llm_model_file_path
            
        self.model_url = model_url or settings.llm_model_url
        self.model = None
        self.model_loaded = False
        
        # C·∫•u h√¨nh GPU + CPU hybrid cho t·ªëi ∆∞u performance
        self.model_kwargs = {
            'n_ctx': kwargs.get('n_ctx', settings.n_ctx),
            'n_threads': kwargs.get('n_threads', settings.n_threads),
            'n_gpu_layers': kwargs.get('n_gpu_layers', settings.n_gpu_layers),  # GPU acceleration
            'n_batch': kwargs.get('n_batch', settings.n_batch),  # Batch size for prompt processing
            'verbose': False,
            # Th√™m GPU acceleration settings
            'use_mmap': True,  # Memory-mapped files
            'use_mlock': True,  # Keep model in RAM
            'f16_kv': True,    # Use half precision for key/value cache
        }
        
        # T·∫£i model n·∫øu ch∆∞a c√≥
        logger.info(f"Checking model path: {self.model_path}")
        logger.info(f"Model exists: {self.model_path.exists()}")
        
        if not self.model_path.exists():
            logger.info("Model file not found, attempting to download...")
            self._download_model()
        else:
            logger.info("Model file found, skipping download")
        
        # VRAM Optimization: Load model khi c·∫ßn thi·∫øt
        # self._load_model()  # Comment out ƒë·ªÉ load on-demand
    
    def _download_model(self):
        """T·∫£i model t·ª´ HuggingFace"""
        logger.info(f"Downloading model from {self.model_url}")
        
        try:
            # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
            self.model_path.parent.mkdir(parents=True, exist_ok=True)
            
            response = requests.get(self.model_url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded_size = 0
            
            with open(self.model_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)
                        
                        if total_size > 0:
                            progress = (downloaded_size / total_size) * 100
                            if downloaded_size % (1024 * 1024 * 100) == 0:  # Log every 100MB
                                logger.info(f"Downloaded {progress:.1f}% ({downloaded_size / (1024**3):.2f} GB)")
            
            logger.info(f"Model downloaded successfully: {self.model_path}")
            
        except Exception as e:
            logger.error(f"Failed to download model: {e}")
            if self.model_path.exists():
                self.model_path.unlink()  # X√≥a file b·ªã l·ªói
            raise
    
    def _load_model(self):
        """Load model v√†o memory"""
        if self.model_loaded:
            return
            
        try:
            logger.info(f"Loading LLM model from {self.model_path}")
            self.model = Llama(model_path=str(self.model_path), **self.model_kwargs)
            self.model_loaded = True
            logger.info("‚úÖ LLM model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load LLM model: {e}")
            self.model = None
            self.model_loaded = False
            raise
    
    def unload_model(self):
        """Unload model ƒë·ªÉ gi·∫£i ph√≥ng VRAM"""
        if self.model is not None:
            logger.info("üîÑ Unloading LLM model to free VRAM...")
            del self.model
            self.model = None
            self.model_loaded = False
            
            # Force garbage collection
            import gc
            gc.collect()
            logger.info("‚úÖ LLM model unloaded, VRAM freed")
    
    def ensure_loaded(self):
        """Ensure model is loaded - load n·∫øu ch∆∞a c√≥"""
        if not self.model_loaded or self.model is None:
            self._load_model()
    
    def is_model_loaded(self) -> bool:
        """Check if model is currently loaded"""
        return self.model_loaded and self.model is not None
    
    def _format_prompt(self, system_prompt: str, user_query: str, context: str = "") -> str:
        """Format prompt theo template t·ªëi ∆∞u ƒë·ªÉ tr√°nh confusion"""
        # S·ª≠ d·ª•ng format ƒë∆°n gi·∫£n v√† r√µ r√†ng h∆°n
        if context:
            instruction = f"""{system_prompt}

TH√îNG TIN T√ÄI LI·ªÜU:
{context}

C√ÇUH·ªéI: {user_query}

TR·∫¢ L·ªúI:"""
        else:
            instruction = f"""{system_prompt}

C√ÇUH·ªéI: {user_query}

TR·∫¢ L·ªúI:"""
        
        return instruction
    
    def generate_response(
        self, 
        user_query: str, 
        context: str = "", 
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        system_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """Sinh response t·ª´ model - VRAM optimized v·ªõi on-demand loading"""
        
        # VRAM Optimization: Ensure model is loaded
        self.ensure_loaded()
        
        if not self.model:
            raise Exception("Model not loaded")
        
        # S·ª≠ d·ª•ng values t·ª´ config v·ªõi t·ªëi ∆∞u anti-hallucination
        if max_tokens is None:
            max_tokens = min(settings.max_tokens, 300)  # Gi·∫£m max_tokens ƒë·ªÉ tr√°nh l·∫∑p v√† hallucination
        if temperature is None:
            temperature = min(settings.temperature, 0.1)  # Gi·∫£m temperature ƒë·ªÉ tƒÉng t√≠nh deterministic
        
        # System prompt t·ªëi ∆∞u cho legal domain - IMPROVED VERSION
        if system_prompt is None:
            system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam. 

QUY T·∫ÆC B·∫ÆT BU·ªòC:
1. CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin trong t√†i li·ªáu
2. Tr·∫£ l·ªùi NG·∫ÆN G·ªåN v√† TR·ª∞C TI·∫æP 
3. KH√îNG t·ª± s√°ng t·∫°o th√¥ng tin
4. N·∫øu h·ªèi v·ªÅ ph√≠/ti·ªÅn - t√¨m ch√≠nh x√°c th√¥ng tin "L·ªÜ PH√ç"

Tr·∫£ l·ªùi ch√≠nh x√°c, ng·∫Øn g·ªçn."""
        
        # Format prompt
        formatted_prompt = self._format_prompt(system_prompt, user_query, context)
        
        try:
            start_time = time.time()
            
            # Generate v·ªõi parameters t·ªëi ∆∞u ƒë·ªÉ tr√°nh l·∫∑p
            response = self.model(
                formatted_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,  # Nucleus sampling ƒë·ªÉ tƒÉng ƒëa d·∫°ng
                top_k=40,   # Top-K sampling
                repeat_penalty=1.1,  # Penalty cho t·ª´ l·∫∑p
                stop=["C√ÇUH·ªéI:", "TH√îNG TIN T√ÄI LI·ªÜU:", "TR·∫¢ L·ªúI:", "\n\nC√ÇUH·ªéI:", "\n\nTR·∫¢ L·ªúI:"],
                echo=False,
                stream=False  # Ensure non-streaming response
            )
            
            processing_time = time.time() - start_time
            
            # Safely extract text from response
            if isinstance(response, dict) and 'choices' in response:
                generated_text = response['choices'][0]['text'].strip()
                
                # Get token usage safely
                usage = response.get('usage', {})
                prompt_tokens = usage.get('prompt_tokens', 0)
                completion_tokens = usage.get('completion_tokens', 0)
                total_tokens = usage.get('total_tokens', prompt_tokens + completion_tokens)
            else:
                raise Exception("Invalid response format from model")
            
            # Clean up response - remove repetitive patterns
            generated_text = self._clean_repetitive_response(generated_text)
            
            result = {
                'response': generated_text,
                'processing_time': processing_time,
                'prompt_tokens': prompt_tokens,
                'completion_tokens': completion_tokens,
                'total_tokens': total_tokens
            }
            
            logger.info(f"Generated response in {processing_time:.2f}s, "
                       f"tokens: {result['total_tokens']}")
            
            return result
            
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            raise
    
    def _clean_repetitive_response(self, text: str) -> str:
        """D·ªçn d·∫πp response ƒë·ªÉ lo·∫°i b·ªè patterns l·∫∑p l·∫°i"""
        import re
        
        # Lo·∫°i b·ªè patterns l·∫∑p ki·ªÉu A. B. C. XI. XI. XI.
        text = re.sub(r'([A-Z]\.)\s*TH·ª¶\s*T·ª§C\s*NU√îI\s*CON\s*NU√îI\s*TRONG\s*N∆Ø·ªöC\s*\n*', '', text, flags=re.IGNORECASE)
        
        # Lo·∫°i b·ªè repetitive numbered/lettered lists
        text = re.sub(r'([A-Z]{1,3}\.)\s*(.+?)\n(\1\s*\2\n){2,}', r'\1 \2\n', text, flags=re.IGNORECASE)
        
        # Lo·∫°i b·ªè c√¢u l·∫∑p l·∫°i li√™n ti·∫øp (3+ l·∫ßn)
        lines = text.split('\n')
        cleaned_lines = []
        prev_line = ""
        repeat_count = 0
        
        for line in lines:
            line = line.strip()
            if line == prev_line and line:
                repeat_count += 1
                if repeat_count < 2:  # Cho ph√©p l·∫∑p t·ªëi ƒëa 1 l·∫ßn
                    cleaned_lines.append(line)
            else:
                repeat_count = 0
                cleaned_lines.append(line)
                prev_line = line
        
        cleaned_text = '\n'.join(cleaned_lines).strip()
        
        # Gi·ªõi h·∫°n ƒë·ªô d√†i response h·ª£p l√Ω
        if len(cleaned_text) > 2000:
            # C·∫Øt t·∫°i c√¢u cu·ªëi c√πng trong 2000 k√Ω t·ª± ƒë·∫ßu
            truncated = cleaned_text[:2000]
            last_sentence = truncated.rfind('.')
            if last_sentence > 1000:  # Ch·ªâ c·∫Øt n·∫øu c√≤n ƒë·ªß n·ªôi dung
                cleaned_text = truncated[:last_sentence + 1]
        
        return cleaned_text
    
    def is_loaded(self) -> bool:
        """Ki·ªÉm tra model ƒë√£ ƒë∆∞·ª£c load ch∆∞a"""
        return self.model is not None
    
    def get_model_info(self) -> Dict[str, Any]:
        """L·∫•y th√¥ng tin v·ªÅ model"""
        return {
            'model_path': str(self.model_path),
            'model_url': self.model_url,
            'is_loaded': self.is_loaded(),
            'model_size_mb': self.model_path.stat().st_size / (1024**2) if self.model_path.exists() else 0,
            'model_kwargs': self.model_kwargs
        }
