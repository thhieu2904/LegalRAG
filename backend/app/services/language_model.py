import logging
import os
import requests
from pathlib import Path
from typing import Optional, List, Dict, Any
from llama_cpp import Llama
import time
from ..core.config import settings

logger = logging.getLogger(__name__)

class LLMService:
    """Service qu·∫£n l√Ω PhoGPT model t·ª´ HuggingFace v·ªõi VRAM Optimization"""
    
    def __init__(self, model_path: Optional[str] = None, model_url: Optional[str] = None, **kwargs):
        # Use absolute path to avoid issues when running from different directories
        if model_path:
            self.model_path = Path(model_path)
        else:
            # Use the computed property from settings which gives absolute path
            self.model_path = settings.llm_model_file_path
            
        self.model_url = model_url or settings.llm_model_url
        self.model = None
        self.model_loaded = False
        
        # C·∫•u h√¨nh GPU + CPU hybrid cho t·ªëi ∆∞u performance
        self.model_kwargs = {
            'n_ctx': kwargs.get('n_ctx', settings.n_ctx),
            'n_threads': kwargs.get('n_threads', settings.n_threads),
            'n_gpu_layers': kwargs.get('n_gpu_layers', settings.n_gpu_layers),  # GPU acceleration
            'n_batch': kwargs.get('n_batch', settings.n_batch),  # Batch size for prompt processing
            'verbose': False,
            # Th√™m GPU acceleration settings
            'use_mmap': True,  # Memory-mapped files
            'use_mlock': True,  # Keep model in RAM
            'f16_kv': True,    # Use half precision for key/value cache
        }
        
        # T·∫£i model n·∫øu ch∆∞a c√≥
        logger.info(f"Checking model path: {self.model_path}")
        logger.info(f"Model exists: {self.model_path.exists()}")
        
        if not self.model_path.exists():
            logger.info("Model file not found, attempting to download...")
            self._download_model()
        else:
            logger.info("Model file found, skipping download")
        
        # VRAM Optimization: Load model khi c·∫ßn thi·∫øt
        # self._load_model()  # Comment out ƒë·ªÉ load on-demand
    
    def _download_model(self):
        """T·∫£i model t·ª´ HuggingFace"""
        logger.info(f"Downloading model from {self.model_url}")
        
        try:
            # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
            self.model_path.parent.mkdir(parents=True, exist_ok=True)
            
            response = requests.get(self.model_url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded_size = 0
            
            with open(self.model_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)
                        
                        if total_size > 0:
                            progress = (downloaded_size / total_size) * 100
                            if downloaded_size % (1024 * 1024 * 100) == 0:  # Log every 100MB
                                logger.info(f"Downloaded {progress:.1f}% ({downloaded_size / (1024**3):.2f} GB)")
            
            logger.info(f"Model downloaded successfully: {self.model_path}")
            
        except Exception as e:
            logger.error(f"Failed to download model: {e}")
            if self.model_path.exists():
                self.model_path.unlink()  # X√≥a file b·ªã l·ªói
            raise
    
    def _load_model(self):
        """Load model v√†o memory"""
        if self.model_loaded:
            return
            
        try:
            logger.info(f"Loading LLM model from {self.model_path}")
            self.model = Llama(model_path=str(self.model_path), **self.model_kwargs)
            self.model_loaded = True
            logger.info("‚úÖ LLM model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load LLM model: {e}")
            self.model = None
            self.model_loaded = False
            raise
    
    def unload_model(self):
        """Unload model ƒë·ªÉ gi·∫£i ph√≥ng VRAM"""
        if self.model is not None:
            logger.info("üîÑ Unloading LLM model to free VRAM...")
            del self.model
            self.model = None
            self.model_loaded = False
            
            # Force garbage collection
            import gc
            gc.collect()
            logger.info("‚úÖ LLM model unloaded, VRAM freed")
    
    def ensure_loaded(self):
        """Ensure model is loaded - load n·∫øu ch∆∞a c√≥"""
        if not self.model_loaded or self.model is None:
            self._load_model()
    
    def is_model_loaded(self) -> bool:
        """Check if model is currently loaded"""
        return self.model_loaded and self.model is not None
    
    def _format_prompt(
        self, 
        system_prompt: str, 
        user_query: str, 
        context: str = "",
        chat_history: Optional[List[Dict[str, str]]] = None
    ) -> str:
        """
        Format prompt theo TEMPLATE CH√çNH TH·ª®C c·ªßa PhoGPT-4B-Chat
        PROMPT_TEMPLATE = "### C√¢u h·ªèi: {instruction}\n### Tr·∫£ l·ªùi:"
        
        ƒê√¢y l√† format ƒê√öNG theo t√†i li·ªáu ch√≠nh th·ª©c, kh√¥ng ph·∫£i prompt bleeding!
        """
        
        # Build instruction t·ª´ context v√† user query
        instruction_parts = []
        
        # 1. System prompt (n·∫øu c√≥)
        if system_prompt:
            instruction_parts.append(system_prompt)
        
        # 2. Chat history (n·∫øu c√≥) 
        if chat_history:
            for turn in chat_history:
                role = turn.get("role")
                content = turn.get("content")
                if role and content:
                    if role == "user":
                        instruction_parts.append(f"Ng∆∞·ªùi d√πng h·ªèi: {content}")
                    elif role == "assistant":
                        instruction_parts.append(f"Tr·ª£ l√Ω ƒë√£ tr·∫£ l·ªùi: {content}")
        
        # 3. Context (n·∫øu c√≥)
        if context:
            instruction_parts.append(f"Th√¥ng tin tham kh·∫£o:\n{context}")
        
        # 4. Query hi·ªán t·∫°i
        instruction_parts.append(f"C√¢u h·ªèi c·∫ßn tr·∫£ l·ªùi: {user_query}")
        
        # Combine instruction
        full_instruction = "\n\n".join(instruction_parts)
        
        # Apply OFFICIAL PhoGPT template
        formatted_prompt = f"### C√¢u h·ªèi: {full_instruction}\n### Tr·∫£ l·ªùi:"
        
        # Log ƒë·ªÉ debug
        logger.debug(f"Official PhoGPT format applied")
        
        return formatted_prompt
    
    def generate_response(
        self, 
        user_query: str, 
        context: str = "", 
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        system_prompt: Optional[str] = None,
        chat_history: Optional[List[Dict[str, str]]] = None  # THAM S·ªê M·ªöI cho ChatML
    ) -> Dict[str, Any]:
        """Sinh response t·ª´ model - VRAM optimized v·ªõi on-demand loading"""
        
        # VRAM Optimization: Ensure model is loaded
        self.ensure_loaded()
        
        if not self.model:
            raise Exception("Model not loaded")
        
        # S·ª≠ d·ª•ng values t·ª´ config - LO·∫†I B·ªé HARDCODE
        if max_tokens is None:
            max_tokens = settings.max_tokens  # L·∫•y t·ª´ .env thay v√¨ hardcode
        if temperature is None:
            temperature = settings.temperature  # L·∫•y t·ª´ .env thay v√¨ hardcode
        
        # System prompt t·ªëi ∆∞u cho legal domain - IMPROVED VERSION
        if system_prompt is None:
            system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam. 

QUY T·∫ÆC B·∫ÆT BU·ªòC:
1. CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin trong t√†i li·ªáu c√≥ s·∫µn
2. Tr·∫£ l·ªùi NG·∫ÆN G·ªåN, CH√çNH X√ÅC v√† TR·ª∞C TI·∫æP
3. KH√îNG t·ª± s√°ng t·∫°o th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu
4. KH√îNG ƒë·∫∑t th√™m c√¢u h·ªèi
5. N·∫øu h·ªèi v·ªÅ ph√≠/l·ªá ph√≠ - tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin "L·ªÜ PH√ç" trong t√†i li·ªáu

ƒê·ªäNH D·∫†NG TR·∫¢ L·ªúI:
- C√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn
- D·∫´n ch·ª©ng t·ª´ t√†i li·ªáu n·∫øu c√≥

Tr·∫£ l·ªùi ch√≠nh x√°c, ng·∫Øn g·ªçn."""
        
        # Format prompt theo chu·∫©n ChatML thay v√¨ ### C√¢u h·ªèi: ### Tr·∫£ l·ªùi:
        formatted_prompt = self._format_prompt(
            system_prompt, 
            user_query, 
            context, 
            chat_history  # Truy·ªÅn chat_history c√≥ c·∫•u tr√∫c
        )
        
        # ======================================================================
        # === QU·∫¢N L√ù CONTEXT WINDOW CH·ª¶ ƒê·ªòNG (B·∫¢O V·ªÜ KH·ªéI OVERFLOW) ===
        # ======================================================================
        
        # 1. ∆Ø·ªõc t√≠nh k√≠ch th∆∞·ªõc token c·ªßa prompt ƒë·∫ßu v√†o (1 token ‚âà 3 k√Ω t·ª± ti·∫øng Vi·ªát)
        prompt_tokens_estimated = len(formatted_prompt) // 3
        
        # 2. L·∫•y t·ªïng context window t·ª´ c·∫•u h√¨nh (.env)
        total_context_window = self.model_kwargs.get('n_ctx', settings.n_ctx)
        
        # 3. T√≠nh to√°n kh√¥ng gian c√≤n l·∫°i ƒë·ªÉ sinh token (tr·ª´ ƒëi buffer an to√†n)
        safety_buffer = 256  # Buffer an to√†n ƒë·ªÉ tr√°nh edge cases
        available_space_for_response = total_context_window - prompt_tokens_estimated - safety_buffer
        
        # üî• NG∆Ø·ª†NG T·ªêI THI·ªÇU ƒê·ªÇ SINH C√ÇU TR·∫¢ L·ªúI C√ì √ù NGHƒ®A
        MINIMUM_RESPONSE_TOKENS = 64  # T·ªëi thi·ªÉu 64 tokens = ~200 chars ti·∫øng Vi·ªát
        
        if available_space_for_response <= 0:
            logger.error(f"üö® Context window overflow! Prompt ({prompt_tokens_estimated} tokens) ƒë√£ v∆∞·ª£t qu√° gi·ªõi h·∫°n ({total_context_window}).")
            raise ValueError(f"Prompt ƒë·∫ßu v√†o qu√° l·ªõn ({prompt_tokens_estimated} tokens), kh√¥ng c√≤n kh√¥ng gian ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi. Gi·ªõi h·∫°n: {total_context_window} tokens.")
            
        if available_space_for_response <= MINIMUM_RESPONSE_TOKENS:
            logger.error(f"üö® Kh√¥ng ƒë·ªß kh√¥ng gian ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi c√≥ √Ω nghƒ©a. C·∫ßn t·ªëi thi·ªÉu {MINIMUM_RESPONSE_TOKENS} tokens, ch·ªâ c√≤n {available_space_for_response} tokens.")
            # Tr·∫£ v·ªÅ m·ªôt response th√¥ng b√°o thay v√¨ crash
            return {
                'response': f"Xin l·ªói, ng·ªØ c·∫£nh qu√° ph·ª©c t·∫°p ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi trong gi·ªõi h·∫°n hi·ªán t·∫°i. (C·∫ßn {MINIMUM_RESPONSE_TOKENS} tokens, ch·ªâ c√≤n {available_space_for_response} tokens)",
                'processing_time': 0.0,
                'prompt_tokens': prompt_tokens_estimated,
                'completion_tokens': 0,
                'total_tokens': prompt_tokens_estimated,
                'context_info': {
                    'total_context_window': total_context_window,
                    'prompt_tokens_estimated': prompt_tokens_estimated,
                    'available_space': available_space_for_response,
                    'max_tokens_requested': max_tokens,
                    'max_tokens_used': 0,
                    'was_adjusted': True,
                    'error_reason': 'insufficient_space'
                }
            }
            
        # 4. ƒêi·ªÅu ch·ªânh ƒë·ªông `max_tokens` ƒë·ªÉ kh√¥ng v∆∞·ª£t qu√° kh√¥ng gian c√≤n l·∫°i
        original_max_tokens = max_tokens
        dynamic_max_tokens = min(max_tokens, available_space_for_response)
        
        # üî• ƒê·∫¢M B·∫¢O DYNAMIC_MAX_TOKENS KH√îNG QU√Å NH·ªé
        if dynamic_max_tokens < MINIMUM_RESPONSE_TOKENS:
            logger.warning(f"‚ö†Ô∏è Dynamic max_tokens ({dynamic_max_tokens}) qu√° nh·ªè, ƒë·∫∑t v·ªÅ minimum {MINIMUM_RESPONSE_TOKENS}")
            dynamic_max_tokens = MINIMUM_RESPONSE_TOKENS
        
        logger.info(f"üìè Context Info: Total={total_context_window}, Prompt‚âà{prompt_tokens_estimated}, Available={available_space_for_response}")
        if dynamic_max_tokens != original_max_tokens:
            logger.warning(f"‚ö†Ô∏è Max Tokens adjusted: {original_max_tokens} ‚Üí {dynamic_max_tokens} (to prevent overflow)")
        else:
            logger.info(f"‚úÖ Max Tokens: {dynamic_max_tokens} (no adjustment needed)")
        
        # ======================================================================
        
        try:
            start_time = time.time()
            
            # Generate v·ªõi parameters t·ªëi ∆∞u ƒë·ªÉ tr√°nh l·∫∑p - S·ª¨ D·ª§NG DYNAMIC MAX_TOKENS
            response = self.model(
                formatted_prompt,
                max_tokens=dynamic_max_tokens,  # ‚ú® S·ª¨ D·ª§NG GI√Å TR·ªä ƒê√É ƒêI·ªÄU CH·ªàNH
                temperature=temperature,
                top_p=0.9,  # Nucleus sampling ƒë·ªÉ tƒÉng ƒëa d·∫°ng
                top_k=40,   # Top-K sampling
                repeat_penalty=1.1,  # Penalty cho t·ª´ l·∫∑p
                stop=["### C√¢u h·ªèi:", "\n### C√¢u h·ªèi:", "### Tr·∫£ l·ªùi:", "\n### Tr·∫£ l·ªùi:"],  # üî• STOP TOKENS CHO FORMAT CH√çNH TH·ª®C
                echo=False,
                stream=False  # Ensure non-streaming response
            )
            
            processing_time = time.time() - start_time
            
            # Safely extract text from response
            if isinstance(response, dict) and 'choices' in response:
                generated_text = response['choices'][0]['text'].strip()
                
                # Get token usage safely
                usage = response.get('usage', {})
                prompt_tokens = usage.get('prompt_tokens', 0)
                completion_tokens = usage.get('completion_tokens', 0)
                total_tokens = usage.get('total_tokens', prompt_tokens + completion_tokens)
            else:
                raise Exception("Invalid response format from model")
            
            # Clean up response - remove repetitive patterns
            generated_text = self._clean_repetitive_response(generated_text)
            
            result = {
                'response': generated_text,
                'processing_time': processing_time,
                'prompt_tokens': prompt_tokens,
                'completion_tokens': completion_tokens,
                'total_tokens': total_tokens,
                # Th√™m th√¥ng tin debug cho context management
                'context_info': {
                    'total_context_window': total_context_window,
                    'prompt_tokens_estimated': prompt_tokens_estimated,
                    'available_space': available_space_for_response,
                    'max_tokens_requested': original_max_tokens,
                    'max_tokens_used': dynamic_max_tokens,
                    'was_adjusted': dynamic_max_tokens != original_max_tokens
                }
            }
            
            logger.info(f"‚úÖ Generated response in {processing_time:.2f}s, "
                       f"tokens: {result['total_tokens']} "
                       f"(prompt: {prompt_tokens}, completion: {completion_tokens})")
            
            return result
            
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            raise
    
    def _clean_repetitive_response(self, text: str) -> str:
        """D·ªçn d·∫πp response ƒë·ªÉ lo·∫°i b·ªè patterns l·∫∑p l·∫°i v√† official format artifacts"""
        import re
        
        # üî• Lo·∫°i b·ªè official format patterns c√≥ th·ªÉ r√≤ r·ªâ
        text = re.sub(r'###\s*C√¢u h·ªèi\s*:', '', text, flags=re.IGNORECASE)
        text = re.sub(r'###\s*Tr·∫£ l·ªùi\s*:', '', text, flags=re.IGNORECASE)
        
        # üî• QUAN TR·ªåNG: Lo·∫°i b·ªè c√¢u h·ªèi th·ª´a do model t·ª± t·∫°o
        text = re.sub(r'C√¢u h·ªèi c·∫ßn tr·∫£ l·ªùi th√™m:.*$', '', text, flags=re.MULTILINE | re.IGNORECASE)
        text = re.sub(r'C√¢u h·ªèi ti·∫øp theo:.*$', '', text, flags=re.MULTILINE | re.IGNORECASE)
        text = re.sub(r'C√¢u h·ªèi kh√°c:.*$', '', text, flags=re.MULTILINE | re.IGNORECASE)
        text = re.sub(r'Th·∫Øc m·∫Øc kh√°c:.*$', '', text, flags=re.MULTILINE | re.IGNORECASE)
        
        # Lo·∫°i b·ªè role indicators c√≥ th·ªÉ r√≤ r·ªâ
        text = re.sub(r'^\s*(user|assistant|system)\s*[:]\s*', '', text, flags=re.MULTILINE)
        text = re.sub(r'^\s*(Ng∆∞·ªùi d√πng|Tr·ª£ l√Ω|H·ªá th·ªëng)\s*[:]\s*', '', text, flags=re.MULTILINE)
        
        # Lo·∫°i b·ªè patterns l·∫∑p ki·ªÉu A. B. C. XI. XI. XI.
        text = re.sub(r'([A-Z]\.)\s*TH·ª¶\s*T·ª§C\s*NU√îI\s*CON\s*NU√îI\s*TRONG\s*N∆Ø·ªöC\s*\n*', '', text, flags=re.IGNORECASE)
        
        # Lo·∫°i b·ªè repetitive numbered/lettered lists
        text = re.sub(r'([A-Z]{1,3}\.)\s*(.+?)\n(\1\s*\2\n){2,}', r'\1 \2\n', text, flags=re.IGNORECASE)
        
        # Lo·∫°i b·ªè c√¢u l·∫∑p l·∫°i li√™n ti·∫øp (3+ l·∫ßn)
        lines = text.split('\n')
        cleaned_lines = []
        prev_line = ""
        repeat_count = 0
        
        for line in lines:
            line = line.strip()
            if line == prev_line and line:
                repeat_count += 1
                if repeat_count < 2:  # Cho ph√©p l·∫∑p t·ªëi ƒëa 1 l·∫ßn
                    cleaned_lines.append(line)
            else:
                repeat_count = 0
                cleaned_lines.append(line)
                prev_line = line
        
        cleaned_text = '\n'.join(cleaned_lines).strip()
        
        # Gi·ªõi h·∫°n ƒë·ªô d√†i response h·ª£p l√Ω
        if len(cleaned_text) > 2000:
            # C·∫Øt t·∫°i c√¢u cu·ªëi c√πng trong 2000 k√Ω t·ª± ƒë·∫ßu
            truncated = cleaned_text[:2000]
            last_sentence = truncated.rfind('.')
            if last_sentence > 1000:  # Ch·ªâ c·∫Øt n·∫øu c√≤n ƒë·ªß n·ªôi dung
                cleaned_text = truncated[:last_sentence + 1]
        
        return cleaned_text
    
    def is_loaded(self) -> bool:
        """Ki·ªÉm tra model ƒë√£ ƒë∆∞·ª£c load ch∆∞a"""
        return self.model is not None
    
    def get_model_info(self) -> Dict[str, Any]:
        """L·∫•y th√¥ng tin v·ªÅ model"""
        return {
            'model_path': str(self.model_path),
            'model_url': self.model_url,
            'is_loaded': self.is_loaded(),
            'model_size_mb': self.model_path.stat().st_size / (1024**2) if self.model_path.exists() else 0,
            'model_kwargs': self.model_kwargs
        }
