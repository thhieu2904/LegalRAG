import logging
import os
import requests
from pathlib import Path
from typing import Optional, List, Dict, Any
from llama_cpp import Llama
import time
from ..core.config import settings

logger = logging.getLogger(__name__)

class LLMService:
    """Service qu·∫£n l√Ω PhoGPT model t·ª´ HuggingFace v·ªõi VRAM Optimization"""
    
    def __init__(self, model_path: Optional[str] = None, model_url: Optional[str] = None, **kwargs):
        # Use absolute path to avoid issues when running from different directories
        if model_path:
            self.model_path = Path(model_path)
        else:
            # Use the computed property from settings which gives absolute path
            self.model_path = settings.llm_model_file_path
            
        self.model_url = model_url or settings.llm_model_url
        self.model = None
        self.model_loaded = False
        
        # C·∫•u h√¨nh GPU + CPU hybrid cho t·ªëi ∆∞u performance
        self.model_kwargs = {
            'n_ctx': kwargs.get('n_ctx', settings.n_ctx),
            'n_threads': kwargs.get('n_threads', settings.n_threads),
            'n_gpu_layers': kwargs.get('n_gpu_layers', settings.n_gpu_layers),  # GPU acceleration
            'n_batch': kwargs.get('n_batch', settings.n_batch),  # Batch size for prompt processing
            'verbose': False,
            # Th√™m GPU acceleration settings
            'use_mmap': True,  # Memory-mapped files
            'use_mlock': True,  # Keep model in RAM
            'f16_kv': True,    # Use half precision for key/value cache
        }
        
        # T·∫£i model n·∫øu ch∆∞a c√≥
        logger.info(f"Checking model path: {self.model_path}")
        logger.info(f"Model exists: {self.model_path.exists()}")
        
        if not self.model_path.exists():
            logger.info("Model file not found, attempting to download...")
            self._download_model()
        else:
            logger.info("Model file found, skipping download")
        
        # VRAM Optimization: Load model khi c·∫ßn thi·∫øt
        # self._load_model()  # Comment out ƒë·ªÉ load on-demand
    
    def _download_model(self):
        """T·∫£i model t·ª´ HuggingFace"""
        logger.info(f"Downloading model from {self.model_url}")
        
        try:
            # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
            self.model_path.parent.mkdir(parents=True, exist_ok=True)
            
            response = requests.get(self.model_url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded_size = 0
            
            with open(self.model_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)
                        
                        if total_size > 0:
                            progress = (downloaded_size / total_size) * 100
                            if downloaded_size % (1024 * 1024 * 100) == 0:  # Log every 100MB
                                logger.info(f"Downloaded {progress:.1f}% ({downloaded_size / (1024**3):.2f} GB)")
            
            logger.info(f"Model downloaded successfully: {self.model_path}")
            
        except Exception as e:
            logger.error(f"Failed to download model: {e}")
            if self.model_path.exists():
                self.model_path.unlink()  # X√≥a file b·ªã l·ªói
            raise
    
    def _load_model(self):
        """Load model v√†o memory"""
        if self.model_loaded:
            return
            
        try:
            logger.info(f"Loading LLM model from {self.model_path}")
            self.model = Llama(model_path=str(self.model_path), **self.model_kwargs)
            self.model_loaded = True
            logger.info("‚úÖ LLM model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load LLM model: {e}")
            self.model = None
            self.model_loaded = False
            raise
    
    def unload_model(self):
        """Unload model ƒë·ªÉ gi·∫£i ph√≥ng VRAM"""
        if self.model is not None:
            logger.info("üîÑ Unloading LLM model to free VRAM...")
            del self.model
            self.model = None
            self.model_loaded = False
            
            # Force garbage collection
            import gc
            gc.collect()
            logger.info("‚úÖ LLM model unloaded, VRAM freed")
    
    def ensure_loaded(self):
        """Ensure model is loaded - load n·∫øu ch∆∞a c√≥"""
        if not self.model_loaded or self.model is None:
            self._load_model()
    
    def is_model_loaded(self) -> bool:
        """Check if model is currently loaded"""
        return self.model_loaded and self.model is not None
    
    def _format_prompt(
        self, 
        system_prompt: str, 
        user_query: str, 
        context: str = "",
        chat_history: Optional[List[Dict[str, str]]] = None
    ) -> str:
        """
        Format prompt theo chu·∫©n ChatML template m√† PhoGPT-Chat mong ƒë·ª£i.
        Template: <|im_start|>role\ncontent<|im_end|>
        
        ƒê√¢y l√† c√°ch ƒê√öNG ƒë·ªÉ s·ª≠ d·ª•ng PhoGPT-Chat, thay v√¨ format ### C√¢u h·ªèi: ### Tr·∫£ l·ªùi:
        """
        messages = []
        
        # 1. System Prompt - ch·ªâ d·∫´n vai tr√≤ v√† quy t·∫Øc
        if system_prompt:
            messages.append(f"<|im_start|>system\n{system_prompt}<|im_end|>")
        
        # 2. Chat History (n·∫øu c√≥) - l·ªãch s·ª≠ h·ªôi tho·∫°i c√≥ c·∫•u tr√∫c
        if chat_history:
            for turn in chat_history:
                role = turn.get("role")
                content = turn.get("content")
                if role and content:
                    messages.append(f"<|im_start|>{role}\n{content}<|im_end|>")

        # 3. User Query hi·ªán t·∫°i (k√®m context n·∫øu c√≥)
        user_content = ""
        if context:
            user_content += f"D·ª±a v√†o th√¥ng tin t√†i li·ªáu sau ƒë√¢y:\n--- B·∫ÆT ƒê·∫¶U T√ÄI LI·ªÜU ---\n{context}\n--- K·∫æT TH√öC T√ÄI LI·ªÜU ---\n\n"
        user_content += f"H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau: {user_query}"
        
        messages.append(f"<|im_start|>user\n{user_content}<|im_end|>")
        
        # 4. Th√™m d·∫•u hi·ªáu ƒë·ªÉ model b·∫Øt ƒë·∫ßu tr·∫£ l·ªùi
        messages.append("<|im_start|>assistant")

        formatted_prompt = "\n".join(messages)
        
        # Log ƒë·ªÉ debug (ch·ªâ hi·ªÉn th·ªã c√°c token ƒë·∫∑c bi·ªát)
        logger.debug(f"ChatML template structure: {len([m for m in messages if 'im_start' in m])} messages")
        
        return formatted_prompt
    
    def generate_response(
        self, 
        user_query: str, 
        context: str = "", 
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        system_prompt: Optional[str] = None,
        chat_history: Optional[List[Dict[str, str]]] = None  # THAM S·ªê M·ªöI cho ChatML
    ) -> Dict[str, Any]:
        """Sinh response t·ª´ model - VRAM optimized v·ªõi on-demand loading"""
        
        # VRAM Optimization: Ensure model is loaded
        self.ensure_loaded()
        
        if not self.model:
            raise Exception("Model not loaded")
        
        # S·ª≠ d·ª•ng values t·ª´ config - LO·∫†I B·ªé HARDCODE
        if max_tokens is None:
            max_tokens = settings.max_tokens  # L·∫•y t·ª´ .env thay v√¨ hardcode
        if temperature is None:
            temperature = settings.temperature  # L·∫•y t·ª´ .env thay v√¨ hardcode
        
        # System prompt t·ªëi ∆∞u cho legal domain - IMPROVED VERSION
        if system_prompt is None:
            system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam. 

QUY T·∫ÆC B·∫ÆT BU·ªòC:
1. CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin trong t√†i li·ªáu
2. Tr·∫£ l·ªùi NG·∫ÆN G·ªåN v√† TR·ª∞C TI·∫æP 
3. KH√îNG t·ª± s√°ng t·∫°o th√¥ng tin
4. N·∫øu h·ªèi v·ªÅ ph√≠/ti·ªÅn - t√¨m ch√≠nh x√°c th√¥ng tin "L·ªÜ PH√ç"

Tr·∫£ l·ªùi ch√≠nh x√°c, ng·∫Øn g·ªçn."""
        
        # Format prompt theo chu·∫©n ChatML thay v√¨ ### C√¢u h·ªèi: ### Tr·∫£ l·ªùi:
        formatted_prompt = self._format_prompt(
            system_prompt, 
            user_query, 
            context, 
            chat_history  # Truy·ªÅn chat_history c√≥ c·∫•u tr√∫c
        )
        
        # ======================================================================
        # === QU·∫¢N L√ù CONTEXT WINDOW CH·ª¶ ƒê·ªòNG (B·∫¢O V·ªÜ KH·ªéI OVERFLOW) ===
        # ======================================================================
        
        # 1. ∆Ø·ªõc t√≠nh k√≠ch th∆∞·ªõc token c·ªßa prompt ƒë·∫ßu v√†o (1 token ‚âà 3 k√Ω t·ª± ti·∫øng Vi·ªát)
        prompt_tokens_estimated = len(formatted_prompt) // 3
        
        # 2. L·∫•y t·ªïng context window t·ª´ c·∫•u h√¨nh (.env)
        total_context_window = self.model_kwargs.get('n_ctx', settings.n_ctx)
        
        # 3. T√≠nh to√°n kh√¥ng gian c√≤n l·∫°i ƒë·ªÉ sinh token (tr·ª´ ƒëi buffer an to√†n)
        safety_buffer = 256  # Buffer an to√†n ƒë·ªÉ tr√°nh edge cases
        available_space_for_response = total_context_window - prompt_tokens_estimated - safety_buffer
        
        if available_space_for_response <= 0:
            logger.error(f"üö® Context window overflow! Prompt ({prompt_tokens_estimated} tokens) ƒë√£ v∆∞·ª£t qu√° gi·ªõi h·∫°n ({total_context_window}).")
            raise ValueError(f"Prompt ƒë·∫ßu v√†o qu√° l·ªõn ({prompt_tokens_estimated} tokens), kh√¥ng c√≤n kh√¥ng gian ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi. Gi·ªõi h·∫°n: {total_context_window} tokens.")
            
        # 4. ƒêi·ªÅu ch·ªânh ƒë·ªông `max_tokens` ƒë·ªÉ kh√¥ng v∆∞·ª£t qu√° kh√¥ng gian c√≤n l·∫°i
        original_max_tokens = max_tokens
        dynamic_max_tokens = min(max_tokens, available_space_for_response)
        
        logger.info(f"üìè Context Info: Total={total_context_window}, Prompt‚âà{prompt_tokens_estimated}, Available={available_space_for_response}")
        if dynamic_max_tokens != original_max_tokens:
            logger.warning(f"‚ö†Ô∏è Max Tokens adjusted: {original_max_tokens} ‚Üí {dynamic_max_tokens} (to prevent overflow)")
        else:
            logger.info(f"‚úÖ Max Tokens: {dynamic_max_tokens} (no adjustment needed)")
        
        # ======================================================================
        
        try:
            start_time = time.time()
            
            # Generate v·ªõi parameters t·ªëi ∆∞u ƒë·ªÉ tr√°nh l·∫∑p - S·ª¨ D·ª§NG DYNAMIC MAX_TOKENS
            response = self.model(
                formatted_prompt,
                max_tokens=dynamic_max_tokens,  # ‚ú® S·ª¨ D·ª§NG GI√Å TR·ªä ƒê√É ƒêI·ªÄU CH·ªàNH
                temperature=temperature,
                top_p=0.9,  # Nucleus sampling ƒë·ªÉ tƒÉng ƒëa d·∫°ng
                top_k=40,   # Top-K sampling
                repeat_penalty=1.1,  # Penalty cho t·ª´ l·∫∑p
                stop=["<|im_end|>", "<|im_start|>", "\n<|im_start|>"],  # ChatML stop tokens
                echo=False,
                stream=False  # Ensure non-streaming response
            )
            
            processing_time = time.time() - start_time
            
            # Safely extract text from response
            if isinstance(response, dict) and 'choices' in response:
                generated_text = response['choices'][0]['text'].strip()
                
                # Get token usage safely
                usage = response.get('usage', {})
                prompt_tokens = usage.get('prompt_tokens', 0)
                completion_tokens = usage.get('completion_tokens', 0)
                total_tokens = usage.get('total_tokens', prompt_tokens + completion_tokens)
            else:
                raise Exception("Invalid response format from model")
            
            # Clean up response - remove repetitive patterns
            generated_text = self._clean_repetitive_response(generated_text)
            
            result = {
                'response': generated_text,
                'processing_time': processing_time,
                'prompt_tokens': prompt_tokens,
                'completion_tokens': completion_tokens,
                'total_tokens': total_tokens,
                # Th√™m th√¥ng tin debug cho context management
                'context_info': {
                    'total_context_window': total_context_window,
                    'prompt_tokens_estimated': prompt_tokens_estimated,
                    'available_space': available_space_for_response,
                    'max_tokens_requested': original_max_tokens,
                    'max_tokens_used': dynamic_max_tokens,
                    'was_adjusted': dynamic_max_tokens != original_max_tokens
                }
            }
            
            logger.info(f"‚úÖ Generated response in {processing_time:.2f}s, "
                       f"tokens: {result['total_tokens']} "
                       f"(prompt: {prompt_tokens}, completion: {completion_tokens})")
            
            return result
            
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            raise
    
    def _clean_repetitive_response(self, text: str) -> str:
        """D·ªçn d·∫πp response ƒë·ªÉ lo·∫°i b·ªè patterns l·∫∑p l·∫°i v√† ChatML tokens r√≤ r·ªâ"""
        import re
        
        # üî• QUAN TR·ªåNG: Lo·∫°i b·ªè ChatML tokens c√≥ th·ªÉ r√≤ r·ªâ
        text = re.sub(r'<\|im_start\|>', '', text)
        text = re.sub(r'<\|im_end\|>', '', text) 
        text = re.sub(r'<\|.*?\|>', '', text)  # Lo·∫°i b·ªè b·∫•t k·ª≥ special token n√†o kh√°c
        
        # üî• CRITICAL: Lo·∫°i b·ªè c√°c pattern format c≈© c√≥ th·ªÉ r√≤ r·ªâ t·ª´ context
        text = re.sub(r'###\s*C√¢u h·ªèi\s*:', '', text, flags=re.IGNORECASE)
        text = re.sub(r'###\s*Tr·∫£ l·ªùi\s*:', '', text, flags=re.IGNORECASE)
        text = re.sub(r'C√ÇUH·ªéI\s*:', '', text, flags=re.IGNORECASE)
        text = re.sub(r'TR·∫¢\s*L·ªúI\s*:', '', text, flags=re.IGNORECASE)
        
        # Lo·∫°i b·ªè role indicators c√≥ th·ªÉ r√≤ r·ªâ
        text = re.sub(r'^\s*(user|assistant|system)\s*[:]\s*', '', text, flags=re.MULTILINE)
        
        # Lo·∫°i b·ªè patterns l·∫∑p ki·ªÉu A. B. C. XI. XI. XI.
        text = re.sub(r'([A-Z]\.)\s*TH·ª¶\s*T·ª§C\s*NU√îI\s*CON\s*NU√îI\s*TRONG\s*N∆Ø·ªöC\s*\n*', '', text, flags=re.IGNORECASE)
        
        # Lo·∫°i b·ªè repetitive numbered/lettered lists
        text = re.sub(r'([A-Z]{1,3}\.)\s*(.+?)\n(\1\s*\2\n){2,}', r'\1 \2\n', text, flags=re.IGNORECASE)
        
        # Lo·∫°i b·ªè c√¢u l·∫∑p l·∫°i li√™n ti·∫øp (3+ l·∫ßn)
        lines = text.split('\n')
        cleaned_lines = []
        prev_line = ""
        repeat_count = 0
        
        for line in lines:
            line = line.strip()
            if line == prev_line and line:
                repeat_count += 1
                if repeat_count < 2:  # Cho ph√©p l·∫∑p t·ªëi ƒëa 1 l·∫ßn
                    cleaned_lines.append(line)
            else:
                repeat_count = 0
                cleaned_lines.append(line)
                prev_line = line
        
        cleaned_text = '\n'.join(cleaned_lines).strip()
        
        # Gi·ªõi h·∫°n ƒë·ªô d√†i response h·ª£p l√Ω
        if len(cleaned_text) > 2000:
            # C·∫Øt t·∫°i c√¢u cu·ªëi c√πng trong 2000 k√Ω t·ª± ƒë·∫ßu
            truncated = cleaned_text[:2000]
            last_sentence = truncated.rfind('.')
            if last_sentence > 1000:  # Ch·ªâ c·∫Øt n·∫øu c√≤n ƒë·ªß n·ªôi dung
                cleaned_text = truncated[:last_sentence + 1]
        
        return cleaned_text
    
    def is_loaded(self) -> bool:
        """Ki·ªÉm tra model ƒë√£ ƒë∆∞·ª£c load ch∆∞a"""
        return self.model is not None
    
    def get_model_info(self) -> Dict[str, Any]:
        """L·∫•y th√¥ng tin v·ªÅ model"""
        return {
            'model_path': str(self.model_path),
            'model_url': self.model_url,
            'is_loaded': self.is_loaded(),
            'model_size_mb': self.model_path.stat().st_size / (1024**2) if self.model_path.exists() else 0,
            'model_kwargs': self.model_kwargs
        }
