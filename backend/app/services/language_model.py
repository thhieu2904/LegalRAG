import logging
import os
import requests
from pathlib import Path
from typing import Optional, List, Dict, Any
from llama_cpp import Llama
import time
from ..core.config import settings

logger = logging.getLogger(__name__)

class LLMService:
    """Service quáº£n lÃ½ PhoGPT model tá»« HuggingFace vá»›i VRAM Optimization"""
    
    def __init__(self, model_path: Optional[str] = None, model_url: Optional[str] = None, **kwargs):
        # Use absolute path to avoid issues when running from different directories
        if model_path:
            self.model_path = Path(model_path)
        else:
            # Use the computed property from settings which gives absolute path
            self.model_path = settings.llm_model_file_path
            
        self.model_url = model_url or settings.llm_model_url
        self.model = None
        self.model_loaded = False
        
        # Cáº¥u hÃ¬nh GPU + CPU hybrid cho tá»‘i Æ°u performance
        self.model_kwargs = {
            'n_ctx': kwargs.get('n_ctx', settings.n_ctx),
            'n_threads': kwargs.get('n_threads', settings.n_threads),
            'n_gpu_layers': kwargs.get('n_gpu_layers', settings.n_gpu_layers),  # GPU acceleration
            'n_batch': kwargs.get('n_batch', settings.n_batch),  # Batch size for prompt processing
            'verbose': False,
            # ThÃªm GPU acceleration settings
            'use_mmap': True,  # Memory-mapped files
            'use_mlock': True,  # Keep model in RAM
            'f16_kv': True,    # Use half precision for key/value cache
        }
        
        # Táº£i model náº¿u chÆ°a cÃ³
        logger.info(f"Checking model path: {self.model_path}")
        logger.info(f"Model exists: {self.model_path.exists()}")
        
        if not self.model_path.exists():
            logger.info("Model file not found, attempting to download...")
            self._download_model()
        else:
            logger.info("Model file found, skipping download")
        
        # VRAM Optimization: Load model khi cáº§n thiáº¿t
        # self._load_model()  # Comment out Ä‘á»ƒ load on-demand
    
    def _download_model(self):
        """Táº£i model tá»« HuggingFace"""
        logger.info(f"Downloading model from {self.model_url}")
        
        try:
            # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
            self.model_path.parent.mkdir(parents=True, exist_ok=True)
            
            response = requests.get(self.model_url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded_size = 0
            
            with open(self.model_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)
                        
                        if total_size > 0:
                            progress = (downloaded_size / total_size) * 100
                            if downloaded_size % (1024 * 1024 * 100) == 0:  # Log every 100MB
                                logger.info(f"Downloaded {progress:.1f}% ({downloaded_size / (1024**3):.2f} GB)")
            
            logger.info(f"Model downloaded successfully: {self.model_path}")
            
        except Exception as e:
            logger.error(f"Failed to download model: {e}")
            if self.model_path.exists():
                self.model_path.unlink()  # XÃ³a file bá»‹ lá»—i
            raise
    
    def _load_model(self):
        """Load model vÃ o memory"""
        if self.model_loaded:
            return
            
        try:
            logger.info(f"Loading LLM model from {self.model_path}")
            self.model = Llama(model_path=str(self.model_path), **self.model_kwargs)
            self.model_loaded = True
            logger.info("âœ… LLM model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load LLM model: {e}")
            self.model = None
            self.model_loaded = False
            raise
    
    def unload_model(self):
        """Unload model Ä‘á»ƒ giáº£i phÃ³ng VRAM"""
        if self.model is not None:
            logger.info("ğŸ”„ Unloading LLM model to free VRAM...")
            del self.model
            self.model = None
            self.model_loaded = False
            
            # Force garbage collection
            import gc
            gc.collect()
            logger.info("âœ… LLM model unloaded, VRAM freed")
    
    def ensure_loaded(self):
        """Ensure model is loaded - load náº¿u chÆ°a cÃ³"""
        if not self.model_loaded or self.model is None:
            self._load_model()
    
    def is_model_loaded(self) -> bool:
        """Check if model is currently loaded"""
        return self.model_loaded and self.model is not None
    
    def _format_prompt(
        self, 
        system_prompt: str, 
        user_query: str, 
        context: str = "",
        chat_history: Optional[List[Dict[str, str]]] = None
    ) -> str:
        """
        Format prompt theo TEMPLATE CHÃNH THá»¨C cá»§a PhoGPT-4B-Chat
        PROMPT_TEMPLATE = "### CÃ¢u há»i: {instruction}\n### Tráº£ lá»i:"
        
        ÄÃ¢y lÃ  format ÄÃšNG theo tÃ i liá»‡u chÃ­nh thá»©c, khÃ´ng pháº£i prompt bleeding!
        """
        
        # Build instruction tá»« context vÃ  user query
        instruction_parts = []
        
        # 1. System prompt (náº¿u cÃ³)
        if system_prompt:
            instruction_parts.append(system_prompt)
        
        # 2. Chat history (náº¿u cÃ³) 
        if chat_history:
            for turn in chat_history:
                role = turn.get("role")
                content = turn.get("content")
                if role and content:
                    if role == "user":
                        instruction_parts.append(f"NgÆ°á»i dÃ¹ng há»i: {content}")
                    elif role == "assistant":
                        instruction_parts.append(f"Trá»£ lÃ½ Ä‘Ã£ tráº£ lá»i: {content}")
        
        # 3. Context (náº¿u cÃ³)
        if context:
            instruction_parts.append(f"ThÃ´ng tin tham kháº£o:\n{context}")
        
        # 4. Query hiá»‡n táº¡i
        instruction_parts.append(f"CÃ¢u há»i cáº§n tráº£ lá»i: {user_query}")
        
        # Combine instruction
        full_instruction = "\n\n".join(instruction_parts)
        
        # Apply OFFICIAL PhoGPT template
        formatted_prompt = f"### CÃ¢u há»i: {full_instruction}\n### Tráº£ lá»i:"
        
        # Log Ä‘á»ƒ debug
        logger.debug(f"Official PhoGPT format applied")
        
        return formatted_prompt
    
    def generate_response(
        self, 
        user_query: str, 
        context: str = "", 
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        system_prompt: Optional[str] = None,
        chat_history: Optional[List[Dict[str, str]]] = None  # THAM Sá» Má»šI cho ChatML
    ) -> Dict[str, Any]:
        """Sinh response tá»« model - VRAM optimized vá»›i on-demand loading"""
        
        # VRAM Optimization: Ensure model is loaded
        self.ensure_loaded()
        
        if not self.model:
            raise Exception("Model not loaded")
        
        # Sá»­ dá»¥ng values tá»« config - LOáº I Bá» HARDCODE
        if max_tokens is None:
            max_tokens = settings.max_tokens  # Láº¥y tá»« .env thay vÃ¬ hardcode
        if temperature is None:
            temperature = settings.temperature  # Láº¥y tá»« .env thay vÃ¬ hardcode
        
        # System prompt tá»‘i Æ°u cho legal domain vá»›i enhanced metadata awareness
        if system_prompt is None:
            system_prompt = """Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn vá» phÃ¡p luáº­t Viá»‡t Nam.

ğŸš¨ QUY Táº®C Báº®T BUá»˜C - KHÃ”NG ÄÆ¯á»¢C VI PHáº M:
1. **Æ¯U TIÃŠN TUYá»†T Äá»I:** Náº¿u trong ngá»¯ cáº£nh cÃ³ thÃ´ng tin Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u báº±ng ğŸ¯, hÃ£y Æ°u tiÃªn sá»­ dá»¥ng thÃ´ng tin Ä‘Ã³ trÆ°á»›c tiÃªn
2. CHá»ˆ tráº£ lá»i dá»±a trÃªn thÃ´ng tin CÃ“ TRONG tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p
3. Tráº£ lá»i NGáº®N Gá»ŒN, CHÃNH XÃC vÃ  TRá»°C TIáº¾P (tá»‘i Ä‘a 9-10 cÃ¢u)
4. KHÃ”NG tá»± sÃ¡ng táº¡o thÃ´ng tin khÃ´ng cÃ³ trong tÃ i liá»‡u
5. KHÃ”NG Ä‘áº·t thÃªm cÃ¢u há»i

ğŸ¯ THÃ”NG TIN METADATA Cáº¦N QUAN TÃ‚M Äá»”C BIá»†T:
- Khi há»i vá» PHÃ/Lá»† PHÃ â†’ TÃ¬m pháº§n cÃ³ Ä‘Ã¡nh dáº¥u ğŸ¯ Lá»† PHÃ
- Khi há»i vá» THá»œI GIAN â†’ TÃ¬m pháº§n cÃ³ Ä‘Ã¡nh dáº¥u ğŸ¯ THá»œI GIAN Xá»¬ LÃ  
- Khi há»i vá» BIá»‚U MáºªU â†’ TÃ¬m pháº§n cÃ³ Ä‘Ã¡nh dáº¥u ğŸ¯ BIá»‚U MáºªU
- Khi há»i vá» CÆ  QUAN â†’ TÃ¬m pháº§n cÃ³ Ä‘Ã¡nh dáº¥u ğŸ¯ CÆ  QUAN THá»°C HIá»†N

ğŸ“‹ QUY Táº®C Vá»€ BIá»‚U MáºªU/Tá»œ KHAI:
- Khi thá»§ tá»¥c cÃ³ biá»ƒu máº«u Ä‘i kÃ¨m (has_form = true), hÃ£y Ä‘á» cáº­p: "Xem biá»ƒu máº«u/tá» khai Ä‘Ã­nh kÃ¨m"
- LuÃ´n kiá»ƒm tra thÃ´ng tin form trong metadata trÆ°á»›c khi tráº£ lá»i vá» biá»ƒu máº«u
- Náº¿u cÃ³ form, hÆ°á»›ng dáº«n ngÆ°á»i dÃ¹ng táº£i vá» vÃ  sá»­ dá»¥ng

Äá»ŠNH Dáº NG TRáº¢ Lá»œI:
- CÃ¢u tráº£ lá»i ngáº¯n gá»n, chÃ­nh xÃ¡c
- Æ¯u tiÃªn thÃ´ng tin Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u ğŸ¯ náº¿u cÃ³
- Náº¿u cÃ³ form Ä‘i kÃ¨m, Ä‘á» cáº­p: "ğŸ“‹ Xem biá»ƒu máº«u Ä‘Ã­nh kÃ¨m" á»Ÿ cuá»‘i cÃ¢u tráº£ lá»i
- Náº¿u thÃ´ng tin khÃ´ng cÃ³, tráº£ lá»i: "TÃ i liá»‡u khÃ´ng Ä‘á» cáº­p Ä‘áº¿n váº¥n Ä‘á» nÃ y"

Tráº£ lá»i chÃ­nh xÃ¡c, ngáº¯n gá»n."""
        
        # Format prompt theo chuáº©n ChatML thay vÃ¬ ### CÃ¢u há»i: ### Tráº£ lá»i:
        formatted_prompt = self._format_prompt(
            system_prompt, 
            user_query, 
            context, 
            chat_history  # Truyá»n chat_history cÃ³ cáº¥u trÃºc
        )
        
        # ======================================================================
        # === QUáº¢N LÃ CONTEXT WINDOW CHá»¦ Äá»˜NG (Báº¢O Vá»† KHá»I OVERFLOW) ===
        # ======================================================================
        
        # 1. Æ¯á»›c tÃ­nh kÃ­ch thÆ°á»›c token cá»§a prompt Ä‘áº§u vÃ o (1 token â‰ˆ 3 kÃ½ tá»± tiáº¿ng Viá»‡t)
        prompt_tokens_estimated = len(formatted_prompt) // 3
        
        # 2. Láº¥y tá»•ng context window tá»« cáº¥u hÃ¬nh (.env)
        total_context_window = self.model_kwargs.get('n_ctx', settings.n_ctx)
        
        # 3. TÃ­nh toÃ¡n khÃ´ng gian cÃ²n láº¡i Ä‘á»ƒ sinh token (trá»« Ä‘i buffer an toÃ n)
        safety_buffer = 256  # Buffer an toÃ n Ä‘á»ƒ trÃ¡nh edge cases
        available_space_for_response = total_context_window - prompt_tokens_estimated - safety_buffer
        
        # ğŸ”¥ NGÆ¯á» NG Tá»I THIá»‚U Äá»‚ SINH CÃ‚U TRáº¢ Lá»œI CÃ“ Ã NGHÄ¨A
        MINIMUM_RESPONSE_TOKENS = 64  # Tá»‘i thiá»ƒu 64 tokens = ~200 chars tiáº¿ng Viá»‡t
        
        if available_space_for_response <= 0:
            logger.error(f"ğŸš¨ Context window overflow! Prompt ({prompt_tokens_estimated} tokens) Ä‘Ã£ vÆ°á»£t quÃ¡ giá»›i háº¡n ({total_context_window}).")
            raise ValueError(f"Prompt Ä‘áº§u vÃ o quÃ¡ lá»›n ({prompt_tokens_estimated} tokens), khÃ´ng cÃ²n khÃ´ng gian Ä‘á»ƒ sinh cÃ¢u tráº£ lá»i. Giá»›i háº¡n: {total_context_window} tokens.")
            
        if available_space_for_response <= MINIMUM_RESPONSE_TOKENS:
            logger.error(f"ğŸš¨ KhÃ´ng Ä‘á»§ khÃ´ng gian Ä‘á»ƒ sinh cÃ¢u tráº£ lá»i cÃ³ Ã½ nghÄ©a. Cáº§n tá»‘i thiá»ƒu {MINIMUM_RESPONSE_TOKENS} tokens, chá»‰ cÃ²n {available_space_for_response} tokens.")
            # Tráº£ vá» má»™t response thÃ´ng bÃ¡o thay vÃ¬ crash
            return {
                'response': f"Xin lá»—i, ngá»¯ cáº£nh quÃ¡ phá»©c táº¡p Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i trong giá»›i háº¡n hiá»‡n táº¡i. (Cáº§n {MINIMUM_RESPONSE_TOKENS} tokens, chá»‰ cÃ²n {available_space_for_response} tokens)",
                'processing_time': 0.0,
                'prompt_tokens': prompt_tokens_estimated,
                'completion_tokens': 0,
                'total_tokens': prompt_tokens_estimated,
                'context_info': {
                    'total_context_window': total_context_window,
                    'prompt_tokens_estimated': prompt_tokens_estimated,
                    'available_space': available_space_for_response,
                    'max_tokens_requested': max_tokens,
                    'max_tokens_used': 0,
                    'was_adjusted': True,
                    'error_reason': 'insufficient_space'
                }
            }
            
        # 4. Äiá»u chá»‰nh Ä‘á»™ng `max_tokens` Ä‘á»ƒ khÃ´ng vÆ°á»£t quÃ¡ khÃ´ng gian cÃ²n láº¡i
        original_max_tokens = max_tokens
        dynamic_max_tokens = min(max_tokens, available_space_for_response)
        
        # ğŸ”¥ Äáº¢M Báº¢O DYNAMIC_MAX_TOKENS KHÃ”NG QUÃ NHá»
        if dynamic_max_tokens < MINIMUM_RESPONSE_TOKENS:
            logger.warning(f"âš ï¸ Dynamic max_tokens ({dynamic_max_tokens}) quÃ¡ nhá», Ä‘áº·t vá» minimum {MINIMUM_RESPONSE_TOKENS}")
            dynamic_max_tokens = MINIMUM_RESPONSE_TOKENS
        
        logger.info(f"ğŸ“ Context Info: Total={total_context_window}, Promptâ‰ˆ{prompt_tokens_estimated}, Available={available_space_for_response}")
        if dynamic_max_tokens != original_max_tokens:
            logger.warning(f"âš ï¸ Max Tokens adjusted: {original_max_tokens} â†’ {dynamic_max_tokens} (to prevent overflow)")
        else:
            logger.info(f"âœ… Max Tokens: {dynamic_max_tokens} (no adjustment needed)")
        
        # ======================================================================
        
        try:
            start_time = time.time()
            
            # Generate vá»›i parameters tá»‘i Æ°u Ä‘á»ƒ trÃ¡nh láº·p - Sá»¬ Dá»¤NG DYNAMIC MAX_TOKENS
            response = self.model(
                formatted_prompt,
                max_tokens=dynamic_max_tokens,  # âœ¨ Sá»¬ Dá»¤NG GIÃ TRá»Š ÄÃƒ ÄIá»€U CHá»ˆNH
                temperature=temperature,
                top_p=0.9,  # Nucleus sampling Ä‘á»ƒ tÄƒng Ä‘a dáº¡ng
                top_k=40,   # Top-K sampling
                repeat_penalty=1.1,  # Penalty cho tá»« láº·p
                stop=["### CÃ¢u há»i:", "\n### CÃ¢u há»i:", "### Tráº£ lá»i:", "\n### Tráº£ lá»i:"],  # ğŸ”¥ STOP TOKENS CHO FORMAT CHÃNH THá»¨C
                echo=False,
                stream=False  # Ensure non-streaming response
            )
            
            processing_time = time.time() - start_time
            
            # Safely extract text from response
            if isinstance(response, dict) and 'choices' in response:
                generated_text = response['choices'][0]['text'].strip()
                
                # Get token usage safely
                usage = response.get('usage', {})
                prompt_tokens = usage.get('prompt_tokens', 0)
                completion_tokens = usage.get('completion_tokens', 0)
                total_tokens = usage.get('total_tokens', prompt_tokens + completion_tokens)
            else:
                raise Exception("Invalid response format from model")
            
            # Clean up response - remove repetitive patterns
            generated_text = self._clean_repetitive_response(generated_text)
            
            result = {
                'response': generated_text,
                'processing_time': processing_time,
                'prompt_tokens': prompt_tokens,
                'completion_tokens': completion_tokens,
                'total_tokens': total_tokens,
                # ThÃªm thÃ´ng tin debug cho context management
                'context_info': {
                    'total_context_window': total_context_window,
                    'prompt_tokens_estimated': prompt_tokens_estimated,
                    'available_space': available_space_for_response,
                    'max_tokens_requested': original_max_tokens,
                    'max_tokens_used': dynamic_max_tokens,
                    'was_adjusted': dynamic_max_tokens != original_max_tokens
                }
            }
            
            logger.info(f"âœ… Generated response in {processing_time:.2f}s, "
                       f"tokens: {result['total_tokens']} "
                       f"(prompt: {prompt_tokens}, completion: {completion_tokens})")
            
            return result
            
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            raise
    
    def _clean_repetitive_response(self, text: str) -> str:
        """Dá»n dáº¹p response Ä‘á»ƒ loáº¡i bá» patterns láº·p láº¡i vÃ  official format artifacts"""
        import re
        
        # ğŸ”¥ Loáº¡i bá» official format patterns cÃ³ thá»ƒ rÃ² rá»‰
        text = re.sub(r'###\s*CÃ¢u há»i\s*:', '', text, flags=re.IGNORECASE)
        text = re.sub(r'###\s*Tráº£ lá»i\s*:', '', text, flags=re.IGNORECASE)
        
        # ğŸ”¥ QUAN TRá»ŒNG: Loáº¡i bá» cÃ¢u há»i thá»«a do model tá»± táº¡o
        text = re.sub(r'CÃ¢u há»i cáº§n tráº£ lá»i thÃªm:.*$', '', text, flags=re.MULTILINE | re.IGNORECASE)
        text = re.sub(r'CÃ¢u há»i tiáº¿p theo:.*$', '', text, flags=re.MULTILINE | re.IGNORECASE)
        text = re.sub(r'CÃ¢u há»i khÃ¡c:.*$', '', text, flags=re.MULTILINE | re.IGNORECASE)
        text = re.sub(r'Tháº¯c máº¯c khÃ¡c:.*$', '', text, flags=re.MULTILINE | re.IGNORECASE)
        
        # Loáº¡i bá» role indicators cÃ³ thá»ƒ rÃ² rá»‰
        text = re.sub(r'^\s*(user|assistant|system)\s*[:]\s*', '', text, flags=re.MULTILINE)
        text = re.sub(r'^\s*(NgÆ°á»i dÃ¹ng|Trá»£ lÃ½|Há»‡ thá»‘ng)\s*[:]\s*', '', text, flags=re.MULTILINE)
        
        # Loáº¡i bá» patterns láº·p kiá»ƒu A. B. C. XI. XI. XI.
        text = re.sub(r'([A-Z]\.)\s*THá»¦\s*Tá»¤C\s*NUÃ”I\s*CON\s*NUÃ”I\s*TRONG\s*NÆ¯á»šC\s*\n*', '', text, flags=re.IGNORECASE)
        
        # Loáº¡i bá» repetitive numbered/lettered lists
        text = re.sub(r'([A-Z]{1,3}\.)\s*(.+?)\n(\1\s*\2\n){2,}', r'\1 \2\n', text, flags=re.IGNORECASE)
        
        # Loáº¡i bá» cÃ¢u láº·p láº¡i liÃªn tiáº¿p (3+ láº§n)
        lines = text.split('\n')
        cleaned_lines = []
        prev_line = ""
        repeat_count = 0
        
        for line in lines:
            line = line.strip()
            if line == prev_line and line:
                repeat_count += 1
                if repeat_count < 2:  # Cho phÃ©p láº·p tá»‘i Ä‘a 1 láº§n
                    cleaned_lines.append(line)
            else:
                repeat_count = 0
                cleaned_lines.append(line)
                prev_line = line
        
        cleaned_text = '\n'.join(cleaned_lines).strip()
        
        # Giá»›i háº¡n Ä‘á»™ dÃ i response há»£p lÃ½
        if len(cleaned_text) > 2000:
            # Cáº¯t táº¡i cÃ¢u cuá»‘i cÃ¹ng trong 2000 kÃ½ tá»± Ä‘áº§u
            truncated = cleaned_text[:2000]
            last_sentence = truncated.rfind('.')
            if last_sentence > 1000:  # Chá»‰ cáº¯t náº¿u cÃ²n Ä‘á»§ ná»™i dung
                cleaned_text = truncated[:last_sentence + 1]
        
        return cleaned_text
    
    def is_loaded(self) -> bool:
        """Kiá»ƒm tra model Ä‘Ã£ Ä‘Æ°á»£c load chÆ°a"""
        return self.model is not None
    
    def get_model_info(self) -> Dict[str, Any]:
        """Láº¥y thÃ´ng tin vá» model"""
        return {
            'model_path': str(self.model_path),
            'model_url': self.model_url,
            'is_loaded': self.is_loaded(),
            'model_size_mb': self.model_path.stat().st_size / (1024**2) if self.model_path.exists() else 0,
            'model_kwargs': self.model_kwargs
        }
