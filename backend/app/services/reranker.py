import logging
import os
import time
import torch
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from sentence_transformers import CrossEncoder
import numpy as np
from ..core.config import settings

logger = logging.getLogger(__name__)

class RerankerService:
    """Service qu·∫£n l√Ω Vietnamese Reranker model - VRAM optimized v·ªõi on-demand loading"""
    
    def __init__(self, model_name: Optional[str] = None):
        self.model_name = model_name or settings.reranker_model_name
        self.model = None
        self.model_loaded = False
        
        # VRAM Optimization: Load model khi c·∫ßn thi·∫øt
        
    def get_optimal_device(self) -> str:
        """Get optimal device for reranker"""
        if torch.cuda.is_available():
            logger.info("üéÆ CUDA available - using GPU for reranker")
            return 'cuda'
        else:
            logger.info("üíª CUDA not available - falling back to CPU")
            return 'cpu'
        # self._load_model()  # Comment out ƒë·ªÉ load on-demand
    
    def _load_model(self):
        """Load model t·ª´ cache local ho·∫∑c download n·∫øu c·∫ßn - GPU for optimal performance"""
        if self.model_loaded:
            return
            
        try:
            logger.info(f"Loading reranker model: {self.model_name}")
            
            # Th·ª≠ load t·ª´ local cache tr∆∞·ªõc - s·ª≠ d·ª•ng GPU v·ªõi max_length=2304 theo documentation
            local_model_path = self._get_local_model_path()
            if local_model_path and local_model_path.exists():
                logger.info(f"Found local model at: {local_model_path}")
                try:
                    # üöÄ CORRECT CONFIG: max_length=2304 theo Vietnamese_Reranker documentation
                    # (256 for query + 2048 for passages = 2304 total)
                    # Model config c√≥ max_position_embeddings=8194 nh∆∞ng trained v·ªõi 2304
                    # Tokenizer c√≥ model_max_length=8192 nh∆∞ng optimal performance ·ªü 2304
                    
                    # S·ª≠ d·ª•ng model_kwargs ƒë·ªÉ optimize memory v√† performance
                    model_kwargs = {
                        'torch_dtype': 'auto'  # S·ª≠ d·ª•ng dtype t·ª´ model config (float32)
                    }
                    
                    self.model = CrossEncoder(
                        str(local_model_path), 
                        device=self.get_optimal_device(),  # ‚Üê Dynamic device selection
                        max_length=2304,
                        trust_remote_code=False,  # Security best practice  
                        model_kwargs=model_kwargs
                    )
                    self.model_loaded = True
                    logger.info(f"‚úÖ Reranker model loaded from local cache on {self.get_optimal_device().upper()} (max_length=2304, trained optimal)")
                    return
                except Exception as e:
                    logger.warning(f"Failed to load from local cache on CPU: {e}")
            
            # Fallback: load t·ª´ HuggingFace v·ªõi max_length=2304 theo documentation
            logger.info("Loading from HuggingFace (may download) on CPU with correct max_length=2304 (trained optimal)")
            # Model h·ªó tr·ª£ max 8192 tokens nh∆∞ng trained v·ªõi 2304 ƒë·ªÉ ƒë·∫£m b·∫£o quality
            
            # S·ª≠ d·ª•ng model_kwargs ƒë·ªÉ optimize memory v√† performance
            model_kwargs = {
                'torch_dtype': 'auto'  # S·ª≠ d·ª•ng dtype t·ª´ model config (float32)
            }
            
            self.model = CrossEncoder(
                self.model_name, 
                device=self.get_optimal_device(),  # ‚Üê Dynamic device selection
                max_length=2304,
                trust_remote_code=False,  # Security best practice
                model_kwargs=model_kwargs
            )
            self.model_loaded = True
            logger.info(f"‚úÖ Reranker model loaded from HuggingFace on {self.get_optimal_device().upper()} (max_length=2304, trained optimal)")
            
        except Exception as e:
            logger.error(f"Failed to load reranker model: {e}")
            self.model = None
            self.model_loaded = False
            raise
    
    def unload_model(self):
        """Unload reranker model ƒë·ªÉ gi·∫£i ph√≥ng VRAM"""
        if self.model is not None:
            logger.info("üîÑ Unloading Reranker model to free VRAM...")
            del self.model
            self.model = None
            self.model_loaded = False
            
            # Force garbage collection v√† CUDA cache clear
            import gc
            gc.collect()
            
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("‚úÖ CUDA cache cleared")
            except ImportError:
                pass
                
            logger.info("‚úÖ Reranker model unloaded, VRAM freed")
    
    def ensure_loaded(self):
        """Ensure reranker model is loaded"""
        if not self.model_loaded or self.model is None:
            self._load_model()
    
    def is_model_loaded(self) -> bool:
        """Check if reranker model is loaded"""
        return self.model_loaded and self.model is not None
    
    def _get_local_model_path(self):
        """T√¨m ƒë∆∞·ªùng d·∫´n local model n·∫øu c√≥"""
        try:
            # S·ª≠ d·ª•ng config path
            cache_hub_path = settings.hf_cache_path / "hub"
            model_dir = cache_hub_path / "models--AITeamVN--Vietnamese_Reranker"
            
            if model_dir.exists():
                # T√¨m snapshot directory
                snapshots_dir = model_dir / "snapshots"
                if snapshots_dir.exists():
                    # L·∫•y snapshot m·ªõi nh·∫•t
                    snapshot_dirs = [d for d in snapshots_dir.iterdir() if d.is_dir()]
                    if snapshot_dirs:
                        latest_snapshot = max(snapshot_dirs, key=lambda x: x.stat().st_mtime)
                        logger.info(f"Found local snapshot: {latest_snapshot}")
                        return latest_snapshot
            
            return None
        except Exception as e:
            logger.warning(f"Error finding local model path: {e}")
            return None
    
    def rerank_documents(
        self, 
        query: str, 
        documents: List[Dict[str, Any]], 
        top_k: Optional[int] = None,
        router_confidence: Optional[float] = None,
        router_confidence_level: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Rerank danh s√°ch documents d·ª±a tr√™n ƒë·ªô li√™n quan v·ªõi query
        
        Args:
            query: C√¢u h·ªèi c·∫ßn t√¨m
            documents: Danh s√°ch documents c·∫ßn rerank
            top_k: S·ªë l∆∞·ª£ng documents t·ªët nh·∫•t c·∫ßn tr·∫£ v·ªÅ (n·∫øu None th√¨ tr·∫£ v·ªÅ t·∫•t c·∫£)
            router_confidence: Confidence score t·ª´ router (0.0-1.0)
            router_confidence_level: Level t·ª´ router ('low', 'medium', 'high')
        
        Returns:
            Danh s√°ch documents ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp l·∫°i theo ƒë·ªô li√™n quan
        """
        # VRAM Optimization: Ensure model is loaded
        self.ensure_loaded()
        
        if not self.model:
            logger.warning("Reranker model not loaded, returning original order")
            return documents[:top_k] if top_k else documents
        
        if not documents:
            return []
        
        # üõ°Ô∏è ROUTER TRUST MODE: Khi router c√≥ HIGH confidence, tin t∆∞·ªüng router h∆°n
        trust_router = (router_confidence_level == 'high' and router_confidence and router_confidence >= 0.85)
        if trust_router:
            logger.info(f"üõ°Ô∏è ROUTER TRUST MODE: Router confidence {router_confidence:.3f} (HIGH) - Minimal rerank interference")
        
        try:
            # üîç DEBUG: Log s·ªë l∆∞·ª£ng documents v√† th·ªùi gian rerank
            rerank_start_time = time.time()
            logger.info(f"üîç RERANK QUERY: '{query}' ({len(query)} chars)")
            logger.info(f"üî¢ RERANK INPUT: {len(documents)} documents to process")
            
            # üöÄ PERFORMANCE OPTIMIZATION: Lo·∫°i b·ªè CPU preprocessing 
            # Chu·∫©n b·ªã pairs (query, document_content) tr·ª±c ti·∫øp cho reranker
            pairs = []
            for i, doc in enumerate(documents):
                content = doc['content']
                
                # üöÄ CORRECT PROCESSING: Ph√π h·ª£p v·ªõi max_length=2304 (256 query + 2048 passage)
                # CrossEncoder s·∫Ω t·ª± x·ª≠ l√Ω v·ªõi max_length=2304 ƒë√£ ƒë∆∞·ª£c set
                cleaned_content = content.replace("**", "").replace("*", "").replace("#", "")
                cleaned_content = " ".join(cleaned_content.split())  # Normalize whitespace
                
                # Truncate theo documentation: max ~2048 tokens cho passage (‚âà 6000 chars Vietnamese)
                if len(cleaned_content) > 6000:  # Soft limit tr∆∞·ªõc khi tokenization
                    cleaned_content = cleaned_content[:6000] + "..."
                
                logger.info(f"üîç RERANK DOC[{i}]: {len(cleaned_content)} chars")
                pairs.append((query, cleaned_content))
            
            # T√≠nh rerank scores
            logger.info(f"üî• RERANKING {len(documents)} documents with optimized settings...")
            scores = self.model.predict(pairs)
            rerank_time = time.time() - rerank_start_time
            logger.info(f"‚è±Ô∏è RERANK COMPLETED in {rerank_time:.2f}s ({len(documents)} docs)")
            
            # G√°n ƒëi·ªÉm rerank v√†o m·ªói document
            reranked_docs = []
            for doc, score in zip(documents, scores):
                doc_copy = doc.copy()
                doc_copy['rerank_score'] = float(score)
                reranked_docs.append(doc_copy)
            
            # S·∫Øp x·∫øp theo rerank score gi·∫£m d·∫ßn
            reranked_docs.sort(key=lambda x: x['rerank_score'], reverse=True)
            
            # Log th√¥ng tin v·ªÅ top document
            if reranked_docs:
                top_doc = reranked_docs[0]
                logger.info(f"Top document after reranking: score={top_doc['rerank_score']:.4f}, "
                          f"similarity={top_doc.get('similarity', 'N/A')}")
                
                # üõ°Ô∏è ROUTER TRUST MODE: Kh√¥ng trigger conservative strategy khi router HIGH confidence
                if trust_router:
                    logger.info(f"üõ°Ô∏è TRUSTING ROUTER: Accepting rerank results without conservative override (router: {router_confidence:.3f})")
                elif top_doc['rerank_score'] < 0.2:
                    logger.warning(f"‚ö†Ô∏è  LOW RERANK SCORE ({top_doc['rerank_score']:.4f}) - Conservative strategy may be triggered")
            
            # Tr·∫£ v·ªÅ top_k documents n·∫øu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
            if top_k:
                return reranked_docs[:top_k]
            
            return reranked_docs
            
        except Exception as e:
            logger.error(f"Error during reranking: {e}")
            # Fallback v·ªÅ s·∫Øp x·∫øp theo similarity score ban ƒë·∫ßu
            return sorted(documents, key=lambda x: x.get('similarity', 0), reverse=True)[:top_k] if top_k else documents
    
    # üóëÔ∏è REMOVED: CPU intensive preprocessing functions
    # _extract_query_keywords() v√† _extract_relevant_content() ƒë√£ ƒë∆∞·ª£c lo·∫°i b·ªè
    # ƒë·ªÉ t·ªëi ∆∞u h√≥a performance v√† ƒë·ªÉ GPU CrossEncoder t·ª± x·ª≠ l√Ω
    
    def get_consensus_document(
        self, 
        query: str, 
        documents: List[Dict[str, Any]], 
        top_k: int = 20,
        consensus_threshold: float = 0.3,
        min_rerank_score: float = 0.03,
        router_confidence: Optional[float] = None,
        router_confidence_level: Optional[str] = None,
        router_selected_document: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """
        T√¨m document c√≥ consensus cao nh·∫•t d·ª±a tr√™n nhi·ªÅu chunks ƒë∆∞·ª£c rerank
        
        Args:
            query: C√¢u h·ªèi t·ª´ user
            documents: Danh s√°ch documents ƒë·ªÉ rerank
            top_k: S·ªë l∆∞·ª£ng chunks t·ªët nh·∫•t ƒë·ªÉ xem x√©t
            consensus_threshold: Ng∆∞·ª°ng consensus t·ªëi thi·ªÉu (0.0-1.0)
            min_rerank_score: ƒêi·ªÉm rerank t·ªëi thi·ªÉu ƒë·ªÉ xem x√©t chunk
            router_confidence: Confidence score t·ª´ router (0.0-1.0)
            router_confidence_level: M·ª©c ƒë·ªô confidence t·ª´ router ('low', 'medium', 'high')
            router_selected_document: Document ID m√† router ƒë√£ ch·ªçn (VD: 'DOC_011')
        
        Returns:
            Document chunk t·ªët nh·∫•t ho·∫∑c None
        """
        if not documents:
            return None
        
        # ‚ö° ROUTER TRUST MODE: N·∫øu router c√≥ confidence cao, tin t∆∞·ªüng router decision
        if router_confidence and router_confidence > 0.85:
            logger.info(f"üéØ ROUTER TRUST MODE: High confidence {router_confidence:.3f} > 0.85 - Using router decision")
            
            # üîç FIXED: T√¨m chunk t·ª´ document m√† router ƒë√£ ch·ªçn
            if router_selected_document:
                router_chunk = self._find_chunk_from_document(documents, router_selected_document)
                if router_chunk:
                    logger.info(f"‚úÖ Found chunk from router-selected document: {router_selected_document}")
                    return router_chunk
                else:
                    logger.warning(f"‚ö†Ô∏è No chunk found from router-selected document {router_selected_document}, falling back to first document")
            
            # Fallback: Tr·∫£ v·ªÅ document ƒë·∫ßu ti√™n
            return documents[0] if documents else None
            
        # B∆∞·ªõc 1: Rerank t·∫•t c·∫£ documents v√† l·∫•y top-k
        reranked = self.rerank_documents(query, documents, top_k=top_k)

    def _analyze_document_consensus(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Ph√¢n t√≠ch xem chunks thu·ªôc documents n√†o v√† v·ªõi t·∫ßn su·∫•t ra sao
        
        Returns:
            Dict mapping document_id -> {chunks: [...], scores: [...], best_chunk: chunk}
        """
        document_groups = {}
        
        for chunk in chunks:
            # T√¨m document identifier t·ª´ chunk metadata
            doc_id = self._extract_document_id(chunk)
            
            if doc_id not in document_groups:
                document_groups[doc_id] = {
                    'chunks': [],
                    'scores': [],
                    'best_chunk': None,
                    'best_score': -1
                }
            
            # Th√™m chunk v√†o group
            rerank_score = chunk.get('rerank_score', 0)
            document_groups[doc_id]['chunks'].append(chunk)
            document_groups[doc_id]['scores'].append(rerank_score)
            
            # C·∫≠p nh·∫≠t best chunk cho document n√†y
            if rerank_score > document_groups[doc_id]['best_score']:
                document_groups[doc_id]['best_score'] = rerank_score
                document_groups[doc_id]['best_chunk'] = chunk
                
        return document_groups

    def _extract_document_id(self, chunk: Dict[str, Any]) -> str:
        """
        Tr√≠ch xu·∫•t document identifier t·ª´ chunk metadata
        Th·ª≠ nhi·ªÅu c√°ch ƒë·ªÉ t√¨m unique document ID
        """
        # Th·ª≠ c√°ch 1: source.file_path
        if 'source' in chunk and isinstance(chunk['source'], dict):
            if 'file_path' in chunk['source']:
                return chunk['source']['file_path']
            if 'document_title' in chunk['source']:
                return chunk['source']['document_title']
        
        # Th·ª≠ c√°ch 2: metadata.source
        if 'metadata' in chunk and isinstance(chunk['metadata'], dict):
            if 'source' in chunk['metadata']:
                source = chunk['metadata']['source']
                if isinstance(source, dict):
                    if 'file_path' in source:
                        return source['file_path']
                    if 'document_title' in source:
                        return source['document_title']
                elif isinstance(source, str):
                    return source
        
        # Th·ª≠ c√°ch 3: document_title tr·ª±c ti·∫øp
        if 'document_title' in chunk:
            return chunk['document_title']
            
        # Fallback: d√πng id ho·∫∑c content hash
        if 'id' in chunk:
            return f"doc_from_chunk_{chunk['id']}"
            
        # Last resort: hash content
        content = chunk.get('content', '')
        import hashlib
        return f"doc_hash_{hashlib.md5(content.encode()).hexdigest()[:8]}"

    def _find_best_consensus(
        self, 
        document_analysis: Dict[str, Any], 
        consensus_threshold: float,
        total_chunks: int
    ) -> Optional[Dict[str, Any]]:
        """
        T√¨m document c√≥ consensus ratio cao nh·∫•t v√† ƒë·∫°t threshold
        
        Returns:
            Best consensus info ho·∫∑c None n·∫øu kh√¥ng c√≥ document n√†o ƒë·∫°t threshold
        """
        best_consensus = None
        
        for doc_id, info in document_analysis.items():
            chunk_count = len(info['chunks'])
            consensus_ratio = chunk_count / total_chunks
            
            logger.info(f"üìä Document '{doc_id}': {chunk_count}/{total_chunks} chunks "
                       f"(ratio: {consensus_ratio:.2f}, threshold: {consensus_threshold})")
            
            # Ki·ªÉm tra xem c√≥ ƒë·∫°t threshold kh√¥ng
            if consensus_ratio >= consensus_threshold:
                if (best_consensus is None or 
                    consensus_ratio > best_consensus['consensus_ratio'] or
                    (consensus_ratio == best_consensus['consensus_ratio'] and 
                     info['best_score'] > best_consensus['best_score'])):
                    
                    best_consensus = {
                        'document_id': doc_id,
                        'chunk_count': chunk_count,
                        'consensus_ratio': consensus_ratio,
                        'best_chunk': info['best_chunk'],
                        'best_score': info['best_score'],
                        'all_chunks': info['chunks']
                    }
        
        return best_consensus
    
    # ‚ùå DEPRECATED METHOD REMOVED
    # def get_best_document(self, query: str, documents: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    #     """
    #     DEPRECATED: Use get_consensus_document() instead for better accuracy
    #     """
    #     pass
        return reranked[0] if reranked else None
    
    def _find_chunk_from_document(self, documents: List[Dict[str, Any]], target_document_id: str) -> Optional[Dict[str, Any]]:
        """
        T√¨m chunk t·ª´ document c·ª• th·ªÉ m√† router ƒë√£ ch·ªçn
        
        Args:
            documents: List c√°c chunks t·ª´ vector search
            target_document_id: Document ID m√† router ch·ªçn (VD: 'DOC_011')
        
        Returns:
            Chunk t·ª´ target document ho·∫∑c None
        """
        try:
            for doc in documents:
                # Check multiple possible locations for document ID
                doc_id = None
                
                # Method 1: Check metadata.source field
                if 'metadata' in doc and 'source' in doc['metadata']:
                    source = doc['metadata']['source']
                    if isinstance(source, dict) and 'file_path' in source:
                        file_path = source['file_path']
                        # Extract DOC_XXX from file path
                        import re
                        match = re.search(r'DOC_(\d+)', file_path)
                        if match:
                            doc_id = f"DOC_{match.group(1)}"
                
                # Method 2: Check direct metadata fields
                if not doc_id and 'metadata' in doc:
                    metadata = doc['metadata']
                    for key in ['document_id', 'doc_id', 'source_document']:
                        if key in metadata:
                            potential_id = metadata[key]
                            if target_document_id in str(potential_id):
                                doc_id = target_document_id
                                break
                
                # Method 3: Check source field directly
                if not doc_id and 'source' in doc:
                    source_str = str(doc['source'])
                    if target_document_id in source_str:
                        doc_id = target_document_id
                
                # If found matching document, return it
                if doc_id == target_document_id:
                    logger.info(f"‚úÖ Found chunk from document {target_document_id}: {doc.get('id', 'Unknown ID')}")
                    return doc
            
            logger.warning(f"‚ùå No chunk found from document {target_document_id} in {len(documents)} available chunks")
            return None
            
        except Exception as e:
            logger.error(f"Error finding chunk from document {target_document_id}: {e}")
            return None

    def is_loaded(self) -> bool:
        """Ki·ªÉm tra xem model ƒë√£ ƒë∆∞·ª£c load ch∆∞a"""
        return self.model is not None
    
    def get_model_info(self) -> Dict[str, Any]:
        """L·∫•y th√¥ng tin v·ªÅ model"""
        return {
            'model_name': self.model_name,
            'is_loaded': self.is_loaded(),
            'model_type': 'CrossEncoder' if self.model else None
        }
