"""
Optimized Enhanced RAG Service v·ªõi VRAM-optimized architecture
T√≠ch h·ª£p:
1. Ambiguous Query Detection & Processing
2. Enhanced Context Expansion v·ªõi Nucleus Strategy  
3. VRAM-optimized model placement (CPU embedding, GPU LLM/Reranker)
4. Session management
"""

import logging
import time
import uuid
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from pathlib import Path

from .vector_database import VectorDBService
from .language_model import LLMService
from .result_reranker import RerankerService
from .smart_clarification import SmartClarificationService
from .smart_router import EnhancedSmartQueryRouter, RouterBasedAmbiguousQueryService
from .smart_clarification import SmartClarificationService
from .context_expander import EnhancedContextExpansionService
from ..core.config import settings

logger = logging.getLogger(__name__)

def convert_numpy_types(obj: Any) -> Any:
    """Convert numpy types to Python native types for JSON serialization"""
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(v) for v in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(v) for v in obj)
    else:
        return obj

@dataclass
class OptimizedChatSession:
    """Session chat v·ªõi th√¥ng tin t·ªëi ∆∞u v·ªõi Stateful Router support"""
    session_id: str
    created_at: float
    last_accessed: float
    query_history: List[Dict[str, Any]] = field(default_factory=list)
    context_cache: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # Stateful Router State
    last_successful_collection: Optional[str] = None
    last_successful_confidence: float = 0.0
    last_successful_timestamp: Optional[float] = None
    cached_rag_content: Optional[Dict[str, Any]] = None
    consecutive_low_confidence_count: int = 0
    
    def update_successful_routing(self, collection: str, confidence: float, rag_content: Optional[Dict[str, Any]] = None):
        """C·∫≠p nh·∫≠t state khi routing th√†nh c√¥ng v·ªõi confidence cao"""
        self.last_successful_collection = collection
        self.last_successful_confidence = confidence
        self.last_successful_timestamp = time.time()
        if rag_content:
            self.cached_rag_content = rag_content
        self.consecutive_low_confidence_count = 0  # Reset counter
        
    def should_override_confidence(self, current_confidence: float, confidence_threshold: float = 0.50) -> bool:
        """Ki·ªÉm tra c√≥ n√™n override confidence th·∫•p kh√¥ng"""
        if not self.last_successful_collection:
            return False
            
        # Check time window - ch·ªâ override trong v√≤ng 10 ph√∫t
        if self.last_successful_timestamp and (time.time() - self.last_successful_timestamp > 600):
            return False
            
        # Override n·∫øu confidence hi·ªán t·∫°i th·∫•p nh∆∞ng c√≥ successful context
        return current_confidence < confidence_threshold and self.last_successful_confidence > 0.85
        
    def increment_low_confidence(self):
        """TƒÉng counter khi g·∫∑p confidence th·∫•p"""
        self.consecutive_low_confidence_count += 1
        
    def clear_routing_state(self):
        """Clear state khi user chuy·ªÉn ch·ªß ƒë·ªÅ ho√†n to√†n"""
        self.last_successful_collection = None
        self.last_successful_confidence = 0.0
        self.last_successful_timestamp = None
        self.cached_rag_content = None
        self.consecutive_low_confidence_count = 0

class OptimizedEnhancedRAGService:
    """
    Enhanced RAG Service ƒë∆∞·ª£c t·ªëi ∆∞u VRAM v√† performance
    
    Ki·∫øn tr√∫c:
    - Embedding Model: CPU (ti·∫øt ki·ªám VRAM cho query ng·∫Øn)
    - LLM: GPU (c·∫ßn song song h√≥a cho context d√†i)  
    - Reranker: GPU (c·∫ßn song song h√≥a cho multiple comparisons)
    """
    
    def __init__(
        self,
        documents_dir: str,
        vectordb_service: VectorDBService,
        llm_service: LLMService
    ):
        self.documents_dir = documents_dir
        self.vectordb_service = vectordb_service
        self.llm_service = llm_service
        
        # Initialize supporting services
        self._initialize_services()
        
        # Chat sessions management
        self.chat_sessions: Dict[str, OptimizedChatSession] = {}
        
        # Performance metrics
        self.metrics = {
            "total_queries": 0,
            "ambiguous_detected": 0,
            "context_expansions": 0,
            "avg_response_time": 0.0
        }
        
        logger.info("‚úÖ Optimized Enhanced RAG Service initialized")
        
    def _initialize_services(self):
        """Kh·ªüi t·∫°o c√°c service h·ªó tr·ª£ v·ªõi Enhanced Smart Router"""
        try:
            # Enhanced Smart Query Router v·ªõi Example Questions Database
            embedding_model = self.vectordb_service.embedding_model
            if embedding_model is None:
                raise ValueError("VectorDB embedding model not initialized")
            self.smart_router = EnhancedSmartQueryRouter(embedding_model=embedding_model)
            logger.info("‚úÖ Enhanced Smart Query Router initialized")
            
            # Reranker Service (GPU)
            self.reranker_service = RerankerService()
            logger.info("‚úÖ Reranker Service initialized (GPU)")
            
            # Router-based Ambiguous Query Service (CPU)
            self.ambiguous_service = RouterBasedAmbiguousQueryService(
                router=self.smart_router
            )
            logger.info("‚úÖ Router-based Ambiguous Query Service initialized (CPU)")
            
            # Smart Clarification Service
            self.clarification_service = SmartClarificationService()
            logger.info("‚úÖ Smart Clarification Service initialized")
            
            # Enhanced Context Expansion Service  
            self.context_expansion_service = EnhancedContextExpansionService(
                vectordb_service=self.vectordb_service,
                documents_dir=self.documents_dir
            )
            logger.info("‚úÖ Enhanced Context Expansion Service initialized")
            
        except Exception as e:
            logger.error(f"Error initializing services: {e}")
            raise
            
    def create_session(self, metadata: Optional[Dict[str, Any]] = None) -> str:
        """T·∫°o session chat m·ªõi"""
        session_id = str(uuid.uuid4())
        
        session = OptimizedChatSession(
            session_id=session_id,
            created_at=time.time(),
            last_accessed=time.time(),
            metadata=metadata or {}
        )
        
        self.chat_sessions[session_id] = session
        logger.info(f"Created new chat session: {session_id}")
        
        return session_id
        
    def get_session(self, session_id: str) -> Optional[OptimizedChatSession]:
        """L·∫•y session theo ID"""
        session = self.chat_sessions.get(session_id)
        if session:
            session.last_accessed = time.time()
        return session
        
    def enhanced_query(
        self,
        query: str,
        session_id: Optional[str] = None,
        max_context_length: int = 8000,  # INCREASED: TƒÉng t·ª´ 3000 l√™n 8000 ƒë·ªÉ ƒë·∫£m b·∫£o full document context
        use_ambiguous_detection: bool = True,
        use_full_document_expansion: bool = True
    ) -> Dict[str, Any]:
        """
        Query ch√≠nh v·ªõi t·∫•t c·∫£ t·ªëi ∆∞u h√≥a - THI·∫æT K·∫æ G·ªêC: FULL DOCUMENT EXPANSION
        
        Flow:
        1. Detect ambiguous query (CPU embedding)
        2. Route query n·∫øu clear
        3. Broad search (CPU embedding) 
        4. Rerank (GPU reranker)
        5. Context expansion: TO√ÄN B·ªò DOCUMENT (ƒë·∫£m b·∫£o ng·ªØ c·∫£nh ph√°p lu·∫≠t ƒë·∫ßy ƒë·ªß)
        6. Generate answer (GPU LLM)
        
        TRI·∫æT L√ù: VƒÉn b·∫£n ph√°p lu·∫≠t ph·∫£i ƒë∆∞·ª£c hi·ªÉu trong TO√ÄN B·ªò ng·ªØ c·∫£nh c·ªßa document g·ªëc
        """
        start_time = time.time()
        self.metrics["total_queries"] += 1
        
        try:
            # Get or create session
            if session_id:
                session = self.get_session(session_id)
                if not session:
                    return {"error": f"Session {session_id} not found"}
            else:
                session_id = self.create_session()
                session = self.get_session(session_id)
                
            logger.info(f"Processing query in session {session_id}: {query[:50]}...")
            
            # Step 1: Enhanced Smart Query Routing v·ªõi MULTI-LEVEL Confidence Processing + Stateful Router
            if use_ambiguous_detection:
                routing_result = self.smart_router.route_query(query, session)
                confidence_level = routing_result.get('confidence_level', 'low')
                was_overridden = routing_result.get('was_overridden', False)
                
                logger.info(f"Router confidence: {confidence_level} (score: {routing_result['confidence']:.3f})")
                if was_overridden:
                    logger.info(f"üî• Session-based confidence override applied!")
                
                if confidence_level in ['high', 'override_high']:
                    # HIGH CONFIDENCE (including overridden) - Route tr·ª±c ti·∫øp
                    target_collection = routing_result['target_collection']
                    inferred_filters = routing_result.get('inferred_filters', {})
                    best_collections = [target_collection] if target_collection else [settings.chroma_collection_name]
                    logger.info(f"‚úÖ HIGH CONFIDENCE routing to: {target_collection}")
                    
                elif confidence_level in ['low-medium', 'override_medium']:
                    # MEDIUM CONFIDENCE (including overridden) - Route v·ªõi caution
                    target_collection = routing_result['target_collection']
                    inferred_filters = routing_result.get('inferred_filters', {})
                    best_collections = [target_collection] if target_collection else [settings.chroma_collection_name]
                    logger.info(f"‚ö†Ô∏è MEDIUM CONFIDENCE routing to: {target_collection}")
                    
                else:
                    # T·∫§T C·∫¢ CONFIDENCE < THRESHOLD - H·ªèi l·∫°i user, kh√¥ng route
                    logger.info(f"ü§î CONFIDENCE KH√îNG ƒê·ª¶ CAO ({confidence_level}) - h·ªèi l·∫°i user thay v√¨ route")
                    return self._generate_smart_clarification(routing_result, query, session_id, start_time)
            
            else:
                # Fallback routing logic (gi·ªØ nguy√™n logic c≈©)
                routing_result = self.smart_router.route_query(query)
                confidence_level = 'fallback'  # Set default confidence level for fallback
                if routing_result.get('status') == 'routed' and routing_result.get('target_collection'):
                    target_collection = routing_result['target_collection']
                    inferred_filters = routing_result.get('inferred_filters', {})
                    best_collections = [target_collection]
                    logger.info(f"Fallback routed to collection: {target_collection}")
                else:
                    best_collections = [settings.chroma_collection_name]
                    inferred_filters = {}
                    confidence_level = 'fallback'  # Ensure confidence_level is set
            
            # Step 2: Focused Search v·ªõi ƒê·ªòNG BROAD_SEARCH_K d·ª±a tr√™n router confidence
            # üöÄ PERFORMANCE OPTIMIZATION: Gi·∫£m s·ªë documents c·∫ßn rerank
            dynamic_k = settings.broad_search_k  # default 12
            if confidence_level == 'high':
                dynamic_k = max(8, settings.broad_search_k - 4)  # Router t·ª± tin ‚Üí √≠t docs h∆°n
                logger.info(f"üéØ HIGH CONFIDENCE: Gi·∫£m broad_search_k xu·ªëng {dynamic_k}")
            elif confidence_level in ['low-medium', 'override_medium']:
                dynamic_k = min(15, settings.broad_search_k + 3)  # Router kh√¥ng ch·∫Øc ‚Üí nhi·ªÅu docs h∆°n
                logger.info(f"üîç MEDIUM CONFIDENCE: TƒÉng broad_search_k l√™n {dynamic_k}")
            else:
                logger.info(f"üìä DEFAULT/FALLBACK: S·ª≠ d·ª•ng broad_search_k={dynamic_k}")
            
            broad_search_results = []
            for collection_name in best_collections[:2]:  # Limit to top 2 collections
                try:
                    # ‚úÖ CRITICAL FIX: Pass smart filters to vector search v·ªõi dynamic K
                    results = self.vectordb_service.search_in_collection(
                        collection_name=collection_name,
                        query=query,
                        top_k=dynamic_k,
                        similarity_threshold=settings.similarity_threshold,
                        where_filter=inferred_filters if inferred_filters else None
                    )
                    
                    for result in results:
                        result["collection"] = collection_name
                        
                    broad_search_results.extend(results)
                    
                except Exception as e:
                    logger.warning(f"Error searching in collection {collection_name}: {e}")
            
            logger.info(f"üìä Dynamic search: {len(broad_search_results)} docs (k={dynamic_k}, confidence={confidence_level})")
            
            if not broad_search_results:
                return {
                    "type": "no_results",
                    "message": "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n.",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            logger.info(f"Found {len(broad_search_results)} candidate chunks")
            
            # Step 4: SEQUENTIAL PROCESSING ƒë·ªÉ t·ªëi ∆∞u VRAM (6GB limit)
            # Phase 1: Reranking - Load Reranker, Unload LLM n·∫øu c·∫ßn
            logger.info("üîÑ PHASE 1: Reranking (GPU) - Optimizing VRAM usage...")
            
            # Temporarily unload LLM ƒë·ªÉ ƒë·∫£m b·∫£o VRAM cho reranker
            if hasattr(self.llm_service, 'unload_model'):
                self.llm_service.unload_model()
            
            if settings.use_reranker and len(broad_search_results) > 1:
                # ‚úÖ FIX CRITICAL BUG: Rerank T·∫§T C·∫¢ documents thay v√¨ ch·ªâ top 10
                # ƒê√¢y l√† l·ªói logic nghi√™m tr·ªçng - kh√¥ng ƒë∆∞·ª£c v·ª©t b·ªè documents ti·ªÅm nƒÉng!
                docs_to_rerank = broad_search_results  # RERANK ALL DOCUMENTS
                logger.info(f"Reranking ALL {len(broad_search_results)} candidate documents (FIXED BUG)")
                
                nucleus_chunks = self.reranker_service.rerank_documents(
                    query=query,
                    documents=docs_to_rerank,
                    top_k=1,  # CH·ªà 1 nucleus chunk cao nh·∫•t - s·∫Ω expand to√†n b·ªô document ch·ª©a chunk n√†y
                    router_confidence=routing_result.get('confidence', 0.0),
                    router_confidence_level=routing_result.get('confidence_level', 'low')
                )
                
                # Unload reranker sau khi ho√†n th√†nh ƒë·ªÉ gi·∫£i ph√≥ng VRAM
                if hasattr(self.reranker_service, 'unload_model'):
                    self.reranker_service.unload_model()
                
                # üö® INTELLIGENT CONFIDENCE CHECK - Ki·ªÉm tra COMBINED confidence tr∆∞·ªõc khi g·ªçi LLM
                router_confidence = routing_result.get('confidence', 0.0)
                best_score = nucleus_chunks[0].get('rerank_score', 0) if nucleus_chunks and len(nucleus_chunks) > 0 else 0.0
                
                # Calculate combined confidence score
                combined_confidence = (router_confidence * 0.4 + best_score * 0.6)  # Reranker c√≥ tr·ªçng s·ªë cao h∆°n
                logger.info(f"üéØ Combined Confidence: {combined_confidence:.4f} (Router: {router_confidence:.4f}, Rerank: {best_score:.4f})")
                
                # SMART CLARIFICATION THRESHOLD - Tr√°nh c√¢u tr·∫£ l·ªùi sai l·ªách
                CLARIFICATION_THRESHOLD = 0.3  # ƒêi·ªÅu ch·ªânh threshold n√†y theo c·∫ßn thi·∫øt
                
                if combined_confidence < CLARIFICATION_THRESHOLD:
                    logger.warning(f"üö® COMBINED CONFIDENCE QU√Å TH·∫§P ({combined_confidence:.4f} < {CLARIFICATION_THRESHOLD}) - K√≠ch ho·∫°t Smart Clarification")
                    
                    return self._generate_smart_clarification(routing_result, query, session_id, start_time)
                
                if nucleus_chunks and len(nucleus_chunks) > 0:
                    logger.info(f"Best rerank score: {best_score:.4f}")
                    logger.info("üéØ PURE RERANKER MODE - No protective logic, full expansion strategy")
            
                logger.info(f"Selected {len(nucleus_chunks)} nucleus chunk with rerank-based strategy")
            else:
                nucleus_chunks = broad_search_results[:1]  # Fallback: l·∫•y chunk t·ªët nh·∫•t theo vector similarity
                
            # Step 5: INTELLIGENT Context Expansion - ∆Øu ti√™n nucleus chunk + context li√™n quan
            expanded_context = None
            logger.info("üéØ INTELLIGENT CONTEXT EXPANSION - ∆Øu ti√™n nucleus chunk t·ª´ reranker")
            self.metrics["context_expansions"] += 1
            
            # üß† SMART OPTIMIZATION: ∆Øu ti√™n nucleus chunk + context li√™n quan thay v√¨ c·∫Øt ng·∫´u nhi√™n
            # Logic: Lu√¥n gi·ªØ nguy√™n nucleus chunk + th√™m context xung quanh n·∫øu c√≤n ch·ªó
            # Step 5: Context Expansion - THI·∫æT K·∫æ G·ªêC: FULL DOCUMENT
            logger.info("Context expansion: Loading TO√ÄN B·ªò DOCUMENT ƒë·ªÉ ƒë·∫£m b·∫£o ng·ªØ c·∫£nh ph√°p lu·∫≠t ƒë·∫ßy ƒë·ªß")
            
            expanded_context = self.context_expansion_service.expand_context_with_nucleus(
                nucleus_chunks=nucleus_chunks,
                max_context_length=max_context_length,
                include_full_document=use_full_document_expansion
            )
            
            context_text = self._build_context_from_expanded(expanded_context)
            logger.info(f"Context expanded: {expanded_context['total_length']} chars from {len(expanded_context.get('source_documents', []))} documents")
            
            # Phase 2: LLM Generation - Load LLM cho generation phase
            logger.info("üîÑ PHASE 2: LLM Generation (GPU) - Loading LLM for final answer...")
            
            # Step 6: Generate Answer (GPU LLM)
            if not session:
                return {
                    "type": "error",
                    "error": "Session not found",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            answer = self._generate_answer_with_context(
                query=query,
                context=context_text,
                session=session
            )
            
            # Update session history
            session.query_history.append({
                "query": query,
                "answer": answer,
                "timestamp": time.time(),
                "nucleus_chunks_count": len(nucleus_chunks),
                "context_length": len(context_text)
            })
            
            # Keep only last 5 queries in session (gi·∫£m t·ª´ 10 ƒë·ªÉ ti·∫øt ki·ªám memory)
            if len(session.query_history) > 5:
                session.query_history = session.query_history[-5:]
            
            # üî• Update session state for Stateful Router
            # Ch·ªâ update state khi routing th√†nh c√¥ng v·ªõi confidence cao
            if routing_result and routing_result.get('confidence', 0) >= 0.85:
                target_collection = routing_result.get('target_collection')
                if target_collection:
                    rag_content = {
                        "context_text": context_text,
                        "nucleus_chunks": nucleus_chunks,
                        "expanded_context": expanded_context,
                        "collections": best_collections
                    }
                    session.update_successful_routing(
                        collection=target_collection, 
                        confidence=routing_result.get('confidence', 0),
                        rag_content=rag_content
                    )
                    logger.info(f"üî• Updated session state: {target_collection} (confidence: {routing_result.get('confidence', 0):.3f})")
                
            processing_time = time.time() - start_time
            self.metrics["avg_response_time"] = (
                (self.metrics["avg_response_time"] * (self.metrics["total_queries"] - 1) + processing_time) 
                / self.metrics["total_queries"]
            )
            
            return {
                "type": "answer",
                "answer": answer,
                "context_info": {
                    "nucleus_chunks": len(nucleus_chunks),
                    "context_length": len(context_text),
                    "source_collections": list(set(chunk.get("collection", "") for chunk in nucleus_chunks)),
                    "source_documents": list(expanded_context.get("source_documents", [])) if expanded_context else []
                },
                "context_details": {
                    "total_length": expanded_context.get("total_length", len(context_text)) if expanded_context else len(context_text),
                    "expansion_strategy": expanded_context.get("expansion_strategy", "unknown") if expanded_context else "no_expansion",
                    "source_documents": expanded_context.get("source_documents", []) if expanded_context else [],
                    "nucleus_chunks_count": len(nucleus_chunks)
                },
                "session_id": session_id,
                "processing_time": processing_time,
                "routing_info": {"best_collections": best_collections}
            }
            
        except Exception as e:
            logger.error(f"Error in enhanced query: {e}")
            return {
                "type": "error",
                "error": str(e),
                "session_id": session_id,
                "processing_time": time.time() - start_time
            }
            
    def handle_clarification(
        self,
        session_id: str,
        selected_option: str,
        original_query: str
    ) -> Dict[str, Any]:
        """X·ª≠ l√Ω ph·∫£n h·ªìi clarification"""
        session = self.get_session(session_id)
        if not session:
            return {"error": f"Session {session_id} not found"}
            
        # T·∫°o refined query
        refined_query = f"{original_query} - {selected_option}"
        
        # Process refined query (s·∫Ω kh√¥ng ambiguous n·ªØa)
        return self.enhanced_query(
            query=refined_query,
            session_id=session_id,
            use_ambiguous_detection=False  # Skip ambiguous detection for refined query
        )
        
    def _build_context_from_expanded(self, expanded_context: Dict[str, Any]) -> str:
        """Build context string t·ª´ expanded context"""
        context_parts = []
        
        for doc_content in expanded_context.get("expanded_content", []):
            source = doc_content.get("source", "N/A")
            text = doc_content.get("text", "")
            chunk_count = doc_content.get("chunk_count", 0)
            
            context_parts.append(f"=== T√†i li·ªáu: {source} ({chunk_count} ƒëo·∫°n) ===\n{text}")
            
        return "\n\n".join(context_parts)
        
    def _generate_answer_with_context(
        self,
        query: str,
        context: str,
        session: OptimizedChatSession
    ) -> str:
        """Generate answer v·ªõi context v√† session history"""
        
        # Build conversation context if needed
        conversation_context = ""
        if len(session.query_history) > 0:
            # üöÄ PERFORMANCE OPTIMIZATION: Ch·ªâ l·∫•y 1 l∆∞·ª£t h·ªèi-ƒë√°p g·∫ßn nh·∫•t ƒë·ªÉ gi·∫£m prompt length
            recent_queries = session.query_history[-1:]  # Only last 1 query thay v√¨ 3
            logger.info(f"‚ö° Chat history: {len(recent_queries)} entries (optimized for speed)")
            conversation_context = "L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y:\n" + "\n".join([
                f"Q: {item['query']}\nA: {item['answer'][:100]}..."  # Gi·∫£m t·ª´ 200 xu·ªëng 100 chars
                for item in recent_queries
            ]) + "\n\n"
            
        # ALWAYS use FULL system prompt - No conservative strategy
        system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam.

üö® QUY T·∫ÆC B·∫ÆT BU·ªòC - KH√îNG ƒê∆Ø·ª¢C VI PH·∫†M:
1. CH·ªà tr·∫£ l·ªùi d·ª±a CH√çNH X√ÅC tr√™n th√¥ng tin C√ì TRONG t√†i li·ªáu
2. N·∫øu h·ªèi v·ªÅ PH√ç/TI·ªÄN - t√¨m th√¥ng tin "üí∞ TH√îNG TIN PH√ç/L·ªÜ PH√ç" trong t√†i li·ªáu
3. N·∫øu c√≥ th√¥ng tin "Mi·ªÖn l·ªá ph√≠" - ph·∫£i ∆∞u ti√™n n√™u r√µ ƒëi·ªÅu n√†y
4. KH√îNG t·ª± s√°ng t·∫°o th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu
5. Tr·∫£ l·ªùi NG·∫ÆN G·ªåN (t·ªëi ƒëa 3-4 c√¢u)
6. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn - n√≥i "Theo th√¥ng tin trong t√†i li·ªáu..."

V√≠ d·ª• tr·∫£ l·ªùi t·ªët:
- "Theo th√¥ng tin trong t√†i li·ªáu, ƒëƒÉng k√Ω khai sinh ƒë√∫ng h·∫°n ƒë∆∞·ª£c MI·ªÑN L·ªÜ PH√ç."
- "T√†i li·ªáu n√™u r√µ l·ªá ph√≠ l√† X ƒë·ªìng cho tr∆∞·ªùng h·ª£p Y."

TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c t·ª± t·∫°o ra th√¥ng tin v·ªÅ ph√≠ ho·∫∑c c√°c quy ƒë·ªãnh kh√¥ng c√≥ trong t√†i li·ªáu."""
        
        logger.info(f"üìù ALWAYS using FULL system prompt (Conservative mode disabled), context length: {len(context)}")
        
        # Build enhanced context v·ªõi conversation history
        enhanced_context = conversation_context + context
        
        # üî• TOKEN MANAGEMENT - Ki·ªÉm so√°t ƒë·ªô d√†i ƒë·ªÉ tr√°nh context overflow
        from app.core.config import settings
        
        # ∆Ø·ªõc t√≠nh token ƒë∆°n gi·∫£n (1 token ‚âà 3-4 k√Ω t·ª± ti·∫øng Vi·ªát)
        estimated_tokens = len(system_prompt + enhanced_context + query + "Tr·∫£ l·ªùi: ") // 3
        max_context_tokens = settings.n_ctx - 500  # ƒê·ªÉ l·∫°i 500 token cho response
        
        if estimated_tokens > max_context_tokens:
            # C·∫Øt b·ªõt context ƒë·ªÉ fit trong gi·ªõi h·∫°n
            logger.warning(f"üö® Context overflow detected: {estimated_tokens} tokens > {max_context_tokens} max")
            
            # T√≠nh to√°n space c√≤n l·∫°i cho context
            fixed_parts_length = len(system_prompt + conversation_context + query + "Tr·∫£ l·ªùi: ")
            remaining_space = (max_context_tokens * 3) - fixed_parts_length
            
            if remaining_space > 500:  # ƒê·∫£m b·∫£o c√≥ √≠t nh·∫•t 500 k√Ω t·ª± cho context
                truncated_context = context[:remaining_space] + "\n\n[...TH√îNG TIN ƒê√É ƒê∆Ø·ª¢C R√öT G·ªåN ƒê·ªÇ TR√ÅNH QU√Å T·∫¢I...]"
                enhanced_context = conversation_context + truncated_context
                logger.info(f"‚úÇÔ∏è Context truncated from {len(context)} to {len(truncated_context)} chars")
            else:
                # N·∫øu kh√¥ng ƒë·ªß ch·ªó, b·ªè conversation history
                enhanced_context = context[:max_context_tokens * 3 // 2] + "\n\n[...R√öT G·ªåN...]"
                logger.warning("‚ö†Ô∏è Removed conversation history due to extreme context overflow")
        
        logger.info(f"üìù Final context length: {len(enhanced_context)} chars (~{len(enhanced_context)//3} tokens)")

        try:
            response_data = self.llm_service.generate_response(
                user_query=query,
                context=enhanced_context,
                max_tokens=settings.max_tokens,
                temperature=settings.temperature,
                system_prompt=system_prompt
            )
            
            # Extract response text from dict
            if isinstance(response_data, dict) and "response" in response_data:
                return response_data["response"].strip()
            elif isinstance(response_data, str):
                return response_data.strip()
            else:
                return str(response_data).strip()
            
        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            return f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t·∫°o c√¢u tr·∫£ l·ªùi: {e}"
            
    def get_health_status(self) -> Dict[str, Any]:
        """Tr·∫°ng th√°i health c·ªßa service"""
        try:
            collections = self.vectordb_service.list_collections()
            total_documents = 0
            
            for collection_info in collections:
                try:
                    collection = self.vectordb_service.get_collection(collection_info["name"])
                    count = collection.count()
                    total_documents += count
                except:
                    continue
                    
            return {
                "status": "healthy",
                "total_collections": len(collections),
                "total_documents": total_documents,
                "llm_loaded": self.llm_service.model is not None,
                "reranker_loaded": self.reranker_service.model is not None,
                "embedding_device": "CPU (VRAM optimized)",
                "llm_device": "GPU",
                "reranker_device": "GPU",
                "active_sessions": len(self.chat_sessions),
                "metrics": self.metrics,
                "router_stats": self.smart_router.get_collection_info(),
                "context_expansion": {
                    "total_chunks_cached": len(self.context_expansion_service.document_metadata_cache),
                    **self.context_expansion_service.get_stats()
                }
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
            
    def cleanup_old_sessions(self, max_age_hours: int = 24):
        """D·ªçn d·∫πp sessions c≈©"""
        current_time = time.time()
        cutoff_time = current_time - (max_age_hours * 3600)
        
        old_sessions = [
            session_id for session_id, session in self.chat_sessions.items()
            if session.last_accessed < cutoff_time
        ]
        
        for session_id in old_sessions:
            del self.chat_sessions[session_id]
            
        if old_sessions:
            logger.info(f"Cleaned up {len(old_sessions)} old sessions")
            
        return len(old_sessions)

    # API Compatibility Methods
    def query(self, question: Optional[str] = None, query: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """Compatibility method cho API routes"""
        # H·ªó tr·ª£ c·∫£ 'question' v√† 'query' parameters
        query_text = question or query
        if not query_text:
            raise ValueError("Either 'question' or 'query' parameter is required")
        return self.enhanced_query(query_text, **kwargs)
    
    @property
    def query_router(self):
        """Compatibility property cho API routes"""
        # T·∫°o wrapper v·ªõi explain_routing method
        class RouterWrapper:
            def __init__(self, smart_router):
                self.smart_router = smart_router
                # Copy t·∫•t c·∫£ methods t·ª´ smart_router
                for attr in dir(smart_router):
                    if not attr.startswith('_') and callable(getattr(smart_router, attr)):
                        setattr(self, attr, getattr(smart_router, attr))
            
            def explain_routing(self, question: str) -> Dict[str, Any]:
                """Explain routing decision cho question"""
                try:
                    # S·ª≠ d·ª•ng smart router ƒë·ªÉ classify
                    route_result = self.smart_router.route_query(question)
                    return {
                        'question': question,
                        'route': route_result.get('route_name', 'general'),
                        'confidence': route_result.get('confidence', 0.0),
                        'reasoning': route_result.get('reasoning', 'No reasoning available'),
                        'suggested_collections': route_result.get('suggested_collections', [])
                    }
                except Exception as e:
                    logger.error(f"Error explaining routing: {e}")
                    return {
                        'question': question,
                        'route': 'general',
                        'confidence': 0.0,
                        'reasoning': f'Error: {str(e)}',
                        'suggested_collections': []
                    }
        
        return RouterWrapper(self.smart_router)

    def build_index(self, collection_name: Optional[str] = None, force_rebuild: bool = False, **kwargs) -> Dict[str, Any]:
        """Build index cho collection ho·∫∑c t·∫•t c·∫£ collections"""
        try:
            if collection_name:
                # Build specific collection
                if self.vectordb_service.collection_exists(collection_name):
                    stats = self.vectordb_service.get_collection_stats(collection_name)
                    return {
                        'status': 'success',
                        'collections_processed': 1,
                        'collection_name': collection_name,
                        'message': f'Collection {collection_name} already exists and ready',
                        'document_count': stats.get('document_count', 0)
                    }
                else:
                    return {
                        'status': 'error',
                        'collections_processed': 0,
                        'collection_name': collection_name,
                        'error': f'Collection {collection_name} does not exist',
                        'suggestion': 'Run python tools/2_build_vectordb_final.py to build collections'
                    }
            else:
                # Build all collections - return info about existing ones
                collections = self.vectordb_service.list_collections()
                return {
                    'status': 'success',
                    'collections_processed': len(collections),
                    'message': f'Found {len(collections)} existing collections',
                    'collections': [col['name'] for col in collections],
                    'total_documents': sum(col.get('document_count', 0) for col in collections),
                    'suggestion': 'Use python tools/2_build_vectordb_final.py to build new collections from documents'
                }
        except Exception as e:
            logger.error(f"Error in build_index: {e}")
            return {
                'status': 'error',
                'collections_processed': 0,
                'error': str(e)
            }
    
    def _generate_smart_clarification(self, routing_result: Dict[str, Any], query: str, session_id: str, start_time: float) -> Dict[str, Any]:
        """T·∫°o clarification th√¥ng minh d·ª±a tr√™n confidence level"""
        try:
            # G·ªçi Smart Clarification Service ƒë·ªÉ t·∫°o clarification th√¥ng minh
            clarification_service = SmartClarificationService()
            clarification_response = clarification_service.generate_clarification(
                query=query,
                confidence=routing_result.get('confidence', 0.0),
                routing_result=routing_result
            )
            
            # Merge clarification response with required fields
            processing_time = time.time() - start_time
            
            # Get the main response from clarification service
            response = clarification_response.copy()
            
            # Add required fields that API expects
            response.update({
                "session_id": session_id,
                "processing_time": processing_time,
                "routing_info": {
                    "target_collection": routing_result.get('target_collection'),
                    "router_confidence": routing_result.get('confidence', 0.0),
                    "status": "smart_clarification"
                }
            })
            
            return convert_numpy_types(response)
            
        except Exception as e:
            logger.error(f"Error generating smart clarification: {e}")
            processing_time = time.time() - start_time
            
            fallback_response = {
                "type": "clarification_needed",
                "confidence": routing_result.get('confidence', 0.0),
                "clarification": {
                    "message": "Xin l·ªói, t√¥i kh√¥ng r√µ √Ω ƒë·ªãnh c·ªßa c√¢u h·ªèi. B·∫°n c√≥ th·ªÉ di·ªÖn ƒë·∫°t r√µ h∆°n kh√¥ng?",
                    "options": [
                        {
                            'id': 'retry',
                            'title': "H√£y di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi",
                            'description': "T√¥i s·∫Ω c·ªë g·∫Øng hi·ªÉu r√µ h∆°n",
                            'action': 'manual_input'
                        }
                    ],
                    "style": "fallback"
                },
                "session_id": session_id,
                "processing_time": processing_time,
                "routing_info": {
                    "target_collection": routing_result.get('target_collection'),
                    "router_confidence": routing_result.get('confidence', 0.0),
                    "status": "smart_clarification_error",
                    "error": str(e)
                }
            }
            return convert_numpy_types(fallback_response)
    
    def _activate_vector_backup_strategy(self, routing_result: Dict[str, Any], query: str, session_id: str, start_time: float) -> Dict[str, Any]:
        """K√≠ch ho·∫°t Vector Backup Strategy khi Smart Router ho√†n to√†n th·∫•t b·∫°i"""
        try:
            logger.info("üö® Activating Vector Backup Strategy - searching across all collections")
            
            # Th·ª±c hi·ªán vector search tr·ª±c ti·∫øp tr√™n t·∫•t c·∫£ collections ƒë·ªÉ t√¨m topics li√™n quan
            all_collections = self.vectordb_service.list_collections()
            backup_results = []
            
            for collection_info in all_collections[:3]:  # Limit to top 3 collections for performance
                collection_name = collection_info["name"]
                try:
                    collection = self.vectordb_service.get_collection(collection_name)
                    search_results = self.vectordb_service.search_in_collection(
                        collection_name,
                        query,
                        top_k=2,  # Ch·ªâ l·∫•y top 2 results per collection
                        similarity_threshold=0.3,
                        where_filter={}
                    )
                    
                    if search_results:
                        best_result = search_results[0]
                        backup_results.append({
                            'collection': collection_name,
                            'score': best_result.get('similarity', best_result.get('score', 0)),
                            'content': best_result.get('content', best_result.get('document', ''))[:200] + "...",
                            'metadata': best_result.get('metadata', {}),
                            'source': best_result.get('metadata', {}).get('source', 'N/A')
                        })
                        
                except Exception as e:
                    logger.warning(f"Error searching collection {collection_name}: {e}")
                    continue
            
            # Sort by score v√† t·∫°o suggestions
            backup_results.sort(key=lambda x: x['score'], reverse=True)
            
            options = []
            for i, result in enumerate(backup_results[:3], 1):
                # Tr√≠ch xu·∫•t title t·ª´ metadata n·∫øu c√≥
                metadata = result.get('metadata', {})
                title = metadata.get('document_title', metadata.get('title', f"Th·ªß t·ª•c {result['collection']}"))
                
                option = {
                    'id': str(i),
                    'title': title,
                    'description': f"ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng: {result['score']:.2f} - {result['content']}",
                    'collection': result['collection'],
                    'backup_score': result['score']
                }
                options.append(option)
            
            clarification_msg = "T√¥i kh√¥ng t√¨m th·∫•y c√¢u h·ªèi m·∫´u ph√π h·ª£p, nh∆∞ng d·ª±a tr√™n t√¨m ki·∫øm trong d·ªØ li·ªáu, c√¢u h·ªèi c·ªßa b·∫°n c√≥ th·ªÉ li√™n quan ƒë·∫øn:"
            
            if not options:
                clarification_msg = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p. B·∫°n c√≥ th·ªÉ th·ª≠ v·ªõi t·ª´ kh√≥a kh√°c kh√¥ng?"
            
            logger.info(f"Vector backup strategy found {len(options)} potential matches")
            
            return {
                "type": "clarification_needed",
                "status": "vector_backup",
                "confidence": routing_result.get('confidence', 0.0),
                "clarification": clarification_msg,
                "options": options,
                "backup_results": len(backup_results),
                "session_id": session_id,
                "processing_time": time.time() - start_time,
                "strategy": "vector_backup"
            }
            
        except Exception as e:
            logger.error(f"Error in vector backup strategy: {e}")
            return {
                "type": "clarification_needed",
                "status": "fallback_error", 
                "clarification": "Xin l·ªói, c√≥ l·ªói h·ªá th·ªëng khi x·ª≠ l√Ω c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i sau.",
                "session_id": session_id,
                "processing_time": time.time() - start_time
            }
    
    @property  
    def document_processor(self):
        """Compatibility property cho API routes"""
        import os
        
        class DocumentProcessorCompat:
            def get_available_collections(self, documents_dir):
                """L·∫•y danh s√°ch collections c√≥ th·ªÉ t·∫°o t·ª´ documents"""
                try:
                    if not os.path.exists(documents_dir):
                        return []
                    
                    collections = []
                    for item in os.listdir(documents_dir):
                        item_path = os.path.join(documents_dir, item)
                        if os.path.isdir(item_path):
                            # ƒê·∫øm s·ªë files PDF trong th∆∞ m·ª•c
                            pdf_count = len([f for f in os.listdir(item_path) 
                                           if f.lower().endswith('.pdf')])
                            if pdf_count > 0:
                                collections.append({
                                    'name': item,
                                    'path': item_path,
                                    'document_count': pdf_count
                                })
                    return collections
                except Exception as e:
                    logger.error(f"Error getting available collections: {e}")
                    return []
        
        return DocumentProcessorCompat()
