"""
Optimized Enhanced RAG Service v·ªõi VRAM-optimized architecture
T√≠ch h·ª£p:
1. Ambiguous Query Detection & Processing
2. Enhanced Context Expansion v·ªõi Nucleus Strategy  
3. VRAM-optimized model placement (CPU embedding, GPU LLM/Reranker)
4. Session management
"""

import logging
import time
import uuid
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from pathlib import Path

from .vector_database import VectorDBService
from .language_model import LLMService
from .result_reranker import RerankerService
from .smart_router import EnhancedSmartQueryRouter, RouterBasedAmbiguousQueryService
from .context_expander import EnhancedContextExpansionService
from ..core.config import settings

logger = logging.getLogger(__name__)

@dataclass
class OptimizedChatSession:
    """Session chat v·ªõi th√¥ng tin t·ªëi ∆∞u"""
    session_id: str
    created_at: float
    last_accessed: float
    query_history: List[Dict[str, Any]] = field(default_factory=list)
    context_cache: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

class OptimizedEnhancedRAGService:
    """
    Enhanced RAG Service ƒë∆∞·ª£c t·ªëi ∆∞u VRAM v√† performance
    
    Ki·∫øn tr√∫c:
    - Embedding Model: CPU (ti·∫øt ki·ªám VRAM cho query ng·∫Øn)
    - LLM: GPU (c·∫ßn song song h√≥a cho context d√†i)  
    - Reranker: GPU (c·∫ßn song song h√≥a cho multiple comparisons)
    """
    
    def __init__(
        self,
        documents_dir: str,
        vectordb_service: VectorDBService,
        llm_service: LLMService
    ):
        self.documents_dir = documents_dir
        self.vectordb_service = vectordb_service
        self.llm_service = llm_service
        
        # Initialize supporting services
        self._initialize_services()
        
        # Chat sessions management
        self.chat_sessions: Dict[str, OptimizedChatSession] = {}
        
        # Performance metrics
        self.metrics = {
            "total_queries": 0,
            "ambiguous_detected": 0,
            "context_expansions": 0,
            "avg_response_time": 0.0
        }
        
        logger.info("‚úÖ Optimized Enhanced RAG Service initialized")
        
    def _initialize_services(self):
        """Kh·ªüi t·∫°o c√°c service h·ªó tr·ª£ v·ªõi Enhanced Smart Router"""
        try:
            # Enhanced Smart Query Router v·ªõi Example Questions Database
            embedding_model = self.vectordb_service.embedding_model
            if embedding_model is None:
                raise ValueError("VectorDB embedding model not initialized")
            self.smart_router = EnhancedSmartQueryRouter(embedding_model=embedding_model)
            logger.info("‚úÖ Enhanced Smart Query Router initialized")
            
            # Reranker Service (GPU)
            self.reranker_service = RerankerService()
            logger.info("‚úÖ Reranker Service initialized (GPU)")
            
            # Router-based Ambiguous Query Service (CPU)
            self.ambiguous_service = RouterBasedAmbiguousQueryService(
                router=self.smart_router
            )
            logger.info("‚úÖ Router-based Ambiguous Query Service initialized (CPU)")
            
            # Enhanced Context Expansion Service  
            self.context_expansion_service = EnhancedContextExpansionService(
                vectordb_service=self.vectordb_service,
                documents_dir=self.documents_dir
            )
            logger.info("‚úÖ Enhanced Context Expansion Service initialized")
            
        except Exception as e:
            logger.error(f"Error initializing services: {e}")
            raise
            
    def create_session(self, metadata: Optional[Dict[str, Any]] = None) -> str:
        """T·∫°o session chat m·ªõi"""
        session_id = str(uuid.uuid4())
        
        session = OptimizedChatSession(
            session_id=session_id,
            created_at=time.time(),
            last_accessed=time.time(),
            metadata=metadata or {}
        )
        
        self.chat_sessions[session_id] = session
        logger.info(f"Created new chat session: {session_id}")
        
        return session_id
        
    def get_session(self, session_id: str) -> Optional[OptimizedChatSession]:
        """L·∫•y session theo ID"""
        session = self.chat_sessions.get(session_id)
        if session:
            session.last_accessed = time.time()
        return session
        
    def enhanced_query(
        self,
        query: str,
        session_id: Optional[str] = None,
        max_context_length: int = 3000,
        use_ambiguous_detection: bool = True,
        use_full_document_expansion: bool = True
    ) -> Dict[str, Any]:
        """
        Query ch√≠nh v·ªõi t·∫•t c·∫£ t·ªëi ∆∞u h√≥a
        
        Flow:
        1. Detect ambiguous query (CPU embedding)
        2. Route query n·∫øu clear
        3. Broad search (CPU embedding) 
        4. Rerank (GPU reranker)
        5. Context expansion v·ªõi nucleus strategy
        6. Generate answer (GPU LLM)
        """
        start_time = time.time()
        self.metrics["total_queries"] += 1
        
        try:
            # Get or create session
            if session_id:
                session = self.get_session(session_id)
                if not session:
                    return {"error": f"Session {session_id} not found"}
            else:
                session_id = self.create_session()
                session = self.get_session(session_id)
                
            logger.info(f"Processing query in session {session_id}: {query[:50]}...")
            
            # Step 1: Enhanced Smart Query Routing v·ªõi MULTI-LEVEL Confidence Processing
            if use_ambiguous_detection:
                routing_result = self.smart_router.route_query(query)
                confidence_level = routing_result.get('confidence_level', 'low')
                
                logger.info(f"Router confidence: {confidence_level} (score: {routing_result['confidence']:.3f})")
                
                if confidence_level == 'high':
                    # HIGH CONFIDENCE - Route tr·ª±c ti·∫øp
                    target_collection = routing_result['target_collection']
                    inferred_filters = routing_result.get('inferred_filters', {})
                    best_collections = [target_collection] if target_collection else [settings.chroma_collection_name]
                    logger.info(f"‚úÖ HIGH CONFIDENCE routing to: {target_collection}")
                    
                else:
                    # T·∫§T C·∫¢ CONFIDENCE < HIGH THRESHOLD - H·ªèi l·∫°i user, kh√¥ng route
                    logger.info(f"ü§î CONFIDENCE KH√îNG ƒê·ª¶ CAO ({confidence_level}) - h·ªèi l·∫°i user thay v√¨ route")
                    return self._generate_smart_clarification(routing_result, query, session_id, start_time)
            
            else:
                # Fallback routing logic (gi·ªØ nguy√™n logic c≈©)
                routing_result = self.smart_router.route_query(query)
                if routing_result.get('status') == 'routed' and routing_result.get('target_collection'):
                    target_collection = routing_result['target_collection']
                    inferred_filters = routing_result.get('inferred_filters', {})
                    best_collections = [target_collection]
                    logger.info(f"Fallback routed to collection: {target_collection}")
                else:
                    best_collections = [settings.chroma_collection_name]
                    inferred_filters = {}
            
            # Step 2: Focused Search trong target collection v·ªõi smart filters
            broad_search_results = []
            for collection_name in best_collections[:2]:  # Limit to top 2 collections
                try:
                    # ‚úÖ CRITICAL FIX: Pass smart filters to vector search
                    results = self.vectordb_service.search_in_collection(
                        collection_name=collection_name,
                        query=query,
                        top_k=settings.broad_search_k,
                        similarity_threshold=settings.similarity_threshold,
                        where_filter=inferred_filters if inferred_filters else None
                    )
                    
                    for result in results:
                        result["collection"] = collection_name
                        
                    broad_search_results.extend(results)
                    
                except Exception as e:
                    logger.warning(f"Error searching in collection {collection_name}: {e}")
            
            if not broad_search_results:
                return {
                    "type": "no_results",
                    "message": "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n.",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            logger.info(f"Found {len(broad_search_results)} candidate chunks")
            
            # Step 4: Reranking (GPU - high precision) - L·∫§Y 1 CHUNK CAO NH·∫§T SAU RERANK
            if settings.use_reranker and len(broad_search_results) > 1:
                # ‚úÖ FIX CRITICAL BUG: Rerank T·∫§T C·∫¢ documents thay v√¨ ch·ªâ top 10
                # ƒê√¢y l√† l·ªói logic nghi√™m tr·ªçng - kh√¥ng ƒë∆∞·ª£c v·ª©t b·ªè documents ti·ªÅm nƒÉng!
                docs_to_rerank = broad_search_results  # RERANK ALL DOCUMENTS
                logger.info(f"Reranking ALL {len(broad_search_results)} candidate documents (FIXED BUG)")
                
                nucleus_chunks = self.reranker_service.rerank_documents(
                    query=query,
                    documents=docs_to_rerank,
                    top_k=1  # CH·ªà 1 nucleus chunk cao nh·∫•t - s·∫Ω expand to√†n b·ªô document ch·ª©a chunk n√†y
                )
                
                # ‚úÖ INTELLIGENT VALIDATION: Ki·ªÉm tra rerank score v√† ƒëi·ªÅu ch·ªânh strategy
                if nucleus_chunks and len(nucleus_chunks) > 0:
                    best_score = nucleus_chunks[0].get('rerank_score', 0)
                    logger.info(f"Best rerank score: {best_score:.4f}")
                    
                    # CRITICAL DECISION POINT: N·∫øu ƒëi·ªÉm th·∫•p, chuy·ªÉn sang Conservative Strategy
                    if best_score < 0.2:  # Ng∆∞·ª°ng nghi√™m ng·∫∑t h∆°n
                        logger.warning(f"‚ö†Ô∏è  LOW RERANK SCORE ({best_score:.4f}) - Chuy·ªÉn sang Conservative Strategy!")
                        logger.warning("üí° Strategy: Ch·ªâ s·ª≠ d·ª•ng chunk c√≥ li√™n quan nh·∫•t, KH√îNG expand full document")
                        
                        # Conservative Strategy: KH√îNG expand full document
                        use_full_document_expansion = False
                        max_context_length = 800  # Gi·∫£m context length drastically
                        
                        # L·ªçc th√™m 1 l·∫ßn n·ªØa - ch·ªâ gi·ªØ chunks th·ª±c s·ª± c√≥ keyword li√™n quan
                        filtered_chunks = []
                        query_keywords = query.lower().split()
                        
                        for chunk in nucleus_chunks:
                            chunk_content = chunk.get('content', '').lower()
                            # Ki·ªÉm tra xem chunk c√≥ ch·ª©a t·ª´ kh√≥a li√™n quan kh√¥ng
                            if any(keyword in chunk_content for keyword in ['ph√≠', 'ti·ªÅn', 'l·ªá ph√≠', 'mi·ªÖn', 'cost', 'fee']):
                                filtered_chunks.append(chunk)
                                logger.info(f"‚úÖ Chunk ƒë∆∞·ª£c gi·ªØ l·∫°i v√¨ c√≥ t·ª´ kh√≥a li√™n quan")
                                break  # Ch·ªâ l·∫•y 1 chunk t·ªët nh·∫•t
                        
                        nucleus_chunks = filtered_chunks if filtered_chunks else nucleus_chunks[:1]
                        logger.info(f"üéØ Conservative Strategy: S·ª≠ d·ª•ng {len(nucleus_chunks)} chunk v·ªõi context gi·ªõi h·∫°n")
                    
                    else:
                        logger.info(f"‚úÖ HIGH RERANK SCORE ({best_score:.4f}) - S·ª≠ d·ª•ng Full Document Strategy")
                
                logger.info(f"Selected {len(nucleus_chunks)} nucleus chunk with rerank-based strategy")
            else:
                nucleus_chunks = broad_search_results[:1]  # Fallback: l·∫•y chunk t·ªët nh·∫•t theo vector similarity
                
            # Step 5: Intelligent Context Expansion - D·ª±a tr√™n rerank score ƒë·ªÉ quy·∫øt ƒë·ªãnh strategy
            expanded_context = None
            if use_full_document_expansion:
                logger.info("üìà Using FULL DOCUMENT expansion strategy")
                self.metrics["context_expansions"] += 1
                expanded_context = self.context_expansion_service.expand_context_with_nucleus(
                    nucleus_chunks=nucleus_chunks,
                    max_context_length=max_context_length,
                    include_full_document=True  # Full document expansion
                )
                
                context_text = self._build_context_from_expanded(expanded_context)
                logger.info(f"Context expanded: {expanded_context['total_length']} chars from {len(expanded_context.get('source_documents', []))} documents")
                
            else:
                logger.info("üéØ Using CONSERVATIVE chunk-only strategy")
                # Conservative Strategy: Ch·ªâ s·ª≠ d·ª•ng chunk ch√≠nh x√°c, KH√îNG expand
                context_parts = []
                for chunk in nucleus_chunks:
                    source = chunk.get('metadata', {}).get('source', 'N/A')
                    content = chunk.get('content', '')
                    
                    # Ch·ªâ l·∫•y nh·ªØng c√¢u c√≥ li√™n quan ƒë·∫øn query
                    query_keywords = ['ph√≠', 'ti·ªÅn', 'l·ªá ph√≠', 'mi·ªÖn', 'cost', 'fee']
                    sentences = content.split('.')
                    relevant_sentences = []
                    
                    for sentence in sentences:
                        if any(keyword in sentence.lower() for keyword in query_keywords):
                            relevant_sentences.append(sentence.strip())
                    
                    if relevant_sentences:
                        relevant_content = '. '.join(relevant_sentences[:2])  # Top 2 relevant sentences only
                        context_parts.append(f"üìÑ Ngu·ªìn: {source}\n{relevant_content}")
                    else:
                        # Fallback: truncated content
                        truncated_content = content[:400] + "..." if len(content) > 400 else content
                        context_parts.append(f"üìÑ Ngu·ªìn: {source}\n{truncated_content}")
                
                context_text = "\n\n".join(context_parts)
                logger.info(f"Conservative context: {len(context_text)} chars, focused on relevant sentences only")
            
            # Step 6: Generate Answer (GPU LLM)
            if not session:
                return {
                    "type": "error",
                    "error": "Session not found",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            answer = self._generate_answer_with_context(
                query=query,
                context=context_text,
                session=session
            )
            
            # Update session history
            session.query_history.append({
                "query": query,
                "answer": answer,
                "timestamp": time.time(),
                "nucleus_chunks_count": len(nucleus_chunks),
                "context_length": len(context_text)
            })
            
            # Keep only last 10 queries in session
            if len(session.query_history) > 10:
                session.query_history = session.query_history[-10:]
                
            processing_time = time.time() - start_time
            self.metrics["avg_response_time"] = (
                (self.metrics["avg_response_time"] * (self.metrics["total_queries"] - 1) + processing_time) 
                / self.metrics["total_queries"]
            )
            
            return {
                "type": "answer",
                "answer": answer,
                "context_info": {
                    "nucleus_chunks": len(nucleus_chunks),
                    "context_length": len(context_text),
                    "source_collections": list(set(chunk.get("collection", "") for chunk in nucleus_chunks)),
                    "source_documents": list(expanded_context.get("source_documents", [])) if expanded_context else []
                },
                "session_id": session_id,
                "processing_time": processing_time,
                "routing_info": {"best_collections": best_collections}
            }
            
        except Exception as e:
            logger.error(f"Error in enhanced query: {e}")
            return {
                "type": "error",
                "error": str(e),
                "session_id": session_id,
                "processing_time": time.time() - start_time
            }
            
    def handle_clarification(
        self,
        session_id: str,
        selected_option: str,
        original_query: str
    ) -> Dict[str, Any]:
        """X·ª≠ l√Ω ph·∫£n h·ªìi clarification"""
        session = self.get_session(session_id)
        if not session:
            return {"error": f"Session {session_id} not found"}
            
        # T·∫°o refined query
        refined_query = f"{original_query} - {selected_option}"
        
        # Process refined query (s·∫Ω kh√¥ng ambiguous n·ªØa)
        return self.enhanced_query(
            query=refined_query,
            session_id=session_id,
            use_ambiguous_detection=False  # Skip ambiguous detection for refined query
        )
        
    def _build_context_from_expanded(self, expanded_context: Dict[str, Any]) -> str:
        """Build context string t·ª´ expanded context"""
        context_parts = []
        
        for doc_content in expanded_context.get("expanded_content", []):
            source = doc_content.get("source", "N/A")
            text = doc_content.get("text", "")
            chunk_count = doc_content.get("chunk_count", 0)
            
            context_parts.append(f"=== T√†i li·ªáu: {source} ({chunk_count} ƒëo·∫°n) ===\n{text}")
            
        return "\n\n".join(context_parts)
        
    def _generate_answer_with_context(
        self,
        query: str,
        context: str,
        session: OptimizedChatSession
    ) -> str:
        """Generate answer v·ªõi context v√† session history"""
        
        # Build conversation context if needed
        conversation_context = ""
        if len(session.query_history) > 0:
            recent_queries = session.query_history[-3:]  # Last 3 queries
            conversation_context = "L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y:\n" + "\n".join([
                f"Q: {item['query']}\nA: {item['answer'][:200]}..." 
                for item in recent_queries
            ]) + "\n\n"
            
        # Build intelligent system prompt - t√πy thu·ªôc v√†o ƒë·ªô d√†i context
        context_length = len(context)
        if context_length < 1000:
            # Conservative Strategy: Context ng·∫Øn, y√™u c·∫ßu LLM t·∫≠p trung cao ƒë·ªô
            system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam.

üéØ NHI·ªÜM V·ª§: Tr·∫£ l·ªùi CH√çNH X√ÅC d·ª±a tr√™n th√¥ng tin ng·∫Øn g·ªçn ƒë∆∞·ª£c cung c·∫•p.

QUY T·∫ÆC:
1. CH·ªà d√πng th√¥ng tin C√ì TRONG t√†i li·ªáu
2. Tr·∫£ l·ªùi NG·∫ÆN G·ªåN (1-2 c√¢u) 
3. T·∫≠p trung v√†o t·ª´ kh√≥a ch√≠nh trong c√¢u h·ªèi
4. N·∫øu c√≥ th√¥ng tin v·ªÅ "ph√≠" ho·∫∑c "mi·ªÖn ph√≠" - n√™u r√µ ngay

Tr·∫£ l·ªùi tr·ª±c ti·∫øp, kh√¥ng d√†i d√≤ng."""
        else:
            # Full Document Strategy: Context d√†i, c·∫ßn h∆∞·ªõng d·∫´n chi ti·∫øt h∆°n
            system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam.

üö® QUY T·∫ÆC B·∫ÆT BU·ªòC - KH√îNG ƒê∆Ø·ª¢C VI PH·∫†M:
1. CH·ªà tr·∫£ l·ªùi d·ª±a CH√çNH X√ÅC tr√™n th√¥ng tin C√ì TRONG t√†i li·ªáu
2. N·∫øu h·ªèi v·ªÅ PH√ç/TI·ªÄN - t√¨m th√¥ng tin "üí∞ TH√îNG TIN PH√ç/L·ªÜ PH√ç" trong t√†i li·ªáu
3. N·∫øu c√≥ th√¥ng tin "Mi·ªÖn l·ªá ph√≠" - ph·∫£i ∆∞u ti√™n n√™u r√µ ƒëi·ªÅu n√†y
4. KH√îNG t·ª± s√°ng t·∫°o th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu
5. Tr·∫£ l·ªùi NG·∫ÆN G·ªåN (t·ªëi ƒëa 3-4 c√¢u)
6. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn - n√≥i "Theo th√¥ng tin trong t√†i li·ªáu..."

V√≠ d·ª• tr·∫£ l·ªùi t·ªët:
- "Theo th√¥ng tin trong t√†i li·ªáu, ƒëƒÉng k√Ω khai sinh ƒë√∫ng h·∫°n ƒë∆∞·ª£c MI·ªÑN L·ªÜ PH√ç."
- "T√†i li·ªáu n√™u r√µ l·ªá ph√≠ l√† X ƒë·ªìng cho tr∆∞·ªùng h·ª£p Y."

TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c t·ª± t·∫°o ra th√¥ng tin v·ªÅ ph√≠ ho·∫∑c c√°c quy ƒë·ªãnh kh√¥ng c√≥ trong t√†i li·ªáu."""
        
        logger.info(f"üìù Using {'CONSERVATIVE' if context_length < 1000 else 'FULL'} system prompt for context length: {context_length}")
        
        # Build enhanced context v·ªõi conversation history
        enhanced_context = conversation_context + context

        try:
            response_data = self.llm_service.generate_response(
                user_query=query,
                context=enhanced_context,
                max_tokens=settings.max_tokens,
                temperature=settings.temperature,
                system_prompt=system_prompt
            )
            
            # Extract response text from dict
            if isinstance(response_data, dict) and "response" in response_data:
                return response_data["response"].strip()
            elif isinstance(response_data, str):
                return response_data.strip()
            else:
                return str(response_data).strip()
            
        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            return f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t·∫°o c√¢u tr·∫£ l·ªùi: {e}"
            
    def get_health_status(self) -> Dict[str, Any]:
        """Tr·∫°ng th√°i health c·ªßa service"""
        try:
            collections = self.vectordb_service.list_collections()
            total_documents = 0
            
            for collection_info in collections:
                try:
                    collection = self.vectordb_service.get_collection(collection_info["name"])
                    count = collection.count()
                    total_documents += count
                except:
                    continue
                    
            return {
                "status": "healthy",
                "total_collections": len(collections),
                "total_documents": total_documents,
                "llm_loaded": self.llm_service.model is not None,
                "reranker_loaded": self.reranker_service.model is not None,
                "embedding_device": "CPU (VRAM optimized)",
                "llm_device": "GPU",
                "reranker_device": "GPU",
                "active_sessions": len(self.chat_sessions),
                "metrics": self.metrics,
                "router_stats": self.smart_router.get_collection_info(),
                "context_expansion": {
                    "total_chunks_cached": len(self.context_expansion_service.document_metadata_cache),
                    **self.context_expansion_service.get_stats()
                }
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
            
    def cleanup_old_sessions(self, max_age_hours: int = 24):
        """D·ªçn d·∫πp sessions c≈©"""
        current_time = time.time()
        cutoff_time = current_time - (max_age_hours * 3600)
        
        old_sessions = [
            session_id for session_id, session in self.chat_sessions.items()
            if session.last_accessed < cutoff_time
        ]
        
        for session_id in old_sessions:
            del self.chat_sessions[session_id]
            
        if old_sessions:
            logger.info(f"Cleaned up {len(old_sessions)} old sessions")
            
        return len(old_sessions)

    # API Compatibility Methods
    def query(self, question: Optional[str] = None, query: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """Compatibility method cho API routes"""
        # H·ªó tr·ª£ c·∫£ 'question' v√† 'query' parameters
        query_text = question or query
        if not query_text:
            raise ValueError("Either 'question' or 'query' parameter is required")
        return self.enhanced_query(query_text, **kwargs)
    
    @property
    def query_router(self):
        """Compatibility property cho API routes"""
        # T·∫°o wrapper v·ªõi explain_routing method
        class RouterWrapper:
            def __init__(self, smart_router):
                self.smart_router = smart_router
                # Copy t·∫•t c·∫£ methods t·ª´ smart_router
                for attr in dir(smart_router):
                    if not attr.startswith('_') and callable(getattr(smart_router, attr)):
                        setattr(self, attr, getattr(smart_router, attr))
            
            def explain_routing(self, question: str) -> Dict[str, Any]:
                """Explain routing decision cho question"""
                try:
                    # S·ª≠ d·ª•ng smart router ƒë·ªÉ classify
                    route_result = self.smart_router.route_query(question)
                    return {
                        'question': question,
                        'route': route_result.get('route_name', 'general'),
                        'confidence': route_result.get('confidence', 0.0),
                        'reasoning': route_result.get('reasoning', 'No reasoning available'),
                        'suggested_collections': route_result.get('suggested_collections', [])
                    }
                except Exception as e:
                    logger.error(f"Error explaining routing: {e}")
                    return {
                        'question': question,
                        'route': 'general',
                        'confidence': 0.0,
                        'reasoning': f'Error: {str(e)}',
                        'suggested_collections': []
                    }
        
        return RouterWrapper(self.smart_router)

    def build_index(self, collection_name: Optional[str] = None, force_rebuild: bool = False, **kwargs) -> Dict[str, Any]:
        """Build index cho collection ho·∫∑c t·∫•t c·∫£ collections"""
        try:
            if collection_name:
                # Build specific collection
                if self.vectordb_service.collection_exists(collection_name):
                    stats = self.vectordb_service.get_collection_stats(collection_name)
                    return {
                        'status': 'success',
                        'collections_processed': 1,
                        'collection_name': collection_name,
                        'message': f'Collection {collection_name} already exists and ready',
                        'document_count': stats.get('document_count', 0)
                    }
                else:
                    return {
                        'status': 'error',
                        'collections_processed': 0,
                        'collection_name': collection_name,
                        'error': f'Collection {collection_name} does not exist',
                        'suggestion': 'Run python tools/2_build_vectordb_final.py to build collections'
                    }
            else:
                # Build all collections - return info about existing ones
                collections = self.vectordb_service.list_collections()
                return {
                    'status': 'success',
                    'collections_processed': len(collections),
                    'message': f'Found {len(collections)} existing collections',
                    'collections': [col['name'] for col in collections],
                    'total_documents': sum(col.get('document_count', 0) for col in collections),
                    'suggestion': 'Use python tools/2_build_vectordb_final.py to build new collections from documents'
                }
        except Exception as e:
            logger.error(f"Error in build_index: {e}")
            return {
                'status': 'error',
                'collections_processed': 0,
                'error': str(e)
            }
    
    def _generate_smart_clarification(self, routing_result: Dict[str, Any], query: str, session_id: str, start_time: float) -> Dict[str, Any]:
        """T·∫°o clarification th√¥ng minh d·ª±a tr√™n routing result"""
        try:
            self.metrics["ambiguous_detected"] += 1
            
            # T·∫°o clarification v·ªõi suggestions t·ª´ routing result
            best_match = routing_result.get('matched_example', '')
            source_procedure = routing_result.get('source_procedure', '')
            confidence = routing_result.get('confidence', 0.0)
            
            # T·∫°o clarification message v·ªõi context
            clarification_msg = f"T√¥i nghƒ© b·∫°n c√≥ th·ªÉ mu·ªën h·ªèi v·ªÅ '{source_procedure}' (ƒë·ªô tin c·∫≠y: {confidence:.3f}). ƒê√∫ng kh√¥ng?"
            
            # T·∫°o options d·ª±a tr√™n best match v√† c√°c alternatives
            options = []
            
            # Option 1: Best match t·ª´ router
            if source_procedure:
                options.append({
                    'id': '1',
                    'title': f"ƒê√∫ng - v·ªÅ {source_procedure}",
                    'description': f"C√¢u h·ªèi t∆∞∆°ng t·ª±: {best_match[:100]}..." if best_match else "ƒê√∫ng, t√¥i mu·ªën h·ªèi v·ªÅ th·ªß t·ª•c n√†y",
                    'collection': routing_result.get('target_collection'),
                    'procedure': source_procedure
                })
            
            # Option 2: Generic alternatives
            options.append({
                'id': '2', 
                'title': "Kh√¥ng, t√¥i mu·ªën h·ªèi v·ªÅ th·ªß t·ª•c kh√°c",
                'description': "H√£y cho t√¥i bi·∫øt r√µ h∆°n th·ªß t·ª•c n√†o b·∫°n quan t√¢m",
                'collection': None,
                'procedure': None
            })
            
            if not options:
                # Fallback n·∫øu kh√¥ng c√≥ suggestions - return proper structure
                return {
                    "type": "clarification_needed",
                    "status": "smart_clarification",
                    "confidence": routing_result.get('confidence', 0.0),
                    "clarification": {
                        "message": "Xin l·ªói, t√¥i kh√¥ng r√µ √Ω ƒë·ªãnh c·ªßa c√¢u h·ªèi. B·∫°n c√≥ th·ªÉ di·ªÖn ƒë·∫°t r√µ h∆°n kh√¥ng?",
                        "options": [],
                        "suggestions": [
                            "B·∫°n c√≥ th·ªÉ n√≥i r√µ h∆°n v·ªÅ th·ªß t·ª•c n√†o b·∫°n mu·ªën th·ª±c hi·ªán?",
                            "V√≠ d·ª•: ƒëƒÉng k√Ω khai sinh, k·∫øt h√¥n, ch·ª©ng th·ª±c, hay th·ªß t·ª•c kh√°c?"
                        ]
                    },
                    "session_id": session_id,
                    "processing_time": time.time() - start_time,
                    "strategy": "generic_clarification"
                }
            
            logger.info(f"Generated smart clarification with {len(options)} options")
            
            return {
                "type": "clarification_needed",
                "status": "smart_clarification", 
                "confidence": routing_result.get('confidence', 0.0),
                "clarification": {
                    "message": clarification_msg,
                    "options": options
                },
                "matched_example": routing_result.get('matched_example'),
                "source_procedure": routing_result.get('source_procedure'),
                "session_id": session_id,
                "processing_time": time.time() - start_time,
                "strategy": "smart_suggestion"
            }
            
        except Exception as e:
            logger.error(f"Error generating smart clarification: {e}")
            return {
                "type": "clarification_needed", 
                "status": "error",
                "clarification": "Xin l·ªói, c√≥ l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi. B·∫°n c√≥ th·ªÉ th·ª≠ l·∫°i kh√¥ng?",
                "session_id": session_id,
                "processing_time": time.time() - start_time
            }
    
    def _activate_vector_backup_strategy(self, routing_result: Dict[str, Any], query: str, session_id: str, start_time: float) -> Dict[str, Any]:
        """K√≠ch ho·∫°t Vector Backup Strategy khi Smart Router ho√†n to√†n th·∫•t b·∫°i"""
        try:
            logger.info("üö® Activating Vector Backup Strategy - searching across all collections")
            
            # Th·ª±c hi·ªán vector search tr·ª±c ti·∫øp tr√™n t·∫•t c·∫£ collections ƒë·ªÉ t√¨m topics li√™n quan
            all_collections = self.vectordb_service.list_collections()
            backup_results = []
            
            for collection_info in all_collections[:3]:  # Limit to top 3 collections for performance
                collection_name = collection_info["name"]
                try:
                    collection = self.vectordb_service.get_collection(collection_name)
                    search_results = self.vectordb_service.search_in_collection(
                        collection_name,
                        query,
                        top_k=2,  # Ch·ªâ l·∫•y top 2 results per collection
                        similarity_threshold=0.3,
                        where_filter={}
                    )
                    
                    if search_results:
                        best_result = search_results[0]
                        backup_results.append({
                            'collection': collection_name,
                            'score': best_result.get('similarity', best_result.get('score', 0)),
                            'content': best_result.get('content', best_result.get('document', ''))[:200] + "...",
                            'metadata': best_result.get('metadata', {}),
                            'source': best_result.get('metadata', {}).get('source', 'N/A')
                        })
                        
                except Exception as e:
                    logger.warning(f"Error searching collection {collection_name}: {e}")
                    continue
            
            # Sort by score v√† t·∫°o suggestions
            backup_results.sort(key=lambda x: x['score'], reverse=True)
            
            options = []
            for i, result in enumerate(backup_results[:3], 1):
                # Tr√≠ch xu·∫•t title t·ª´ metadata n·∫øu c√≥
                metadata = result.get('metadata', {})
                title = metadata.get('document_title', metadata.get('title', f"Th·ªß t·ª•c {result['collection']}"))
                
                option = {
                    'id': str(i),
                    'title': title,
                    'description': f"ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng: {result['score']:.2f} - {result['content']}",
                    'collection': result['collection'],
                    'backup_score': result['score']
                }
                options.append(option)
            
            clarification_msg = "T√¥i kh√¥ng t√¨m th·∫•y c√¢u h·ªèi m·∫´u ph√π h·ª£p, nh∆∞ng d·ª±a tr√™n t√¨m ki·∫øm trong d·ªØ li·ªáu, c√¢u h·ªèi c·ªßa b·∫°n c√≥ th·ªÉ li√™n quan ƒë·∫øn:"
            
            if not options:
                clarification_msg = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p. B·∫°n c√≥ th·ªÉ th·ª≠ v·ªõi t·ª´ kh√≥a kh√°c kh√¥ng?"
            
            logger.info(f"Vector backup strategy found {len(options)} potential matches")
            
            return {
                "type": "clarification_needed",
                "status": "vector_backup",
                "confidence": routing_result.get('confidence', 0.0),
                "clarification": clarification_msg,
                "options": options,
                "backup_results": len(backup_results),
                "session_id": session_id,
                "processing_time": time.time() - start_time,
                "strategy": "vector_backup"
            }
            
        except Exception as e:
            logger.error(f"Error in vector backup strategy: {e}")
            return {
                "type": "clarification_needed",
                "status": "fallback_error", 
                "clarification": "Xin l·ªói, c√≥ l·ªói h·ªá th·ªëng khi x·ª≠ l√Ω c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i sau.",
                "session_id": session_id,
                "processing_time": time.time() - start_time
            }
    
    @property  
    def document_processor(self):
        """Compatibility property cho API routes"""
        import os
        
        class DocumentProcessorCompat:
            def get_available_collections(self, documents_dir):
                """L·∫•y danh s√°ch collections c√≥ th·ªÉ t·∫°o t·ª´ documents"""
                try:
                    if not os.path.exists(documents_dir):
                        return []
                    
                    collections = []
                    for item in os.listdir(documents_dir):
                        item_path = os.path.join(documents_dir, item)
                        if os.path.isdir(item_path):
                            # ƒê·∫øm s·ªë files PDF trong th∆∞ m·ª•c
                            pdf_count = len([f for f in os.listdir(item_path) 
                                           if f.lower().endswith('.pdf')])
                            if pdf_count > 0:
                                collections.append({
                                    'name': item,
                                    'path': item_path,
                                    'document_count': pdf_count
                                })
                    return collections
                except Exception as e:
                    logger.error(f"Error getting available collections: {e}")
                    return []
        
        return DocumentProcessorCompat()
