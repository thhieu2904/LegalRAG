"""
Optimized Enhanced RAG Service v·ªõi VRAM-optimized architecture
T√≠ch h·ª£p:
1. Ambiguous Query Detection & Processing
2. Enhanced Context Expansion v·ªõi Nucleus Strategy  
3. VRAM-optimized model placement (CPU embedding, GPU LLM/Reranker)
4. Session management
"""

import logging
import time
import uuid
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from pathlib import Path

from .vector import VectorDBService
from .language_model import LLMService
from .reranker import RerankerService
from .clarification import ClarificationService
from .router import QueryRouter, RouterBasedQueryService
from .context import ContextExpander
from .simple_form_detection import SimpleFormDetectionService
from ..core.config import settings

# Import path_config with try/except for graceful fallback
try:
    from ..core.path_config import path_config
except ImportError:
    logger.warning("PathConfig not available, using old structure only")
    path_config = None

logger = logging.getLogger(__name__)

def convert_numpy_types(obj: Any) -> Any:
    """Convert numpy types to Python native types for JSON serialization"""
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(v) for v in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(v) for v in obj)
    else:
        return obj

@dataclass
class OptimizedChatSession:
    """Session chat v·ªõi th√¥ng tin t·ªëi ∆∞u v·ªõi Stateful Router support"""
    session_id: str
    created_at: float
    last_accessed: float
    query_history: List[Dict[str, Any]] = field(default_factory=list)
    context_cache: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # Stateful Router State
    last_successful_collection: Optional[str] = None
    last_successful_confidence: float = 0.0
    last_successful_timestamp: Optional[float] = None
    last_successful_filters: Optional[Dict[str, Any]] = None  # üî• NEW: L∆∞u filters t·ª´ session th√†nh c√¥ng
    cached_rag_content: Optional[Dict[str, Any]] = None
    consecutive_low_confidence_count: int = 0
    
    def update_successful_routing(self, collection: str, confidence: float, filters: Optional[Dict[str, Any]] = None, rag_content: Optional[Dict[str, Any]] = None):
        """C·∫≠p nh·∫≠t state khi routing th√†nh c√¥ng v·ªõi confidence cao"""
        self.last_successful_collection = collection
        self.last_successful_confidence = confidence
        self.last_successful_timestamp = time.time()
        self.last_successful_filters = filters  # üî• NEW: L∆∞u filters
        if rag_content:
            self.cached_rag_content = rag_content
        self.consecutive_low_confidence_count = 0  # Reset counter
        
    def should_override_confidence(self, current_confidence: float) -> bool:
        """
        Ki·ªÉm tra c√≥ n√™n ghi ƒë√® k·∫øt qu·∫£ ƒë·ªãnh tuy·∫øn hi·ªán t·∫°i b·∫±ng ng·ªØ c·∫£nh ƒë√£ l∆∞u kh√¥ng.
        Ghi ƒë√® khi:
        1. ƒêang c√≥ ng·ªØ c·∫£nh t·ªët ƒë∆∞·ª£c l∆∞u t·ª´ tr∆∞·ªõc.
        2. K·∫øt qu·∫£ ƒë·ªãnh tuy·∫øn m·ªõi kh√¥ng ph·∫£i l√† "r·∫•t ch·∫Øc ch·∫Øn".
        """
        if not self.last_successful_collection:
            return False

        # Ch·ªâ ghi ƒë√® trong v√≤ng 10 ph√∫t
        if self.last_successful_timestamp and (time.time() - self.last_successful_timestamp > 600):
            return False

        # Ng∆∞·ª°ng tin c·∫≠y "r·∫•t cao" m√† ch√∫ng ta s·∫Ω kh√¥ng can thi·ªáp
        VERY_HIGH_CONFIDENCE_GATE = 0.82 
        # Ng∆∞·ª°ng t·ªëi thi·ªÉu c·ªßa ng·ªØ c·∫£nh ƒë√£ l∆∞u ƒë·ªÉ ƒë∆∞·ª£c coi l√† "t·ªët"
        MIN_CONTEXT_CONFIDENCE = 0.78

        # N·∫øu ƒë·ªô tin c·∫≠y hi·ªán t·∫°i kh√¥ng ƒë·ªß cao V√Ä ng·ªØ c·∫£nh tr∆∞·ªõc ƒë√≥ ƒë·ªß t·ªët -> Ghi ƒë√®
        if current_confidence < VERY_HIGH_CONFIDENCE_GATE and self.last_successful_confidence >= MIN_CONTEXT_CONFIDENCE:
            logger.info(f"üî• STATEFUL ROUTER: Ghi ƒë√® v√¨ current_confidence ({current_confidence:.3f}) < {VERY_HIGH_CONFIDENCE_GATE} v√† context_confidence ({self.last_successful_confidence:.3f}) >= {MIN_CONTEXT_CONFIDENCE}")
            return True

        return False
        
    def increment_low_confidence(self):
        """TƒÉng counter khi g·∫∑p confidence th·∫•p"""
        self.consecutive_low_confidence_count += 1
        
    def clear_routing_state(self):
        """Clear state khi user chuy·ªÉn ch·ªß ƒë·ªÅ ho√†n to√†n"""
        self.last_successful_collection = None
        self.last_successful_confidence = 0.0
        self.last_successful_timestamp = None
        self.last_successful_filters = None  # üî• NEW: Clear filters c≈©
        self.cached_rag_content = None
        self.consecutive_low_confidence_count = 0
    
    def get_context_summary(self) -> Dict[str, Any]:
        """
        T·∫°o context summary ƒë·ªÉ hi·ªÉn th·ªã tr√™n frontend
        """
        context_summary = {
            "session_id": self.session_id,
            "has_active_context": False,
            "current_collection": None,
            "current_collection_display": None,
            "preserved_document": None,
            "active_filters": {},
            "confidence_level": 0.0,
            "context_age_minutes": 0,
            "query_count": len(self.query_history),
            "last_activity": self.last_accessed
        }
        
        # Collection mappings cho display names
        collection_display_map = {
            "luat_doanh_nghiep_2020": "Lu·∫≠t Doanh nghi·ªáp 2020",
            "luat_dat_dai_2013": "Lu·∫≠t ƒê·∫•t ƒëai 2013", 
            "luat_lao_dong_2019": "Lu·∫≠t Lao ƒë·ªông 2019",
            "luat_hon_nhan_gia_dinh_2014": "Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014",
            "luat_dan_su_2015": "Lu·∫≠t D√¢n s·ª± 2015",
            "luat_hinh_su_2015": "Lu·∫≠t H√¨nh s·ª± 2015",
            "luat_thue_thu_nhap_ca_nhan_2007": "Lu·∫≠t Thu·∫ø Thu nh·∫≠p c√° nh√¢n 2007",
            "luat_bao_hiem_xa_hoi_2014": "Lu·∫≠t B·∫£o hi·ªÉm x√£ h·ªôi 2014"
        }
        
        # Ki·ªÉm tra c√≥ active context kh√¥ng
        if self.last_successful_collection and self.last_successful_timestamp:
            # T√≠nh tu·ªïi c·ªßa context (ph√∫t)
            context_age_seconds = time.time() - self.last_successful_timestamp
            context_age_minutes = int(context_age_seconds / 60)
            
            # Context v·∫´n valid trong 10 ph√∫t
            if context_age_minutes <= 10:
                context_summary.update({
                    "has_active_context": True,
                    "current_collection": self.last_successful_collection,
                    "current_collection_display": collection_display_map.get(
                        self.last_successful_collection, 
                        self.last_successful_collection
                    ),
                    "confidence_level": self.last_successful_confidence,
                    "context_age_minutes": context_age_minutes,
                    "active_filters": self.last_successful_filters or {}
                })
                
                # Ki·ªÉm tra c√≥ document ƒë∆∞·ª£c preserve kh√¥ng
                # üîß FIX: Check multiple sources for document info
                preserved_document = None
                
                # Priority 1: Check session metadata for preserved document (from clarification flow)
                if self.metadata and 'preserved_document' in self.metadata:
                    preserved_doc = self.metadata['preserved_document']
                    if isinstance(preserved_doc, dict) and 'title' in preserved_doc:
                        preserved_document = preserved_doc['title']
                    elif isinstance(preserved_doc, str):
                        preserved_document = preserved_doc
                
                # Priority 2: Check current document from recent queries
                if not preserved_document and self.metadata and 'current_document' in self.metadata:
                    preserved_document = self.metadata['current_document']
                
                # Priority 3: Check successful filters
                if not preserved_document and self.last_successful_filters and "source_file" in self.last_successful_filters:
                    preserved_document = self.last_successful_filters["source_file"]
                
                if preserved_document:
                    context_summary["preserved_document"] = preserved_document
        
        return context_summary

class RAGService:
    """
    RAG Service ƒë∆∞·ª£c t·ªëi ∆∞u VRAM v√† performance
    
    Ki·∫øn tr√∫c:
    - Embedding Model: CPU (ti·∫øt ki·ªám VRAM cho query ng·∫Øn)
    - LLM: GPU (c·∫ßn song song h√≥a cho context d√†i)  
    - Reranker: GPU (c·∫ßn song song h√≥a cho multiple comparisons)
    """
    
    def __init__(
        self,
        documents_dir: Optional[str] = None,  # Made optional, will use path_config if not provided
        vectordb_service: Optional[VectorDBService] = None,
        llm_service: Optional[LLMService] = None
    ):
        # Use new path config by default, fallback to provided documents_dir for backward compatibility
        if documents_dir is None:
            # New structure - use path_config
            self.use_new_structure = True
            self.path_config = path_config
            self.documents_dir = None  # Not used in new structure
        else:
            # Old structure - backward compatibility
            self.use_new_structure = False
            self.path_config = None
            self.documents_dir = documents_dir
            
        self.vectordb_service = vectordb_service
        self.llm_service = llm_service
        
        # Initialize supporting services
        self._initialize_services()
        
        # Chat sessions management
        self.chat_sessions: Dict[str, OptimizedChatSession] = {}
        
        # Performance metrics
        self.metrics = {
            "total_queries": 0,
            "ambiguous_detected": 0,
            "context_expansions": 0,
            "avg_response_time": 0.0
        }
        
        logger.info("‚úÖ Optimized Enhanced RAG Service initialized")
        
    def _initialize_services(self):
        """Kh·ªüi t·∫°o c√°c service h·ªó tr·ª£ v·ªõi Enhanced Smart Router"""
        try:
            # Enhanced Smart Query Router v·ªõi Example Questions Database
            embedding_model = self.vectordb_service.embedding_model
            if embedding_model is None:
                raise ValueError("VectorDB embedding model not initialized")
            self.smart_router = QueryRouter(embedding_model=embedding_model)
            logger.info("‚úÖ Enhanced Smart Query Router initialized")
            
            # Reranker Service (GPU)
            self.reranker_service = RerankerService()
            logger.info("‚úÖ Reranker Service initialized (GPU)")
            
            # Router-based Ambiguous Query Service (CPU)
            self.ambiguous_service = RouterBasedQueryService(
                router=self.smart_router
            )
            logger.info("‚úÖ Router-based Ambiguous Query Service initialized (CPU)")
            
            # Smart Clarification Service
            self.clarification_service = ClarificationService()
            logger.info("‚úÖ Smart Clarification Service initialized")
            
            # Enhanced Context Expansion Service  
            self.context_expansion_service = ContextExpander(
                vectordb_service=self.vectordb_service,
                documents_dir=self.documents_dir
            )
            logger.info("‚úÖ Enhanced Context Expansion Service initialized")
            
            # Simple Form Detection Service - NEW  
            self.form_detection_service = SimpleFormDetectionService()
            logger.info("‚úÖ Simple Form Detection Service initialized")
            
        except Exception as e:
            logger.error(f"Error initializing services: {e}")
            raise
            
    def create_session(self, metadata: Optional[Dict[str, Any]] = None) -> str:
        """T·∫°o session chat m·ªõi"""
        session_id = str(uuid.uuid4())
        
        session = OptimizedChatSession(
            session_id=session_id,
            created_at=time.time(),
            last_accessed=time.time(),
            metadata=metadata or {}
        )
        
        self.chat_sessions[session_id] = session
        logger.info(f"Created new chat session: {session_id}")
        
        return session_id
        
    def get_session(self, session_id: str) -> Optional[OptimizedChatSession]:
        """L·∫•y session theo ID"""
        session = self.chat_sessions.get(session_id)
        if session:
            session.last_accessed = time.time()
        return session
    
    def get_session_context_summary(self, session_id: str) -> Optional[Dict[str, Any]]:
        """
        L·∫•y context summary c·ªßa session ƒë·ªÉ hi·ªÉn th·ªã tr√™n frontend
        """
        session = self.get_session(session_id)
        if not session:
            return None
        return session.get_context_summary()
    
    def reset_session_context(self, session_id: str) -> bool:
        """
        Reset ng·ªØ c·∫£nh c·ªßa session v·ªÅ tr·∫°ng th√°i m·∫∑c ƒë·ªãnh
        """
        session = self.get_session(session_id)
        if not session:
            return False
            
        # Clear routing state but keep session alive
        session.clear_routing_state()
        
        # Optionally clear query history (comment out if want to keep chat history)
        # session.query_history = []
        
        logger.info(f"üßπ Reset context for session: {session_id}")
        return True
        
    def process_query(
        self,
        query: str,
        session_id: Optional[str] = None,
        reranker_k: int = 10,
        llm_k: int = 5,
        threshold: float = 0.7,
        forced_collection: Optional[str] = None,  # ‚ö° TH√äM THAM S·ªê ANTI-LOOP
        forced_document_title: Optional[str] = None  # üî• NEW: Force exact document filtering
    ) -> Dict[str, Any]:
        """
        Query ch√≠nh v·ªõi t·∫•t c·∫£ t·ªëi ∆∞u h√≥a - THI·∫æT K·∫æ G·ªêC: FULL DOCUMENT EXPANSION
        
        Flow:
        1. Detect ambiguous query (CPU embedding)
        2. Route query n·∫øu clear
        3. Broad search (CPU embedding) 
        4. Rerank (GPU reranker)
        5. Context expansion: TO√ÄN B·ªò DOCUMENT (ƒë·∫£m b·∫£o ng·ªØ c·∫£nh ph√°p lu·∫≠t ƒë·∫ßy ƒë·ªß)
        6. Generate answer (GPU LLM)
        
        TRI·∫æT L√ù: VƒÉn b·∫£n ph√°p lu·∫≠t ph·∫£i ƒë∆∞·ª£c hi·ªÉu trong TO√ÄN B·ªò ng·ªØ c·∫£nh c·ªßa document g·ªëc
        """
        start_time = time.time()
        self.metrics["total_queries"] += 1
        
        try:
            # Get or create session
            if session_id:
                session = self.get_session(session_id)
                if not session:
                    # Create new session with provided ID
                    session = OptimizedChatSession(
                        session_id=session_id,
                        created_at=time.time(),
                        last_accessed=time.time(),
                        metadata={}
                    )
                    self.chat_sessions[session_id] = session
                    logger.info(f"üÜï Created new session with provided ID: {session_id}")
            else:
                session_id = self.create_session()
                session = self.get_session(session_id)
                
            logger.info(f"Processing query in session {session_id}: {query[:50]}...")
            
            # Check for preserved document context from manual input
            if not forced_document_title and not forced_collection and session:
                preserved_document = session.metadata.get('preserved_document')
                if preserved_document:
                    logger.info(f"üîÑ Found preserved document context: {preserved_document['title']}")
                    forced_collection = preserved_document['collection']
                    forced_document_title = preserved_document['title']
                    
                # üîß NEW: Check for manual input context
                manual_input_context = session.metadata.get('manual_input_context')
                if manual_input_context and manual_input_context.get('bypass_router'):
                    logger.info(f"üîÑ Found manual input context: {manual_input_context['collection']}")
                    forced_collection = manual_input_context['collection']
                    # Clear manual input context after use
                    session.metadata.pop('manual_input_context', None)
            
            # Step 1: Enhanced Smart Query Routing v·ªõi MULTI-LEVEL Confidence Processing + Stateful Router
            if forced_collection:
                # ÔøΩ FORCED ROUTING: D√†nh cho clarification ho·∫∑c debug
                logger.info(f"‚ö° Forced routing to collection: {forced_collection} (from clarification/manual input)")
                routing_result = {
                    "target_collection": forced_collection,
                    "confidence": 0.95,  # High confidence cho forced routing
                    "inferred_filters": {}
                }
                # Get confidence level from routing result for further processing
                confidence_level = routing_result.get('confidence_level', 'forced_high')
                best_collections = [forced_collection]
                inferred_filters = {}
                
                # üî• NEW: Add document title filter if specified
                if forced_document_title:
                    inferred_filters = {"document_title": forced_document_title}
                    logger.info(f"üéØ Forced document filter: {forced_document_title}")
                
            else:
                # üß† SMART ROUTING: S·ª≠ d·ª•ng router b√¨nh th∆∞·ªùng
                routing_result = self.smart_router.route_query(query, session)
                confidence_level = routing_result.get('confidence_level', 'low')
                was_overridden = routing_result.get('was_overridden', False)
                
                logger.info(f"Router confidence: {confidence_level} (score: {routing_result['confidence']:.3f})")
                if was_overridden:
                    logger.info(f"üî• Session-based confidence override applied!")
                
                if confidence_level in ['high', 'override_high', 'high_followup']:
                    # HIGH CONFIDENCE (including overridden & follow-up) - Route tr·ª±c ti·∫øp
                    target_collection = routing_result['target_collection']
                    inferred_filters = routing_result.get('inferred_filters', {})
                    best_collections = [target_collection] if target_collection else [settings.chroma_collection_name]
                    logger.info(f"‚úÖ HIGH CONFIDENCE routing to: {target_collection}")
                    
                elif confidence_level in ['medium_high', 'medium-high', 'override_medium_high']:
                    # MEDIUM-HIGH CONFIDENCE - Show questions within best document
                    logger.info(f"üéØ MEDIUM-HIGH CONFIDENCE ({routing_result['confidence']:.3f}) - showing questions in document")
                    
                    # üîß FIX: Set session context ƒë·ªÉ follow-up questions c√≥ th·ªÉ ho·∫°t ƒë·ªông 
                    if session:
                        target_collection = routing_result.get('target_collection')
                        session.last_successful_collection = target_collection
                        session.last_successful_filters = routing_result.get('inferred_filters', {})
                        session.last_successful_timestamp = start_time
                        logger.info(f"üîÑ Set session context for follow-up: {target_collection}")
                    
                    return self._generate_smart_clarification(routing_result, query, session_id, start_time)
                    
                elif confidence_level in ['low-medium', 'override_medium', 'medium_followup']:
                    # üî• MEDIUM CONFIDENCE FIX - Trigger clarification instead of routing
                    # V√¨ medium confidence c√≥ risk cao matching sai topic ‚Üí c·∫ßn h·ªèi user x√°c nh·∫≠n
                    logger.info(f"ü§î MEDIUM CONFIDENCE ({routing_result['confidence']:.3f}) - triggering clarification to avoid wrong routing")
                    
                    # üîß FIX: Set session context cho follow-up (medium confidence v·∫´n c√≥ potential collection)
                    if session:
                        target_collection = routing_result.get('target_collection')
                        session.last_successful_collection = target_collection
                        session.last_successful_filters = routing_result.get('inferred_filters', {})
                        session.last_successful_timestamp = start_time
                        logger.info(f"üîÑ Set session context for follow-up (medium): {target_collection}")
                    
                    return self._generate_smart_clarification(routing_result, query, session_id, start_time)
                    
                else:
                    # LOW CONFIDENCE - H·ªèi l·∫°i user, kh√¥ng route
                    logger.info(f"ü§î LOW CONFIDENCE ({confidence_level}) - h·ªèi l·∫°i user thay v√¨ route")
                    
                    # üîß FIX: Set session context n·∫øu c√≥ target collection (low confidence v·∫´n c√≥ th·ªÉ c√≥ best guess)
                    if session and routing_result.get('target_collection'):
                        target_collection = routing_result.get('target_collection')
                        session.last_successful_collection = target_collection
                        session.last_successful_filters = routing_result.get('inferred_filters', {})
                        session.last_successful_timestamp = start_time
                        logger.info(f"üîÑ Set session context for follow-up (low): {target_collection}")
                    
                    return self._generate_smart_clarification(routing_result, query, session_id, start_time)
            
            # Check for preserved document from session override
            preserved_document = None
            if confidence_level == 'override_high' and routing_result.get('inferred_filters') and 'source_file' in routing_result['inferred_filters']:
                preserved_document = routing_result['inferred_filters']['source_file']
                logger.info(f"‚ö° FULL CONTEXT PRESERVATION: Using document {preserved_document} directly from session")
            
            # If we have a preserved document, skip search and go directly to context expansion
            if preserved_document:
                # Create nucleus chunk directly pointing to preserved document
                nucleus_chunks = [{
                    'content': f"Preserved document from previous question: {preserved_document}",
                    'collection': target_collection,
                    'document_title': preserved_document,
                    'source_file': preserved_document,
                    'rerank_score': 1.0,  # High score since we're certain
                    'similarity': 1.0
                }]
                
                # Construct the full path for the source
                full_source_path = f"data/storage/collections/{target_collection}/documents/{preserved_document}/{preserved_document.replace(' ', '_')}.json"  # Adjust based on your naming convention
                nucleus_chunks[0]['source'] = full_source_path  # Add the 'source' key with full path
                
                # Skip to context expansion
                logger.info(f"üîí SESSION CONTINUITY: Skipping vector search and reranking for preserved document")
                expanded_context = self.context_expansion_service.expand_context_with_nucleus(
                    nucleus_chunks=nucleus_chunks
                )
                
                # Skip ahead to context building
                context_text = self._build_context_from_expanded(expanded_context, nucleus_chunks)
                
                # ‚úÖ ENHANCED: Smart context building v·ªõi intent detection
                detected_intent = self._detect_specific_intent(query)
                if detected_intent and expanded_context.get('structured_metadata'):
                    context_text = self._build_smart_context(
                        intent=detected_intent,
                        metadata=expanded_context['structured_metadata'],
                        full_text=context_text
                    )
                
                logger.info(f"Context expanded: {expanded_context['total_length']} chars from {len(expanded_context.get('source_documents', []))} documents")
                if detected_intent:
                    logger.info(f"üéØ Detected intent: {detected_intent} - Applied smart context building")
                
                # Jump to LLM generation
                logger.info("üîÑ PHASE 2: LLM Generation (GPU) - Loading LLM for final answer...")
                
                # Skip all the search and reranking logic, move straight to answer generation
                answer = self._generate_answer_with_context(
                    query=query,
                    context=context_text,
                    session=session
                )
                
                # Update session history
                session.query_history.append({
                    "query": query,
                    "answer": answer,
                    "timestamp": time.time(),
                    "nucleus_chunks_count": len(nucleus_chunks),
                    "context_length": len(context_text),
                    "from_preserved_document": True
                })
                
                # Keep only last 5 queries in session
                if len(session.query_history) > 5:
                    session.query_history = session.query_history[-5:]
                
                # Update session state for next query
                session.update_successful_routing(
                    collection=target_collection, 
                    confidence=routing_result.get('confidence', 0.85),
                    filters={"source_file": preserved_document},
                    rag_content={
                        "context_text": context_text,
                        "nucleus_chunks": nucleus_chunks,
                        "expanded_context": expanded_context,
                        "collections": [target_collection]
                    }
                )
                logger.info(f"üî• Reinforced session state with preserved document: {preserved_document}")
                
                processing_time = time.time() - start_time
                
                # Check for forms
                forms_info = self.check_document_forms([preserved_document])
                
                # Return final response with preserved document
                return {
                    "type": "answer",
                    "answer": answer,
                    "context_info": {
                        "nucleus_chunks": 1,
                        "context_length": len(context_text),
                        "source_collections": [target_collection],
                        "source_documents": [preserved_document],
                        "from_preserved_document": True
                    },
                    "session_id": session_id,
                    "processing_time": processing_time,
                    "routing_info": {
                        "best_collections": [target_collection],
                        "target_collection": target_collection,
                        "confidence": float(routing_result.get('confidence', 0.85)),
                        "confidence_level": "preserved_document",
                        "preserved_document": preserved_document
                    }
                }
                
            else:
                # Step 2: Focused Search v·ªõi DYNAMIC BROAD_SEARCH_K d·ª±a tr√™n router confidence  
                # üöÄ PERFORMANCE OPTIMIZATION: Ch·ªâ optimize cho HIGH confidence v√¨ MEDIUM ƒë√£ trigger clarification
                dynamic_k = settings.broad_search_k  # default 12
                if confidence_level in ['high', 'high_followup']:
                    dynamic_k = max(5, settings.broad_search_k - 6)  # Aggressive: 12-6=6, max(5,6)=6
                    logger.info(f"üéØ HIGH CONFIDENCE: Aggressive reduction to {dynamic_k} docs")
                else:
                    logger.info(f"ÔøΩ HIGH CONFIDENCE ONLY: S·ª≠ d·ª•ng broad_search_k={dynamic_k}")
                
                broad_search_results = []
                for collection_name in best_collections[:2]:  # Limit to top 2 collections
                    try:
                        # ‚úÖ CRITICAL FIX: Pass smart filters to vector search v·ªõi dynamic K
                        # üîç DEBUG: Log filter tr∆∞·ªõc khi t√¨m ki·∫øm ƒë·ªÉ debug v·∫•n ƒë·ªÅ filter b·ªã "ƒë√°nh r∆°i"
                        logger.info(f"üîç Chu·∫©n b·ªã t√¨m ki·∫øm v·ªõi filter: {inferred_filters}")
                        
                        # üî• ADAPTIVE THRESHOLD: H·∫° threshold khi c√≥ filter ho·∫∑c session override
                        adaptive_threshold = settings.similarity_threshold
                        if inferred_filters:
                            adaptive_threshold = max(0.2, settings.similarity_threshold * 0.5)  # H·∫° threshold khi c√≥ filter
                            logger.info(f"üéØ ADAPTIVE THRESHOLD: {settings.similarity_threshold} -> {adaptive_threshold} (c√≥ filter)")
                        elif was_overridden:
                            # üî• SESSION OVERRIDE: H·∫° threshold ƒë·ªÉ ƒë·∫£m b·∫£o t√¨m ƒë∆∞·ª£c context trong session collection
                            adaptive_threshold = max(0.15, settings.similarity_threshold * 0.4)  # H·∫° threshold m·∫°nh cho session override
                            logger.info(f"üéØ SESSION OVERRIDE THRESHOLD: {settings.similarity_threshold} -> {adaptive_threshold} (session override)")
                        else:
                            logger.info(f"üìä STANDARD THRESHOLD: {adaptive_threshold} (kh√¥ng c√≥ filter)")
                        
                        results = self.vectordb_service.search_in_collection(
                            collection_name=collection_name,
                            query=query,
                            top_k=dynamic_k,
                            similarity_threshold=adaptive_threshold,
                            where_filter=inferred_filters if inferred_filters else None
                        )
                        
                        # Process results and add to broad search results
                        for result in results:
                            result["collection"] = collection_name
                        
                        broad_search_results.extend(results)
                        
                    except Exception as e:
                        logger.warning(f"Error searching in collection {collection_name}: {e}")
            
            logger.info(f"üìä Dynamic search: {len(broad_search_results)} docs (k={dynamic_k}, confidence={confidence_level})")
            
            if not broad_search_results:
                return {
                    "type": "no_results",
                    "message": "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n.",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            logger.info(f"Found {len(broad_search_results)} candidate chunks")
            
            # Step 4: SEQUENTIAL PROCESSING ƒë·ªÉ t·ªëi ∆∞u VRAM (6GB limit)
            # Phase 1: Reranking - Load Reranker, Unload LLM n·∫øu c·∫ßn
            logger.info("üîÑ PHASE 1: Reranking (GPU) - Optimizing VRAM usage...")
            
            # Temporarily unload LLM ƒë·ªÉ ƒë·∫£m b·∫£o VRAM cho reranker
            if hasattr(self.llm_service, 'unload_model'):
                self.llm_service.unload_model()
            
            if settings.use_reranker and len(broad_search_results) > 1:
                # ‚úÖ ENHANCED RERANKING: Consensus-based document selection for better accuracy
                docs_to_rerank = broad_search_results  # RERANK ALL DOCUMENTS
                logger.info(f"üéØ ENHANCED RERANKING - Analyzing {len(broad_search_results)} candidates for consensus")
                
                if len(broad_search_results) >= 5:
                    # ‚úÖ NEW METHOD: Consensus-based document selection (more robust)
                    consensus_document = self.reranker_service.get_consensus_document(
                        query=query,
                        documents=docs_to_rerank,
                        top_k=5,  # Analyze top 5 candidates
                        consensus_threshold=0.6,  # 3/5 = 60%
                        min_rerank_score=-0.5,  # Adjusted for legal documents
                        router_confidence=routing_result.get('confidence', 0.0),
                        router_confidence_level=routing_result.get('confidence_level', 'low'),
                        router_selected_document=routing_result.get('best_match', {}).get('document')
                    )
                    
                    if consensus_document:
                        nucleus_chunks = [consensus_document]
                        logger.info(f"‚úÖ CONSENSUS FOUND: Selected document based on chunk agreement")
                    else:
                        # Fallback to traditional single best document
                        logger.warning("‚ùå NO CONSENSUS: Falling back to traditional single best document")
                        nucleus_chunks = self.reranker_service.rerank_documents(
                            query=query,
                            documents=docs_to_rerank,
                            top_k=1,
                            router_confidence=routing_result.get('confidence', 0.0),
                            router_confidence_level=routing_result.get('confidence_level', 'low')
                        )
                else:
                    # Not enough candidates for consensus analysis
                    logger.info(f"INSUFFICIENT CANDIDATES ({len(broad_search_results)}) - Using traditional reranking")
                    nucleus_chunks = self.reranker_service.rerank_documents(
                        query=query,
                        documents=docs_to_rerank,
                        top_k=1,  # CH·ªà 1 nucleus chunk cao nh·∫•t - s·∫Ω expand to√†n b·ªô document ch·ª©a chunk n√†y
                        router_confidence=routing_result.get('confidence', 0.0),
                        router_confidence_level=routing_result.get('confidence_level', 'low')
                    )
                
                # Unload reranker sau khi ho√†n th√†nh ƒë·ªÉ gi·∫£i ph√≥ng VRAM
                if hasattr(self.reranker_service, 'unload_model'):
                    self.reranker_service.unload_model()
                
                # üö® INTELLIGENT CONFIDENCE CHECK - Ki·ªÉm tra COMBINED confidence tr∆∞·ªõc khi g·ªçi LLM
                router_confidence = routing_result.get('confidence', 0.0)
                best_score = nucleus_chunks[0].get('rerank_score', 0) if nucleus_chunks and len(nucleus_chunks) > 0 else 0.0
                
                # Calculate combined confidence score
                combined_confidence = (router_confidence * 0.4 + best_score * 0.6)  # Reranker c√≥ tr·ªçng s·ªë cao h∆°n
                logger.info(f"üéØ Combined Confidence: {combined_confidence:.4f} (Router: {router_confidence:.4f}, Rerank: {best_score:.4f})")
                
                # SMART CLARIFICATION THRESHOLD - Tr√°nh c√¢u tr·∫£ l·ªùi sai l·ªách
                CLARIFICATION_THRESHOLD = 0.3  # ƒêi·ªÅu ch·ªânh threshold n√†y theo c·∫ßn thi·∫øt
                
                if combined_confidence < CLARIFICATION_THRESHOLD:
                    logger.warning(f"üö® COMBINED CONFIDENCE QU√Å TH·∫§P ({combined_confidence:.4f} < {CLARIFICATION_THRESHOLD}) - K√≠ch ho·∫°t Smart Clarification")
                    
                    return self._generate_smart_clarification(routing_result, query, session_id, start_time)
                
                if nucleus_chunks and len(nucleus_chunks) > 0:
                    logger.info(f"Best rerank score: {best_score:.4f}")
                    logger.info("üéØ PURE RERANKER MODE - No protective logic, full expansion strategy")
            
                logger.info(f"Selected {len(nucleus_chunks)} nucleus chunk with rerank-based strategy")
            else:
                nucleus_chunks = broad_search_results[:1]  # Fallback: l·∫•y chunk t·ªët nh·∫•t theo vector similarity
                
            # Step 5: INTELLIGENT Context Expansion - ∆Øu ti√™n nucleus chunk + context li√™n quan
            expanded_context = None
            logger.info("üéØ INTELLIGENT CONTEXT EXPANSION - ∆Øu ti√™n nucleus chunk t·ª´ reranker")
            self.metrics["context_expansions"] += 1
            
            # üß† SMART OPTIMIZATION: ∆Øu ti√™n nucleus chunk + context li√™n quan thay v√¨ c·∫Øt ng·∫´u nhi√™n
            # Logic: Lu√¥n gi·ªØ nguy√™n nucleus chunk + th√™m context xung quanh n·∫øu c√≤n ch·ªó
            # Step 5: Context Expansion - THI·∫æT K·∫æ G·ªêC: FULL DOCUMENT
            logger.info("Context expansion: Loading TO√ÄN B·ªò DOCUMENT ƒë·ªÉ ƒë·∫£m b·∫£o ng·ªØ c·∫£nh ph√°p lu·∫≠t ƒë·∫ßy ƒë·ªß")
            
            expanded_context = self.context_expansion_service.expand_context_with_nucleus(
                nucleus_chunks=nucleus_chunks
            )
            
            # üéØ PHASE 1: Apply highlighting cho nucleus chunks
            context_text = self._build_context_from_expanded(expanded_context, nucleus_chunks)
            
            # ‚úÖ ENHANCED: Smart context building v·ªõi intent detection
            detected_intent = self._detect_specific_intent(query)
            if detected_intent and expanded_context.get('structured_metadata'):
                context_text = self._build_smart_context(
                    intent=detected_intent,
                    metadata=expanded_context['structured_metadata'],
                    full_text=context_text
                )
            
            logger.info(f"Context expanded: {expanded_context['total_length']} chars from {len(expanded_context.get('source_documents', []))} documents")
            if detected_intent:
                logger.info(f"üéØ Detected intent: {detected_intent} - Applied smart context building")
            
            # Phase 2: LLM Generation - Load LLM cho generation phase
            logger.info("üîÑ PHASE 2: LLM Generation (GPU) - Loading LLM for final answer...")
            
            # Step 6: Generate Answer (GPU LLM)
            if not session:
                return {
                    "type": "error",
                    "error": "Session not found",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            answer = self._generate_answer_with_context(
                query=query,
                context=context_text,
                session=session
            )
            
            # Update session history
            session.query_history.append({
                "query": query,
                "answer": answer,
                "timestamp": time.time(),
                "nucleus_chunks_count": len(nucleus_chunks),
                "context_length": len(context_text)
            })
            
            # Keep only last 5 queries in session (gi·∫£m t·ª´ 10 ƒë·ªÉ ti·∫øt ki·ªám memory)
            if len(session.query_history) > 5:
                session.query_history = session.query_history[-5:]
            
            # üî• Update session state for Stateful Router
            # Ch·ªâ update state khi routing th√†nh c√¥ng v·ªõi confidence ƒë·ªß t·ªët (0.78+)
            logger.info(f"üîç Session update check: routing_result={routing_result is not None}, confidence={routing_result.get('confidence', 0) if routing_result else 'None'}")
            if routing_result and routing_result.get('confidence', 0) >= 0.78:
                target_collection = routing_result.get('target_collection')
                logger.info(f"üîç Target collection for session update: {target_collection}")
                if target_collection:
                    rag_content = {
                        "context_text": context_text,
                        "nucleus_chunks": nucleus_chunks,
                        "expanded_context": expanded_context,
                        "collections": best_collections
                    }
                    
                    # üîß FIX: Also preserve document information from successful queries
                    enhanced_filters = routing_result.get('inferred_filters', {}).copy()
                    if expanded_context and expanded_context.get('source_documents'):
                        # Get the first/main document name
                        source_docs = expanded_context['source_documents']
                        if source_docs:
                            main_doc = source_docs[0] if isinstance(source_docs, list) else str(source_docs)
                            # Extract document title from path
                            if isinstance(main_doc, str) and main_doc:
                                doc_name = main_doc.split('\\')[-1].replace('.json', '') if '\\' in main_doc else main_doc
                                enhanced_filters["source_file"] = doc_name
                                # Also store in session metadata for persistence
                                session.metadata["current_document"] = doc_name
                    
                    session.update_successful_routing(
                        collection=target_collection, 
                        confidence=routing_result.get('confidence', 0),
                        filters=enhanced_filters,  # üîß Enhanced filters with document info
                        rag_content=rag_content
                    )
                    logger.info(f"üî• Updated session state: {target_collection} (confidence: {routing_result.get('confidence', 0):.3f})")
                
            processing_time = time.time() - start_time
            self.metrics["avg_response_time"] = (
                (self.metrics["avg_response_time"] * (self.metrics["total_queries"] - 1) + processing_time) 
                / self.metrics["total_queries"]
            )
            
            # üìé CHECK FOR FORMS: If documents have forms, include them in response
            source_documents = expanded_context.get("source_documents", []) if expanded_context else []
            forms_info = self.check_document_forms(source_documents)
            
            response = {
                "type": "answer",
                "answer": answer,
                "context_info": {
                    "nucleus_chunks": len(nucleus_chunks),
                    "context_length": len(context_text),
                    "source_collections": list(set(chunk.get("collection", "") for chunk in nucleus_chunks)),
                    "source_documents": source_documents
                },
                "context_details": {
                    "total_length": expanded_context.get("total_length", len(context_text)) if expanded_context else len(context_text),
                    "expansion_strategy": expanded_context.get("expansion_strategy", "unknown") if expanded_context else "no_expansion",
                    "source_documents": source_documents,
                    "nucleus_chunks_count": len(nucleus_chunks)
                },
                "session_id": session_id,
                "processing_time": processing_time,
                "routing_info": {
                    "best_collections": best_collections,
                    "target_collection": routing_result.get('target_collection'),
                    "confidence": float(routing_result.get('confidence', 0.0)),
                    "original_confidence": float(routing_result.get('original_confidence', 0.0)) if routing_result.get('original_confidence') is not None else None,
                    "was_overridden": routing_result.get('was_overridden', False),
                    "inferred_filters": routing_result.get('inferred_filters', {}),
                    "confidence_level": routing_result.get('confidence_level', 'unknown'),
                    "status": routing_result.get('status', 'routed')
                }
            }
            
            # üìé ENHANCED FORM DETECTION & ATTACHMENT: Integrate with FormDetectionService
            try:
                # Use FormDetectionService to detect and attach forms
                response = self.form_detection_service.enhance_rag_response_with_forms(response)
                
                # Update answer with form references if forms found
                form_attachments = response.get("form_attachments", [])
                if form_attachments:
                    # Check if answer doesn't already mention forms
                    if "form" not in response["answer"].lower() and "m·∫´u" not in response["answer"].lower():
                        # Add form reference to answer
                        form_text = "\n\nüìã **Bi·ªÉu m·∫´u/t·ªù khai li√™n quan:**\n"
                        for form in form_attachments:
                            form_text += f"- {form['document_title']}: Xem bi·ªÉu m·∫´u ƒë√≠nh k√®m\n"
                        response["answer"] += form_text
                    
                    logger.info(f"üìé Enhanced response with {len(form_attachments)} form attachments")
                    
            except Exception as e:
                logger.error(f"Error in form detection: {e}")
                # Continue without forms if error occurs
            
            return response
            
        except Exception as e:
            logger.error(f"Error in enhanced query: {e}")
            return {
                "type": "error",
                "error": str(e),
                "session_id": session_id,
                "processing_time": time.time() - start_time
            }
            
    def handle_clarification(
        self,
        session_id: str,
        selected_option: Dict[str, Any],  # üîß CHANGE: Nh·∫≠n full option object thay v√¨ string
        original_query: str
    ) -> Dict[str, Any]:
        """
        MULTI-TURN CONVERSATION: X·ª≠ l√Ω ph·∫£n h·ªìi clarification v·ªõi nhi·ªÅu giai ƒëo·∫°n
        - Giai ƒëo·∫°n 2: proceed_with_collection ‚Üí Generate document suggestions  
        - Giai ƒëo·∫°n 2.5: proceed_with_document ‚Üí Generate question suggestions within document
        - Giai ƒëo·∫°n 3: proceed_with_question ‚Üí Run RAG with clarified query
        """
        start_time = time.time()  # üîß ADD: Track processing time
        session = self.get_session(session_id)
        if not session:
            return {
                "type": "error", 
                "error": f"Session {session_id} not found",
                "session_id": session_id,
                "processing_time": 0.0
            }
            
        action = selected_option.get('action')
        collection = selected_option.get('collection')
        
        if action == 'proceed_with_collection' and collection:
            # üéØ GIAI ƒêO·∫†N 2: User ch·ªçn collection, hi·ªÉn th·ªã documents ƒë·ªÉ ch·ªçn
            logger.info(f"üéØ Clarification Step 2: User selected collection '{collection}'. Showing documents.")
            
            try:
                # L·∫•y danh s√°ch documents trong collection n√†y t·ª´ smart_router
                collection_questions = self.smart_router.get_example_questions_for_collection(collection)
                
                # Extract unique documents from questions
                collection_documents = {}
                for question in collection_questions:
                    source = question.get('source', '')
                    if source:
                        # Clean up source path to get document name
                        doc_name = source.replace('.json', '').split('/')[-1]
                        if '. ' in doc_name:
                            doc_name = doc_name.split('. ', 1)[1]  # Remove numbering
                        
                        if doc_name not in collection_documents:
                            collection_documents[doc_name] = {
                                "filename": source,
                                "title": doc_name,
                                "description": f"T√†i li·ªáu v·ªÅ {doc_name}",
                                "question_count": 0
                            }
                        collection_documents[doc_name]["question_count"] += 1
                
                if not collection_documents:
                    logger.warning(f"‚ö†Ô∏è No documents found in collection '{collection}'")
                    return {
                        "answer": f"Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o trong '{collection}'. Vui l√≤ng th·ª≠ l·∫°i.",
                        "type": "error",
                        "session_id": session_id,
                        "processing_time": time.time() - start_time
                    }
                
                # Convert to list and limit to top 8 documents
                document_list = list(collection_documents.values())
                document_list = sorted(document_list, key=lambda x: x["question_count"], reverse=True)[:8]
                
                # T·∫°o suggestions cho document selection
                document_suggestions = []
                for i, doc in enumerate(document_list):
                    document_suggestions.append({
                        "id": str(i + 1),
                        "title": doc['title'],
                        "description": f"T√†i li·ªáu: {doc['title']} ({doc['question_count']} c√¢u h·ªèi)",
                        "action": "proceed_with_document",
                        "collection": collection,
                        "document_filename": doc['filename'],
                        "document_title": doc['title']
                    })
                
                # T·∫°o clarification response cho document selection
                clarification_response = {
                    "message": f"B·∫°n ƒë√£ ch·ªçn '{collection}'. H√£y ch·ªçn t√†i li·ªáu c·ª• th·ªÉ:",
                    "options": document_suggestions,
                    "show_manual_input": True,
                    "manual_input_placeholder": "Ho·∫∑c nh·∫≠p c√¢u h·ªèi c·ª• th·ªÉ c·ªßa b·∫°n...",
                    "context": "document_selection",
                    "metadata": {
                        "collection": collection,
                        "available_documents": document_list,
                        "stage": "document_selection"
                    }
                }
                
                # Update session state
                session.metadata["routing_state"] = {
                    "collection": collection,
                    "available_documents": document_list,
                    "stage": "document_selection"
                }
                self.chat_sessions[session_id] = session
                
                return {
                    "answer": clarification_response["message"],
                    "clarification": clarification_response,
                    "collection": collection,
                    "documents": document_list,
                    "type": "clarification_needed",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            except Exception as e:
                logger.error(f"‚ùå Error in document selection: {e}")
                return {
                    "answer": f"C√≥ l·ªói khi t·∫£i danh s√°ch t√†i li·ªáu trong '{collection}'. Vui l√≤ng th·ª≠ l·∫°i.",
                    "type": "error",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
        
        if action == 'show_document_questions' and collection:
            # üéØ MEDIUM-HIGH CONFIDENCE: Show questions for specific procedure directly
            procedure = selected_option.get('procedure')
            document_title = selected_option.get('document_title') or procedure
            
            logger.info(f"üéØ Medium-High Confidence: Showing questions for procedure '{procedure}' in collection '{collection}'")
            
            try:
                # Get questions that match the procedure
                collection_questions = self.smart_router.get_example_questions_for_collection(collection)
                
                # Filter questions by procedure/document title
                matching_questions = []
                for question in collection_questions:
                    question_text = question.get('text', str(question)) if isinstance(question, dict) else str(question)
                    source = question.get('source', '') if isinstance(question, dict) else ''
                    
                    # Check if question is related to the procedure
                    if procedure and (procedure.lower() in question_text.lower() or 
                                    procedure.lower() in source.lower()):
                        matching_questions.append(question)
                
                # If no specific matches, get top questions from collection
                if not matching_questions:
                    matching_questions = collection_questions[:5]
                
                # Create question suggestions
                suggestions = []
                for i, q in enumerate(matching_questions[:5]):
                    question_text = q.get('text', str(q)) if isinstance(q, dict) else str(q)
                    suggestions.append({
                        "id": str(i + 1),
                        "title": question_text,
                        "description": f"C√¢u h·ªèi v·ªÅ {procedure}",
                        "action": "proceed_with_question",
                        "collection": collection,
                        "document_title": document_title,
                        "question_text": question_text,
                        "source_file": q.get('source', '') if isinstance(q, dict) else '',
                        "category": q.get('category', 'general') if isinstance(q, dict) else 'general'
                    })
                
                # Add manual input option
                suggestions.append({
                    "id": str(len(suggestions) + 1),
                    "title": "C√¢u h·ªèi kh√°c...",
                    "description": f"T√¥i mu·ªën h·ªèi v·ªÅ v·∫•n ƒë·ªÅ kh√°c trong {procedure}",
                    "action": "manual_input",
                    "collection": collection,
                    "document_title": document_title
                })
                
                clarification_response = {
                    "message": f"ƒê√¢y l√† c√°c c√¢u h·ªèi th∆∞·ªùng g·∫∑p v·ªÅ '{procedure}'. H√£y ch·ªçn c√¢u h·ªèi ph√π h·ª£p:",
                    "options": suggestions,
                    "show_manual_input": True,
                    "manual_input_placeholder": f"Ho·∫∑c nh·∫≠p c√¢u h·ªèi c·ª• th·ªÉ v·ªÅ {procedure}...",
                    "context": "medium_high_questions",
                    "metadata": {
                        "collection": collection,
                        "procedure": procedure,
                        "document_title": document_title,
                        "stage": "medium_high_questions"
                    }
                }
                
                # Update session state
                session.metadata["routing_state"] = {
                    "collection": collection,
                    "procedure": procedure,
                    "document_title": document_title,
                    "stage": "medium_high_questions"
                }
                self.chat_sessions[session_id] = session
                
                return {
                    "answer": clarification_response["message"],
                    "clarification": clarification_response,
                    "collection": collection,
                    "document_title": document_title,
                    "type": "clarification_needed",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            except Exception as e:
                logger.error(f"‚ùå Error in medium-high question generation: {e}")
                import traceback
                traceback.print_exc()
                return {
                    "answer": f"C√≥ l·ªói khi t·∫£i c√¢u h·ªèi v·ªÅ '{procedure}'. Vui l√≤ng th·ª≠ l·∫°i.",
                    "type": "error",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
        
        if action == 'proceed_with_document' and collection:
            # üéØ GIAI ƒêO·∫†N 2.5: User ch·ªçn document, generate question suggestions trong document ƒë√≥
            document_filename = selected_option.get('document_filename')
            document_title = selected_option.get('document_title')
            
            logger.info(f"üéØ Clarification Step 2.5: User selected document '{document_title}' in collection '{collection}'. Generating question suggestions.")
            
            try:
                # L·∫•y t·∫•t c·∫£ questions trong collection v√† filter theo document
                collection_questions = self.smart_router.get_example_questions_for_collection(collection)
                
                # Filter questions by document source
                document_questions = []
                for question in collection_questions:
                    if question.get('source') and document_filename in question.get('source', ''):
                        document_questions.append(question)
                
                if not document_questions:
                    logger.warning(f"‚ö†Ô∏è No questions found for document {document_title}")
                    # Fallback: Use all collection questions
                    document_questions = collection_questions[:5]
                
                # Create suggestions from document questions
                suggestions = []
                for i, q in enumerate(document_questions[:5]):
                    question_text = q.get('text', str(q)) if isinstance(q, dict) else str(q)
                    suggestions.append({
                        "id": str(i + 1),
                        "title": question_text,
                        "description": f"C√¢u h·ªèi v·ªÅ {document_title}",
                        "action": "proceed_with_question",
                        "collection": collection,
                        "document_filename": document_filename,
                        "document_title": document_title,
                        "question_text": question_text,
                        "source_file": q.get('source', '') if isinstance(q, dict) else '',
                        "category": q.get('category', 'general') if isinstance(q, dict) else 'general'
                    })
                
                # Add manual input option
                suggestions.append({
                    "id": str(len(suggestions) + 1),
                    "title": "C√¢u h·ªèi kh√°c...",
                    "description": f"T√¥i mu·ªën h·ªèi v·ªÅ v·∫•n ƒë·ªÅ kh√°c trong {document_title}",
                    "action": "manual_input",
                    "collection": collection,
                    "document_filename": document_filename,
                    "document_title": document_title
                })
                
                collection_display = self.smart_router.collection_mappings.get(collection, {}).get('display_name', collection.replace('_', ' ').title())
                
                clarification_response = {
                    "message": f"B·∫°n ƒë√£ ch·ªçn t√†i li·ªáu '{document_title}'. H√£y ch·ªçn c√¢u h·ªèi ph√π h·ª£p:",
                    "options": suggestions,
                    "show_manual_input": True,
                    "manual_input_placeholder": f"Ho·∫∑c nh·∫≠p c√¢u h·ªèi c·ª• th·ªÉ v·ªÅ {document_title}...",
                    "context": "question_selection",
                    "metadata": {
                        "collection": collection,
                        "document_filename": document_filename,
                        "document_title": document_title,
                        "stage": "question_selection"
                    }
                }
                
                # Update session state
                session.metadata["routing_state"] = {
                    "collection": collection,
                    "document_filename": document_filename,
                    "document_title": document_title,
                    "stage": "question_selection"
                }
                self.chat_sessions[session_id] = session
                
                return {
                    "answer": clarification_response["message"],
                    "clarification": clarification_response,
                    "collection": collection,
                    "document_title": document_title,
                    "type": "clarification_needed",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            except Exception as e:
                logger.error(f"‚ùå Error in document question generation: {e}")
                return {
                    "answer": f"C√≥ l·ªói khi t·∫£i c√¢u h·ªèi cho '{document_title}'. Vui l√≤ng th·ª≠ l·∫°i.",
                    "type": "error",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
        elif action == 'proceed_with_question':
            # üéØ GIAI ƒêO·∫†N 3 ‚Üí 4: User ch·ªçn c√¢u h·ªèi c·ª• th·ªÉ, ch·∫°y RAG
            question_text = selected_option.get('question_text')
            document_title = selected_option.get('document_title')  # üî• NEW: Get exact document title
            source_file = selected_option.get('source_file')  # üî• NEW: For debugging
            
            if question_text and collection:
                logger.info(f"üöÄ Clarification Step 3‚Üí4: User selected question '{question_text}' in collection '{collection}'.")
                if document_title:
                    logger.info(f"üéØ Target document: '{document_title}' (source: {source_file})")
                
                # Ch·∫°y RAG v·ªõi c√¢u h·ªèi ƒê√É ƒê∆Ø·ª¢C L√ÄM R√ï v√† collection ƒê√É CH·ªà ƒê·ªäNH
                return self.process_query(
                    query=question_text,  # üî• D√πng c√¢u h·ªèi c·ª• th·ªÉ, kh√¥ng ph·∫£i original query m∆° h·ªì
                    session_id=session_id,
                    forced_collection=collection,  # üî• Force routing to selected collection
                    forced_document_title=document_title  # üî• NEW: Force exact document filtering
                )
            else:
                return {
                    "type": "error",
                    "error": "Missing question_text or collection for proceed_with_question action",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
            
        elif action == 'manual_input':
            # üîß IMPROVED: Manual input v·ªõi context preservation
            logger.info(f"üîÑ Manual input requested by user. Preserving valuable context.")
            
            # ‚úÖ SMART CONTEXT PRESERVATION: Gi·ªØ context c√≥ gi√° tr·ªã thay v√¨ clear all
            original_routing = session.metadata.get('original_routing_context', {})
            
            # üîß FIX: Get collection from original routing context, not from selected_option
            selected_collection = (selected_option.get('collection') or 
                                 original_routing.get('target_collection'))
            selected_document = selected_option.get('document_filename')  # Document user ƒë√£ ch·ªçn (if any)
            document_title = selected_option.get('document_title')  # Document title (if any)
            
            logger.info(f"üîç Context check: selected_collection={selected_collection}, original_target={original_routing.get('target_collection')}")
            
            # Determine context to preserve based on conversation stage
            if selected_document and selected_collection:
                # Case 2: User ƒë√£ ch·ªçn document ‚Üí Preserve document-level context
                logger.info(f"üîÑ CASE 2: Preserving document context: {document_title} in {selected_collection}")
                session.last_successful_collection = selected_collection
                session.last_successful_confidence = original_routing.get('confidence', 0.7)
                session.last_successful_timestamp = time.time()
                session.last_successful_filters = original_routing.get('inferred_filters', {})
                
                # Also preserve document-specific context
                session.metadata['preserved_document'] = {
                    'filename': selected_document,
                    'title': document_title,
                    'collection': selected_collection
                }
                
                # Clear only metadata v·ªÅ clarification process
                session.metadata.pop('original_routing_context', None)
                session.metadata.pop('original_query', None)
                
                return {
                    "type": "manual_input_request",
                    "message": f"Vui l√≤ng nh·∫≠p c√¢u h·ªèi c·ª• th·ªÉ v·ªÅ '{document_title}'. T√¥i s·∫Ω t√¨m ki·∫øm trong t√†i li·ªáu n√†y.",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time,
                    "context_preserved": True,
                    "preserved_collection": selected_collection,
                    "preserved_document": document_title
                }
                
            elif selected_collection:
                # Case 1: User ƒë√£ ch·ªçn collection ‚Üí Preserve collection context
                logger.info(f"üîÑ CASE 1: Preserving collection context: {selected_collection}")
                session.last_successful_collection = selected_collection
                session.last_successful_confidence = original_routing.get('confidence', 0.7)
                session.last_successful_timestamp = time.time()
                session.last_successful_filters = original_routing.get('inferred_filters', {})
                
                # Clear only metadata v·ªÅ clarification process
                session.metadata.pop('original_routing_context', None)
                session.metadata.pop('original_query', None)
                
                return {
                    "type": "manual_input_request",
                    "message": f"Vui l√≤ng nh·∫≠p l·∫°i c√¢u h·ªèi c·ª• th·ªÉ h∆°n v·ªÅ '{selected_collection}'. T√¥i s·∫Ω t√¨m ki·∫øm trong lƒ©nh v·ª±c n√†y.",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time,
                    "context_preserved": True,
                    "preserved_collection": selected_collection
                }
            else:
                # üîß FIX: Check if we have ANY collection context to preserve
                if selected_collection:
                    logger.info(f"üîÑ FOUND COLLECTION CONTEXT: Preserving collection context: {selected_collection}")
                    session.last_successful_collection = selected_collection
                    session.last_successful_confidence = original_routing.get('confidence', 0.7)
                    session.last_successful_timestamp = time.time()
                    session.last_successful_filters = original_routing.get('inferred_filters', {})
                    
                    # Clear only metadata v·ªÅ clarification process
                    session.metadata.pop('original_routing_context', None)
                    session.metadata.pop('original_query', None)
                    
                    # üî• NEW: Set manual input context for next query
                    session.metadata['manual_input_context'] = {
                        'collection': selected_collection,
                        'bypass_router': True,
                        'preserve_collection': True
                    }
                    
                    return {
                        "type": "manual_input_request",
                        "message": f"Vui l√≤ng nh·∫≠p l·∫°i c√¢u h·ªèi c·ª• th·ªÉ h∆°n v·ªÅ '{selected_collection}'. T√¥i s·∫Ω t√¨m ki·∫øm trong lƒ©nh v·ª±c n√†y.",
                        "session_id": session_id,
                        "processing_time": time.time() - start_time,
                        "context_preserved": True,
                        "preserved_collection": selected_collection
                    }
                else:
                    # Kh√¥ng c√≥ collection context ‚Üí Clear session (fallback)
                    logger.info(f"üîÑ No collection context to preserve, clearing session state.")
                    session.clear_routing_state()
                    session.metadata.clear()
                    
                    return {
                        "type": "manual_input_request",
                        "message": "Vui l√≤ng nh·∫≠p l·∫°i c√¢u h·ªèi c·ª• th·ªÉ h∆°n. T√¥i s·∫Ω t√¨m ki·∫øm trong ng·ªØ c·∫£nh ph√π h·ª£p.",
                        "session_id": session_id,
                        "processing_time": time.time() - start_time,
                        "context_preserved": False
                    }
            
            # ‚úÖ Update session access time  
            session.last_accessed = time.time()
            
        else:
            # Invalid action
            return {
                "type": "error",
                "error": f"Invalid clarification action: {action}",
                "session_id": session_id, 
                "processing_time": 0.0
            }
        
    def _build_context_from_expanded(self, expanded_context: Dict[str, Any], nucleus_chunks: Optional[List[Dict]] = None) -> str:
        """
        üéØ PHASE 1: Build context v·ªõi highlighting cho nucleus chunks
        üßπ PHASE 3: Clean formatting - b·ªè decorative symbols
        """
        context_parts = []
        
        for doc_content in expanded_context.get("expanded_content", []):
            source = doc_content.get("source", "N/A")
            text = doc_content.get("text", "")
            chunk_count = doc_content.get("chunk_count", 0)
            
            # Apply highlighting cho nucleus chunk n·∫øu c√≥
            if nucleus_chunks and self.context_expansion_service:
                nucleus_chunk = nucleus_chunks[0]  # L·∫•y nucleus chunk ƒë·∫ßu ti√™n
                
                # üîç DEBUG: Log nucleus chunk structure
                logger.info(f"üîç Nucleus chunk keys: {list(nucleus_chunk.keys())}")
                logger.info(f"üîç Nucleus chunk content preview: {nucleus_chunk.get('content', 'NO_CONTENT')[:100]}...")
                
                highlighted_text = self.context_expansion_service._build_highlighted_context(
                    full_content=text,
                    nucleus_chunk=nucleus_chunk
                )
                # üßπ PHASE 3: Clean format - b·ªè d·∫•u ===
                context_parts.append(f"T√†i li·ªáu: {source} ({chunk_count} ƒëo·∫°n)\n{highlighted_text}")
                logger.info("‚úÖ Applied highlighting to nucleus chunk in context")
            else:
                # üßπ PHASE 3: Clean format - b·ªè d·∫•u ===
                context_parts.append(f"T√†i li·ªáu: {source} ({chunk_count} ƒëo·∫°n)\n{text}")
            
        return "\n\n".join(context_parts)
    
    def _detect_specific_intent(self, query: str) -> Optional[str]:
        """
        Ph√°t hi·ªán c√°c √Ω ƒë·ªãnh c·ª• th·ªÉ t·ª´ c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        ƒê√¢y l√† version ƒë∆°n gi·∫£n t·∫≠p trung v√†o metadata fields ph·ªï bi·∫øn
        """
        query_lower = query.lower()
        
        # Intent patterns cho t·ª´ng lo·∫°i th√¥ng tin
        fee_keywords = ['ph√≠', 'l·ªá ph√≠', 'bao nhi√™u ti·ªÅn', 't·ªën ti·ªÅn', 'gi√°', 'chi ph√≠', 'mi·ªÖn ph√≠']
        time_keywords = ['th·ªùi gian', 'bao l√¢u', 'm·∫•t bao l√¢u', 'khi n√†o xong', 'm·∫•y ng√†y', 'th·ªùi h·∫°n']
        form_keywords = ['m·∫´u', 'bi·ªÉu m·∫´u', 't·ªù khai', 'form', 'ƒë∆°n', 'gi·∫•y t·ªù c·∫ßn']
        agency_keywords = ['c∆° quan', 'n∆°i l√†m', 'ƒë√¢u', '·ªü ƒë√¢u', 'ƒë·ªãa ƒëi·ªÉm', 'n·ªôp ·ªü ƒë√¢u']
        requirements_keywords = ['ƒëi·ªÅu ki·ªán', 'y√™u c·∫ßu', 'c·∫ßn g√¨', 'h·ªì s∆°', 'gi·∫•y t·ªù']

        if any(keyword in query_lower for keyword in fee_keywords):
            return 'query_fee'
        if any(keyword in query_lower for keyword in time_keywords):
            return 'query_time'
        if any(keyword in query_lower for keyword in form_keywords):
            return 'query_form'
        if any(keyword in query_lower for keyword in agency_keywords):
            return 'query_agency'
        if any(keyword in query_lower for keyword in requirements_keywords):
            return 'query_requirements'
        
        return None
    
    def _build_smart_context(self, intent: Optional[str], metadata: Dict[str, Any], full_text: str) -> str:
        """
        üßπ PHASE 3: Enhanced smart context building - C·∫£i thi·ªán th√¥ng tin v·ªÅ ph√≠
        """
        priority_info = ""
        
        if intent == 'query_fee':
            fee_text = metadata.get('fee_text', '')
            fee_vnd = metadata.get('fee_vnd', 0)
            
            if fee_text:
                # X·ª≠ l√Ω th√¥ng tin ph√≠ chi ti·∫øt v√† r√µ r√†ng
                if fee_vnd == 0 and "Mi·ªÖn" in fee_text:
                    # Tr∆∞·ªùng h·ª£p mi·ªÖn ph√≠ th·ªß t·ª•c ch√≠nh nh∆∞ng c√≥ ph√≠ ph·ª•
                    priority_info = f"TH√îNG TIN V·ªÄ PH√ç:\n{fee_text}\n\n"
                else:
                    # Tr∆∞·ªùng h·ª£p c√≥ ph√≠
                    priority_info = f"L·ªÜ PH√ç: {fee_text}\n\n"
            elif fee_vnd == 0:
                priority_info = f"L·ªÜ PH√ç: Mi·ªÖn ph√≠\n\n"
            else:
                priority_info = f"L·ªÜ PH√ç: {fee_vnd:,} VNƒê\n\n"
        
        elif intent == 'query_time':
            time_text = metadata.get('processing_time_text', '')
            if time_text:
                priority_info = f"TH·ªúI GIAN X·ª¨ L√ù: {time_text}\n\n"

        elif intent == 'query_form':
            has_form = metadata.get('has_form', False)
            form_text = "C√≥ bi·ªÉu m·∫´u/t·ªù khai c·∫ßn ƒëi·ªÅn" if has_form else "Kh√¥ng c√≥ bi·ªÉu m·∫´u c·ª• th·ªÉ"
            priority_info = f"BI·ªÇU M·∫™U: {form_text}\n\n"
            
        elif intent == 'query_agency':
            agency = metadata.get('executing_agency', '')
            if agency:
                priority_info = f"C∆† QUAN TH·ª∞C HI·ªÜN: {agency}\n\n"
                
        elif intent == 'query_requirements':
            requirements = metadata.get('requirements_conditions', '')
            if requirements:
                priority_info = f"ƒêI·ªÄU KI·ªÜN/Y√äU C·∫¶U: {requirements}\n\n"

        # K·∫øt h·ª£p th√¥ng tin ∆∞u ti√™n v·ªõi full context - CLEAN FORMAT
        if priority_info:
            return f"{priority_info}TH√îNG TIN CHI TI·∫æT:\n{full_text}"
        else:
            # Kh√¥ng c√≥ intent c·ª• th·ªÉ - gi·ªØ nguy√™n context
            return full_text
        
    def _generate_answer_with_context(
        self,
        query: str,
        context: str,
        session: OptimizedChatSession
    ) -> str:
        """Generate answer v·ªõi context v√† session history s·ª≠ d·ª•ng ChatML format"""
        
        # CHU·∫®N B·ªä CHAT HISTORY C√ì C·∫§U TR√öC cho ChatML template
        chat_history_structured = []
        if len(session.query_history) > 0:
            # üöÄ PERFORMANCE OPTIMIZATION: Ch·ªâ l·∫•y 1 l∆∞·ª£t h·ªèi-ƒë√°p g·∫ßn nh·∫•t ƒë·ªÉ gi·∫£m prompt length
            recent_queries = session.query_history[-1:]  # Only last 1 query thay v√¨ 3
            logger.info(f"‚ö° Chat history: {len(recent_queries)} entries (optimized for speed)")
            
            for item in recent_queries:
                chat_history_structured.append({"role": "user", "content": item['query']})
                # R√∫t g·ªçn answer ƒë·ªÉ tr√°nh context overflow
                answer_preview = item['answer'][:100] + "..." if len(item['answer']) > 100 else item['answer']
                chat_history_structured.append({"role": "assistant", "content": answer_preview})
            
        # üéØ PHASE 2: Enhanced Clean System Prompt - C·∫£i thi·ªán kh·∫£ nƒÉng ph√¢n bi·ªát th√¥ng tin
        system_prompt_clean = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam.

QUY T·∫ÆC:
1. ∆Øu ti√™n th√¥ng tin trong [TH√îNG TIN CH√çNH]...[/TH√îNG TIN CH√çNH]
2. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·ª± nhi√™n nh∆∞ n√≥i chuy·ªán (5-7 c√¢u)
3. CH·ªà d·ª±a tr√™n th√¥ng tin c√≥ trong t√†i li·ªáu
4. N·∫øu kh√¥ng c√≥ th√¥ng tin: "T√†i li·ªáu kh√¥ng ƒë·ªÅ c·∫≠p v·∫•n ƒë·ªÅ n√†y"
5. KH√îNG s·ª≠ d·ª•ng k√Ω t·ª± ƒë·∫∑c bi·ªát, emoji, d·∫•u g·∫°ch

PH√ÇN BI·ªÜT C√ÅC LO·∫†I PH√ç:
- Khi h·ªèi v·ªÅ ph√≠ th·ªß t·ª•c: Ki·ªÉm tra fee_vnd v√† fee_text
- N·∫øu fee_vnd = 0: "Mi·ªÖn ph√≠" cho th·ªß t·ª•c ch√≠nh
- N·∫øu fee_text c√≥ "Mi·ªÖn l·ªá ph√≠" + "Ph√≠ c·∫•p b·∫£n sao": Ph√¢n bi·ªát r√µ 2 lo·∫°i
- V√ç D·ª§: "ƒêƒÉng k√Ω k·∫øt h√¥n mi·ªÖn ph√≠. Ch·ªâ t√≠nh ph√≠ 8.000ƒë/b·∫£n khi xin b·∫£n sao tr√≠ch l·ª•c"

TH√îNG TIN QUAN TR·ªåNG:
- Th·ªùi gian: T√¨m processing_time_text - th·ªùi gian x·ª≠ l√Ω
- N∆°i l√†m: T√¨m executing_agency - c∆° quan th·ª±c hi·ªán  
- Bi·ªÉu m·∫´u: T√¨m has_form - c√≥/kh√¥ng c√≥ m·∫´u ƒë∆°n

PHONG C√ÅCH: T·ª± nhi√™n, th√¢n thi·ªán, ch√≠nh x√°c v·ªÅ th√¥ng tin ph√≠."""
        
        logger.info(f"üìù Using ChatML format with structured chat history: {len(chat_history_structured)} messages")
        
        # üî• TOKEN MANAGEMENT - Ki·ªÉm so√°t ƒë·ªô d√†i ƒë·ªÉ tr√°nh context overflow
        from app.core.config import settings
        
        # ∆Ø·ªõc t√≠nh token ƒë∆°n gi·∫£n (1 token ‚âà 3-4 k√Ω t·ª± ti·∫øng Vi·ªát)
        # T√≠nh to√°n cho ChatML format v·ªõi c√°c token ƒë·∫∑c bi·ªát
        chat_history_text = "\n".join([f"{item['role']}: {item['content']}" for item in chat_history_structured])
        estimated_tokens = len(system_prompt_clean + context + query + chat_history_text + "<|im_start|><|im_end|>") // 3
        max_context_tokens = settings.n_ctx - 500  # ƒê·ªÉ l·∫°i 500 token cho response
        
        if estimated_tokens > max_context_tokens:
            # C·∫Øt b·ªõt context ƒë·ªÉ fit trong gi·ªõi h·∫°n
            logger.warning(f"üö® Context overflow detected: {estimated_tokens} tokens > {max_context_tokens} max")
            
            # T√≠nh to√°n space c√≤n l·∫°i cho context
            fixed_parts_length = len(system_prompt_clean + chat_history_text + query + "<|im_start|><|im_end|>")
            remaining_space = (max_context_tokens * 3) - fixed_parts_length
            
            if remaining_space > 500:  # ƒê·∫£m b·∫£o c√≥ √≠t nh·∫•t 500 k√Ω t·ª± cho context
                context = context[:remaining_space] + "\n\n[...TH√îNG TIN ƒê√É ƒê∆Ø·ª¢C R√öT G·ªåN ƒê·ªÇ TR√ÅNH QU√Å T·∫¢I...]"
                logger.info(f"‚úÇÔ∏è Context truncated to {len(context)} chars")
            else:
                # N·∫øu kh√¥ng ƒë·ªß ch·ªó, b·ªè chat history
                chat_history_structured = []
                context = context[:max_context_tokens * 3 // 2] + "\n\n[...R√öT G·ªåN...]"
                logger.warning("‚ö†Ô∏è Removed chat history due to extreme context overflow")
        
        logger.info(f"üìù Final context length: {len(context)} chars (~{len(context)//3} tokens)")

        try:
            response_data = self.llm_service.generate_response(
                user_query=query,
                context=context,
                max_tokens=settings.max_tokens,
                temperature=settings.temperature,
                system_prompt=system_prompt_clean,
                chat_history=chat_history_structured  # üî• THAM S·ªê M·ªöI cho ChatML
            )
            
            # Extract response text from dict
            if isinstance(response_data, dict) and "response" in response_data:
                return response_data["response"].strip()
            elif isinstance(response_data, str):
                return response_data.strip()
            else:
                return str(response_data).strip()
            
        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            return f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t·∫°o c√¢u tr·∫£ l·ªùi: {e}"
            
    def get_health_status(self) -> Dict[str, Any]:
        """Tr·∫°ng th√°i health c·ªßa service"""
        try:
            # L·∫•y th·ªëng k√™ t·ª´ router thay v√¨ vectordb ƒë·ªÉ accurate h∆°n
            try:
                router_collections = self.smart_router.get_collections()
                total_collections = len(router_collections)
                total_documents = sum(col.get('file_count', 0) for col in router_collections)
            except:
                # Fallback n·∫øu router ch∆∞a ready
                total_collections = 0
                total_documents = 0
                    
            return {
                "status": "healthy",
                "total_collections": total_collections,
                "total_documents": total_documents,
                "llm_loaded": self.llm_service is not None and hasattr(self.llm_service, 'model') and self.llm_service.model is not None,
                "reranker_loaded": self.reranker_service is not None and hasattr(self.reranker_service, 'model') and self.reranker_service.model is not None,
                "embedding_device": "CPU (VRAM optimized)",
                "llm_device": "GPU",
                "reranker_device": "GPU", 
                "active_sessions": len(self.chat_sessions),
                "metrics": self.metrics,
                "router_ready": hasattr(self, 'smart_router') and self.smart_router is not None,
                "context_expansion": {
                    "total_chunks_cached": len(self.context_expansion_service.document_metadata_cache),
                    **self.context_expansion_service.get_stats()
                },
                "ambiguous_patterns": 0  # Placeholder
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
            
    def cleanup_old_sessions(self, max_age_hours: int = 24):
        """D·ªçn d·∫πp sessions c≈©"""
        current_time = time.time()
        cutoff_time = current_time - (max_age_hours * 3600)
        
        old_sessions = [
            session_id for session_id, session in self.chat_sessions.items()
            if session.last_accessed < cutoff_time
        ]
        
        for session_id in old_sessions:
            del self.chat_sessions[session_id]
            
        if old_sessions:
            logger.info(f"Cleaned up {len(old_sessions)} old sessions")
            
        return len(old_sessions)

    def check_document_forms(self, source_documents: List[str]) -> List[Dict[str, Any]]:
        """
        Check if documents have forms and collect form information
        
        Args:
            source_documents: List of document names/titles
            
        Returns:
            List of form info dictionaries
        """
        forms_info = []
        
        if not path_config:
            logger.warning("PathConfig not available, cannot check for forms")
            return forms_info
            
        try:
            for doc_name in source_documents:
                # Search for document across collections
                for collection in path_config.list_collections():
                    documents = path_config.list_documents(collection)
                    
                    # Search by document title or file name
                    matching_doc = None
                    for doc in documents:
                        doc_content = path_config.load_document_json(collection, doc["doc_id"])
                        if doc_content:
                            doc_title = doc_content.get('metadata', {}).get('title', '')
                            if (doc_name in doc_title or doc_title in doc_name or 
                                doc_name in doc.get('json_file', '') or doc_name in doc.get('doc_file', '')):
                                matching_doc = doc
                                break
                    
                    if matching_doc:
                        doc_content = path_config.load_document_json(collection, matching_doc["doc_id"])
                        
                        if doc_content and doc_content.get('metadata', {}).get('has_form'):
                            # Get forms for this document
                            forms_path = path_config.get_document_forms_path(collection, matching_doc["doc_id"])
                            if forms_path and forms_path.exists():
                                forms_list = []
                                for form_file in forms_path.iterdir():
                                    if form_file.is_file() and form_file.suffix.lower() in ['.pdf', '.doc', '.docx', '.xls', '.xlsx']:
                                        forms_list.append({
                                            'filename': form_file.name,
                                            'path': str(form_file),
                                            'size': form_file.stat().st_size,
                                            'type': form_file.suffix.lower()
                                        })
                                
                                if forms_list:
                                    forms_info.append({
                                        'document': doc_name,
                                        'collection': collection,
                                        'title': doc_content.get('metadata', {}).get('title', doc_name),
                                        'forms': forms_list
                                    })
                        break  # Found document in this collection
                        
        except Exception as e:
            logger.error(f"Error checking document forms: {e}")
            
        return forms_info

    # API Compatibility Methods
    def query(self, question: Optional[str] = None, query: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """Compatibility method cho API routes"""
        # H·ªó tr·ª£ c·∫£ 'question' v√† 'query' parameters
        query_text = question or query
        if not query_text:
            raise ValueError("Either 'question' or 'query' parameter is required")
        return self.process_query(query_text, **kwargs)
    
    @property
    def query_router(self):
        """Compatibility property cho API routes"""
        # T·∫°o wrapper v·ªõi explain_routing method
        class RouterWrapper:
            def __init__(self, smart_router):
                self.smart_router = smart_router
                # Copy t·∫•t c·∫£ methods t·ª´ smart_router
                for attr in dir(smart_router):
                    if not attr.startswith('_') and callable(getattr(smart_router, attr)):
                        setattr(self, attr, getattr(smart_router, attr))
            
            def explain_routing(self, question: str) -> Dict[str, Any]:
                """Explain routing decision cho question"""
                try:
                    # S·ª≠ d·ª•ng smart router ƒë·ªÉ classify
                    route_result = self.smart_router.route_query(question)
                    return {
                        'question': question,
                        'route': route_result.get('route_name', 'general'),
                        'confidence': route_result.get('confidence', 0.0),
                        'reasoning': route_result.get('reasoning', 'No reasoning available'),
                        'suggested_collections': route_result.get('suggested_collections', [])
                    }
                except Exception as e:
                    logger.error(f"Error explaining routing: {e}")
                    return {
                        'question': question,
                        'route': 'general',
                        'confidence': 0.0,
                        'reasoning': f'Error: {str(e)}',
                        'suggested_collections': []
                    }
        
        return RouterWrapper(self.smart_router)

    def build_index(self, collection_name: Optional[str] = None, force_rebuild: bool = False, **kwargs) -> Dict[str, Any]:
        """Build index cho collection ho·∫∑c t·∫•t c·∫£ collections"""
        try:
            if collection_name:
                # Build specific collection
                if self.vectordb_service.collection_exists(collection_name):
                    stats = self.vectordb_service.get_collection_stats(collection_name)
                    return {
                        'status': 'success',
                        'collections_processed': 1,
                        'collection_name': collection_name,
                        'message': f'Collection {collection_name} already exists and ready',
                        'document_count': stats.get('document_count', 0)
                    }
                else:
                    return {
                        'status': 'error',
                        'collections_processed': 0,
                        'collection_name': collection_name,
                        'error': f'Collection {collection_name} does not exist',
                        'suggestion': 'Run python tools/2_build_vectordb_modernized.py to build collections'
                    }
            else:
                # Build all collections - return info about existing ones
                collections = self.vectordb_service.list_collections()
                return {
                    'status': 'success',
                    'collections_processed': len(collections),
                    'message': f'Found {len(collections)} existing collections',
                    'collections': [col['name'] for col in collections],
                    'total_documents': sum(col.get('document_count', 0) for col in collections),
                    'suggestion': 'Use python tools/2_build_vectordb_modernized.py to build new collections from documents'
                }
        except Exception as e:
            logger.error(f"Error in build_index: {e}")
            return {
                'status': 'error',
                'collections_processed': 0,
                'error': str(e)
            }
    
    def _generate_smart_clarification(self, routing_result: Dict[str, Any], query: str, session_id: str, start_time: float) -> Dict[str, Any]:
        """T·∫°o clarification th√¥ng minh d·ª±a tr√™n confidence level"""
        try:
            # G·ªçi Smart Clarification Service ƒë·ªÉ t·∫°o clarification th√¥ng minh
            clarification_service = ClarificationService()
            clarification_response = clarification_service.generate_clarification(
                query=query,
                confidence=routing_result.get('confidence', 0.0),
                routing_result=routing_result
            )
            
            # Merge clarification response with required fields
            processing_time = time.time() - start_time
            
            # Get the main response from clarification service
            response = clarification_response.copy()
            
            # Add required fields that API expects
            response.update({
                "session_id": session_id,
                "processing_time": processing_time,
                "routing_info": {
                    "target_collection": routing_result.get('target_collection'),
                    "router_confidence": routing_result.get('confidence', 0.0),
                    "status": "smart_clarification"
                }
            })
            
            # üîß STORE ROUTING CONTEXT: Save original routing info to session for Step 2‚Üí3 similarity matching
            session = self.get_session(session_id)
            if session:
                session.metadata['original_routing_context'] = routing_result
                session.metadata['original_query'] = query
                logger.info(f"üíæ Stored original routing context for session {session_id}")
            
            return convert_numpy_types(response)
            
        except Exception as e:
            logger.error(f"Error generating smart clarification: {e}")
            processing_time = time.time() - start_time
            
            fallback_response = {
                "type": "clarification_needed",
                "confidence": routing_result.get('confidence', 0.0),
                "clarification": {
                    "message": "Xin l·ªói, t√¥i kh√¥ng r√µ √Ω ƒë·ªãnh c·ªßa c√¢u h·ªèi. B·∫°n c√≥ th·ªÉ di·ªÖn ƒë·∫°t r√µ h∆°n kh√¥ng?",
                    "options": [
                        {
                            'id': 'retry',
                            'title': "H√£y di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi",
                            'description': "T√¥i s·∫Ω c·ªë g·∫Øng hi·ªÉu r√µ h∆°n",
                            'action': 'manual_input'
                        }
                    ],
                    "style": "fallback"
                },
                "session_id": session_id,
                "processing_time": processing_time,
                "routing_info": {
                    "target_collection": routing_result.get('target_collection'),
                    "router_confidence": routing_result.get('confidence', 0.0),
                    "status": "smart_clarification_error",
                    "error": str(e)
                }
            }
            return convert_numpy_types(fallback_response)
    
    def _activate_vector_backup_strategy(self, routing_result: Dict[str, Any], query: str, session_id: str, start_time: float) -> Dict[str, Any]:
        """K√≠ch ho·∫°t Vector Backup Strategy khi Smart Router ho√†n to√†n th·∫•t b·∫°i"""
        try:
            logger.info("üö® Activating Vector Backup Strategy - searching across all collections")
            
            # Th·ª±c hi·ªán vector search tr·ª±c ti·∫øp tr√™n t·∫•t c·∫£ collections ƒë·ªÉ t√¨m topics li√™n quan
            all_collections = self.vectordb_service.list_collections()
            backup_results = []
            
            for collection_info in all_collections[:3]:  # Limit to top 3 collections for performance
                collection_name = collection_info["name"]
                try:
                    collection = self.vectordb_service.get_collection(collection_name)
                    search_results = self.vectordb_service.search_in_collection(
                        collection_name,
                        query,
                        top_k=2,  # Ch·ªâ l·∫•y top 2 results per collection
                        similarity_threshold=0.3,
                        where_filter={}
                    )
                    
                    if search_results:
                        best_result = search_results[0]
                        backup_results.append({
                            'collection': collection_name,
                            'score': best_result.get('similarity', best_result.get('score', 0)),
                            'content': best_result.get('content', best_result.get('document', ''))[:200] + "...",
                            'metadata': best_result.get('metadata', {}),
                            'source': best_result.get('metadata', {}).get('source', 'N/A')
                        })
                        
                except Exception as e:
                    logger.warning(f"Error searching collection {collection_name}: {e}")
                    continue
            
            # Sort by score v√† t·∫°o suggestions
            backup_results.sort(key=lambda x: x['score'], reverse=True)
            
            options = []
            for i, result in enumerate(backup_results[:3], 1):
                # Tr√≠ch xu·∫•t title t·ª´ metadata n·∫øu c√≥
                metadata = result.get('metadata', {})
                title = metadata.get('document_title', metadata.get('title', f"Th·ªß t·ª•c {result['collection']}"))
                
                option = {
                    'id': str(i),
                    'title': title,
                    'description': f"ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng: {result['score']:.2f} - {result['content']}",
                    'collection': result['collection'],
                    'backup_score': result['score']
                }
                options.append(option)
            
            clarification_msg = "T√¥i kh√¥ng t√¨m th·∫•y c√¢u h·ªèi m·∫´u ph√π h·ª£p, nh∆∞ng d·ª±a tr√™n t√¨m ki·∫øm trong d·ªØ li·ªáu, c√¢u h·ªèi c·ªßa b·∫°n c√≥ th·ªÉ li√™n quan ƒë·∫øn:"
            
            if not options:
                clarification_msg = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p. B·∫°n c√≥ th·ªÉ th·ª≠ v·ªõi t·ª´ kh√≥a kh√°c kh√¥ng?"
            
            logger.info(f"Vector backup strategy found {len(options)} potential matches")
            
            return {
                "type": "clarification_needed",
                "status": "vector_backup",
                "confidence": routing_result.get('confidence', 0.0),
                "clarification": clarification_msg,
                "options": options,
                "backup_results": len(backup_results),
                "session_id": session_id,
                "processing_time": time.time() - start_time,
                "strategy": "vector_backup"
            }
            
        except Exception as e:
            logger.error(f"Error in vector backup strategy: {e}")
            return {
                "type": "clarification_needed",
                "status": "fallback_error", 
                "clarification": "Xin l·ªói, c√≥ l·ªói h·ªá th·ªëng khi x·ª≠ l√Ω c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i sau.",
                "session_id": session_id,
                "processing_time": time.time() - start_time
            }
    
    @property  
    def document_processor(self):
        """Compatibility property cho API routes v·ªõi h·ªó tr·ª£ new structure"""
        import os
        
        class DocumentProcessorCompat:
            def get_available_collections(self, documents_dir=None):
                """L·∫•y danh s√°ch collections t·ª´ new structure ho·∫∑c old structure"""
                try:
                    # Try new structure first
                    if path_config.is_new_structure_available():
                        collections = []
                        for collection_name in path_config.list_collections():
                            documents = path_config.list_documents(collection_name)
                            doc_count = len([doc for doc in documents if doc["json_path"]])
                            
                            if doc_count > 0:
                                collections.append({
                                    'name': collection_name,
                                    'path': str(path_config.get_collection_dir(collection_name)),
                                    'document_count': doc_count,
                                    'structure': 'new'  # Indicate new structure
                                })
                        
                        logger.info(f"üìÅ Found {len(collections)} collections in new structure")
                        return collections
                    
                    # Fallback to old structure
                    if documents_dir and os.path.exists(documents_dir):
                        collections = []
                        for item in os.listdir(documents_dir):
                            item_path = os.path.join(documents_dir, item)
                            if os.path.isdir(item_path):
                                # Count JSON files instead of PDF files for old structure
                                json_count = len([f for f in os.listdir(item_path) 
                                               if f.lower().endswith('.json')])
                                if json_count > 0:
                                    collections.append({
                                        'name': item,
                                        'path': item_path,
                                        'document_count': json_count,
                                        'structure': 'old'  # Indicate old structure
                                    })
                        
                        logger.info(f"üìÅ Found {len(collections)} collections in old structure")
                        return collections
                    
                    logger.warning("üìÅ No document structure found (neither new nor old)")
                    return []
                    
                except Exception as e:
                    logger.error(f"Error getting available collections: {e}")
                    return []
        
        return DocumentProcessorCompat()
