"""
Optimized Enhanced RAG Service v·ªõi VRAM-optimized architecture
T√≠ch h·ª£p:
1. Ambiguous Query Detection & Processing
2. Enhanced Context Expansion v·ªõi Nucleus Strategy  
3. VRAM-optimized model placement (CPU embedding, GPU LLM/Reranker)
4. Session management
"""

import logging
import time
import uuid
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from pathlib import Path

from .vector_database import VectorDBService
from .language_model import LLMService
from .result_reranker import RerankerService
from .smart_clarification import SmartClarificationService
from .smart_router import EnhancedSmartQueryRouter, RouterBasedAmbiguousQueryService
from .smart_clarification import SmartClarificationService
from .context_expander import EnhancedContextExpansionService
from ..core.config import settings

logger = logging.getLogger(__name__)

def convert_numpy_types(obj: Any) -> Any:
    """Convert numpy types to Python native types for JSON serialization"""
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(v) for v in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(v) for v in obj)
    else:
        return obj

@dataclass
class OptimizedChatSession:
    """Session chat v·ªõi th√¥ng tin t·ªëi ∆∞u v·ªõi Stateful Router support"""
    session_id: str
    created_at: float
    last_accessed: float
    query_history: List[Dict[str, Any]] = field(default_factory=list)
    context_cache: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # Stateful Router State
    last_successful_collection: Optional[str] = None
    last_successful_confidence: float = 0.0
    last_successful_timestamp: Optional[float] = None
    last_successful_filters: Optional[Dict[str, Any]] = None  # üî• NEW: L∆∞u filters t·ª´ session th√†nh c√¥ng
    cached_rag_content: Optional[Dict[str, Any]] = None
    consecutive_low_confidence_count: int = 0
    
    def update_successful_routing(self, collection: str, confidence: float, filters: Optional[Dict[str, Any]] = None, rag_content: Optional[Dict[str, Any]] = None):
        """C·∫≠p nh·∫≠t state khi routing th√†nh c√¥ng v·ªõi confidence cao"""
        self.last_successful_collection = collection
        self.last_successful_confidence = confidence
        self.last_successful_timestamp = time.time()
        self.last_successful_filters = filters  # üî• NEW: L∆∞u filters
        if rag_content:
            self.cached_rag_content = rag_content
        self.consecutive_low_confidence_count = 0  # Reset counter
        
    def should_override_confidence(self, current_confidence: float) -> bool:
        """
        Ki·ªÉm tra c√≥ n√™n ghi ƒë√® k·∫øt qu·∫£ ƒë·ªãnh tuy·∫øn hi·ªán t·∫°i b·∫±ng ng·ªØ c·∫£nh ƒë√£ l∆∞u kh√¥ng.
        Ghi ƒë√® khi:
        1. ƒêang c√≥ ng·ªØ c·∫£nh t·ªët ƒë∆∞·ª£c l∆∞u t·ª´ tr∆∞·ªõc.
        2. K·∫øt qu·∫£ ƒë·ªãnh tuy·∫øn m·ªõi kh√¥ng ph·∫£i l√† "r·∫•t ch·∫Øc ch·∫Øn".
        """
        if not self.last_successful_collection:
            return False

        # Ch·ªâ ghi ƒë√® trong v√≤ng 10 ph√∫t
        if self.last_successful_timestamp and (time.time() - self.last_successful_timestamp > 600):
            return False

        # Ng∆∞·ª°ng tin c·∫≠y "r·∫•t cao" m√† ch√∫ng ta s·∫Ω kh√¥ng can thi·ªáp
        VERY_HIGH_CONFIDENCE_GATE = 0.82 
        # Ng∆∞·ª°ng t·ªëi thi·ªÉu c·ªßa ng·ªØ c·∫£nh ƒë√£ l∆∞u ƒë·ªÉ ƒë∆∞·ª£c coi l√† "t·ªët"
        MIN_CONTEXT_CONFIDENCE = 0.78

        # N·∫øu ƒë·ªô tin c·∫≠y hi·ªán t·∫°i kh√¥ng ƒë·ªß cao V√Ä ng·ªØ c·∫£nh tr∆∞·ªõc ƒë√≥ ƒë·ªß t·ªët -> Ghi ƒë√®
        if current_confidence < VERY_HIGH_CONFIDENCE_GATE and self.last_successful_confidence >= MIN_CONTEXT_CONFIDENCE:
            logger.info(f"üî• STATEFUL ROUTER: Ghi ƒë√® v√¨ current_confidence ({current_confidence:.3f}) < {VERY_HIGH_CONFIDENCE_GATE} v√† context_confidence ({self.last_successful_confidence:.3f}) >= {MIN_CONTEXT_CONFIDENCE}")
            return True

        return False
        
    def increment_low_confidence(self):
        """TƒÉng counter khi g·∫∑p confidence th·∫•p"""
        self.consecutive_low_confidence_count += 1
        
    def clear_routing_state(self):
        """Clear state khi user chuy·ªÉn ch·ªß ƒë·ªÅ ho√†n to√†n"""
        self.last_successful_collection = None
        self.last_successful_confidence = 0.0
        self.last_successful_timestamp = None
        self.last_successful_filters = None  # üî• NEW: Clear filters c≈©
        self.cached_rag_content = None
        self.consecutive_low_confidence_count = 0

class OptimizedEnhancedRAGService:
    """
    Enhanced RAG Service ƒë∆∞·ª£c t·ªëi ∆∞u VRAM v√† performance
    
    Ki·∫øn tr√∫c:
    - Embedding Model: CPU (ti·∫øt ki·ªám VRAM cho query ng·∫Øn)
    - LLM: GPU (c·∫ßn song song h√≥a cho context d√†i)  
    - Reranker: GPU (c·∫ßn song song h√≥a cho multiple comparisons)
    """
    
    def __init__(
        self,
        documents_dir: str,
        vectordb_service: VectorDBService,
        llm_service: LLMService
    ):
        self.documents_dir = documents_dir
        self.vectordb_service = vectordb_service
        self.llm_service = llm_service
        
        # Initialize supporting services
        self._initialize_services()
        
        # Chat sessions management
        self.chat_sessions: Dict[str, OptimizedChatSession] = {}
        
        # Performance metrics
        self.metrics = {
            "total_queries": 0,
            "ambiguous_detected": 0,
            "context_expansions": 0,
            "avg_response_time": 0.0
        }
        
        logger.info("‚úÖ Optimized Enhanced RAG Service initialized")
        
    def _initialize_services(self):
        """Kh·ªüi t·∫°o c√°c service h·ªó tr·ª£ v·ªõi Enhanced Smart Router"""
        try:
            # Enhanced Smart Query Router v·ªõi Example Questions Database
            embedding_model = self.vectordb_service.embedding_model
            if embedding_model is None:
                raise ValueError("VectorDB embedding model not initialized")
            self.smart_router = EnhancedSmartQueryRouter(embedding_model=embedding_model)
            logger.info("‚úÖ Enhanced Smart Query Router initialized")
            
            # Reranker Service (GPU)
            self.reranker_service = RerankerService()
            logger.info("‚úÖ Reranker Service initialized (GPU)")
            
            # Router-based Ambiguous Query Service (CPU)
            self.ambiguous_service = RouterBasedAmbiguousQueryService(
                router=self.smart_router
            )
            logger.info("‚úÖ Router-based Ambiguous Query Service initialized (CPU)")
            
            # Smart Clarification Service
            self.clarification_service = SmartClarificationService()
            logger.info("‚úÖ Smart Clarification Service initialized")
            
            # Enhanced Context Expansion Service  
            self.context_expansion_service = EnhancedContextExpansionService(
                vectordb_service=self.vectordb_service,
                documents_dir=self.documents_dir
            )
            logger.info("‚úÖ Enhanced Context Expansion Service initialized")
            
        except Exception as e:
            logger.error(f"Error initializing services: {e}")
            raise
            
    def create_session(self, metadata: Optional[Dict[str, Any]] = None) -> str:
        """T·∫°o session chat m·ªõi"""
        session_id = str(uuid.uuid4())
        
        session = OptimizedChatSession(
            session_id=session_id,
            created_at=time.time(),
            last_accessed=time.time(),
            metadata=metadata or {}
        )
        
        self.chat_sessions[session_id] = session
        logger.info(f"Created new chat session: {session_id}")
        
        return session_id
        
    def get_session(self, session_id: str) -> Optional[OptimizedChatSession]:
        """L·∫•y session theo ID"""
        session = self.chat_sessions.get(session_id)
        if session:
            session.last_accessed = time.time()
        return session
        
    def enhanced_query(
        self,
        query: str,
        session_id: Optional[str] = None,
        reranker_k: int = 10,
        llm_k: int = 5,
        threshold: float = 0.7,
        forced_collection: Optional[str] = None,  # ‚ö° TH√äM THAM S·ªê ANTI-LOOP
        forced_document_title: Optional[str] = None  # üî• NEW: Force exact document filtering
    ) -> Dict[str, Any]:
        """
        Query ch√≠nh v·ªõi t·∫•t c·∫£ t·ªëi ∆∞u h√≥a - THI·∫æT K·∫æ G·ªêC: FULL DOCUMENT EXPANSION
        
        Flow:
        1. Detect ambiguous query (CPU embedding)
        2. Route query n·∫øu clear
        3. Broad search (CPU embedding) 
        4. Rerank (GPU reranker)
        5. Context expansion: TO√ÄN B·ªò DOCUMENT (ƒë·∫£m b·∫£o ng·ªØ c·∫£nh ph√°p lu·∫≠t ƒë·∫ßy ƒë·ªß)
        6. Generate answer (GPU LLM)
        
        TRI·∫æT L√ù: VƒÉn b·∫£n ph√°p lu·∫≠t ph·∫£i ƒë∆∞·ª£c hi·ªÉu trong TO√ÄN B·ªò ng·ªØ c·∫£nh c·ªßa document g·ªëc
        """
        start_time = time.time()
        self.metrics["total_queries"] += 1
        
        try:
            # Get or create session
            if session_id:
                session = self.get_session(session_id)
                if not session:
                    # Create new session with provided ID
                    session = OptimizedChatSession(
                        session_id=session_id,
                        created_at=time.time(),
                        last_accessed=time.time(),
                        metadata={}
                    )
                    self.chat_sessions[session_id] = session
                    logger.info(f"üÜï Created new session with provided ID: {session_id}")
            else:
                session_id = self.create_session()
                session = self.get_session(session_id)
                
            logger.info(f"Processing query in session {session_id}: {query[:50]}...")
            
            # Check for preserved document context from manual input
            if not forced_document_title and not forced_collection and session:
                preserved_document = session.metadata.get('preserved_document')
                if preserved_document:
                    logger.info(f"üîÑ Found preserved document context: {preserved_document['title']}")
                    forced_collection = preserved_document['collection']
                    forced_document_title = preserved_document['title']
            
            # Step 1: Enhanced Smart Query Routing v·ªõi MULTI-LEVEL Confidence Processing + Stateful Router
            if forced_collection:
                # ÔøΩ FORCED ROUTING: D√†nh cho clarification ho·∫∑c debug
                logger.info(f"‚ö° Forced routing to collection: {forced_collection} (from clarification)")
                routing_result = {
                    "target_collection": forced_collection,
                    "confidence": 0.95,  # High confidence cho forced routing
                    "inferred_filters": {}
                }
                # Get confidence level from routing result for further processing
                confidence_level = routing_result.get('confidence_level', 'forced_high')
                best_collections = [forced_collection]
                inferred_filters = {}
                
                # üî• NEW: Add document title filter if specified
                if forced_document_title:
                    inferred_filters = {"document_title": forced_document_title}
                    logger.info(f"üéØ Forced document filter: {forced_document_title}")
                
            else:
                # üß† SMART ROUTING: S·ª≠ d·ª•ng router b√¨nh th∆∞·ªùng
                routing_result = self.smart_router.route_query(query, session)
                confidence_level = routing_result.get('confidence_level', 'low')
                was_overridden = routing_result.get('was_overridden', False)
                
                logger.info(f"Router confidence: {confidence_level} (score: {routing_result['confidence']:.3f})")
                if was_overridden:
                    logger.info(f"üî• Session-based confidence override applied!")
                
                if confidence_level in ['high', 'override_high']:
                    # HIGH CONFIDENCE (including overridden) - Route tr·ª±c ti·∫øp
                    target_collection = routing_result['target_collection']
                    inferred_filters = routing_result.get('inferred_filters', {})
                    best_collections = [target_collection] if target_collection else [settings.chroma_collection_name]
                    logger.info(f"‚úÖ HIGH CONFIDENCE routing to: {target_collection}")
                    
                elif confidence_level in ['low-medium', 'override_medium']:
                    # MEDIUM CONFIDENCE (including overridden) - Route v·ªõi caution
                    target_collection = routing_result['target_collection']
                    inferred_filters = routing_result.get('inferred_filters', {})
                    best_collections = [target_collection] if target_collection else [settings.chroma_collection_name]
                    logger.info(f"‚ö†Ô∏è MEDIUM CONFIDENCE routing to: {target_collection}")
                    
                else:
                    # T·∫§T C·∫¢ CONFIDENCE < THRESHOLD - H·ªèi l·∫°i user, kh√¥ng route
                    logger.info(f"ü§î CONFIDENCE KH√îNG ƒê·ª¶ CAO ({confidence_level}) - h·ªèi l·∫°i user thay v√¨ route")
                    return self._generate_smart_clarification(routing_result, query, session_id, start_time)
            
            # Step 2: Focused Search v·ªõi ƒê·ªòNG BROAD_SEARCH_K d·ª±a tr√™n router confidence
            # üöÄ PERFORMANCE OPTIMIZATION: Gi·∫£m s·ªë documents c·∫ßn rerank
            dynamic_k = settings.broad_search_k  # default 12
            if confidence_level == 'high':
                dynamic_k = max(8, settings.broad_search_k - 4)  # Router t·ª± tin ‚Üí √≠t docs h∆°n
                logger.info(f"üéØ HIGH CONFIDENCE: Gi·∫£m broad_search_k xu·ªëng {dynamic_k}")
            elif confidence_level in ['low-medium', 'override_medium']:
                dynamic_k = min(15, settings.broad_search_k + 3)  # Router kh√¥ng ch·∫Øc ‚Üí nhi·ªÅu docs h∆°n
                logger.info(f"üîç MEDIUM CONFIDENCE: TƒÉng broad_search_k l√™n {dynamic_k}")
            else:
                logger.info(f"üìä DEFAULT/FALLBACK: S·ª≠ d·ª•ng broad_search_k={dynamic_k}")
            
            broad_search_results = []
            for collection_name in best_collections[:2]:  # Limit to top 2 collections
                try:
                    # ‚úÖ CRITICAL FIX: Pass smart filters to vector search v·ªõi dynamic K
                    # üîç DEBUG: Log filter tr∆∞·ªõc khi t√¨m ki·∫øm ƒë·ªÉ debug v·∫•n ƒë·ªÅ filter b·ªã "ƒë√°nh r∆°i"
                    logger.info(f"üîç Chu·∫©n b·ªã t√¨m ki·∫øm v·ªõi filter: {inferred_filters}")
                    
                    # üî• ADAPTIVE THRESHOLD: H·∫° threshold khi c√≥ filter v√¨ filter ƒë√£ ƒë·∫£m b·∫£o relevance
                    adaptive_threshold = settings.similarity_threshold
                    if inferred_filters:
                        adaptive_threshold = max(0.2, settings.similarity_threshold * 0.5)  # H·∫° threshold khi c√≥ filter
                        logger.info(f"üéØ ADAPTIVE THRESHOLD: {settings.similarity_threshold} -> {adaptive_threshold} (c√≥ filter)")
                    else:
                        logger.info(f"üìä STANDARD THRESHOLD: {adaptive_threshold} (kh√¥ng c√≥ filter)")
                    
                    results = self.vectordb_service.search_in_collection(
                        collection_name=collection_name,
                        query=query,
                        top_k=dynamic_k,
                        similarity_threshold=adaptive_threshold,
                        where_filter=inferred_filters if inferred_filters else None
                    )
                    
                    for result in results:
                        result["collection"] = collection_name
                        
                    broad_search_results.extend(results)
                    
                except Exception as e:
                    logger.warning(f"Error searching in collection {collection_name}: {e}")
            
            logger.info(f"üìä Dynamic search: {len(broad_search_results)} docs (k={dynamic_k}, confidence={confidence_level})")
            
            if not broad_search_results:
                return {
                    "type": "no_results",
                    "message": "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n.",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            logger.info(f"Found {len(broad_search_results)} candidate chunks")
            
            # Step 4: SEQUENTIAL PROCESSING ƒë·ªÉ t·ªëi ∆∞u VRAM (6GB limit)
            # Phase 1: Reranking - Load Reranker, Unload LLM n·∫øu c·∫ßn
            logger.info("üîÑ PHASE 1: Reranking (GPU) - Optimizing VRAM usage...")
            
            # Temporarily unload LLM ƒë·ªÉ ƒë·∫£m b·∫£o VRAM cho reranker
            if hasattr(self.llm_service, 'unload_model'):
                self.llm_service.unload_model()
            
            if settings.use_reranker and len(broad_search_results) > 1:
                # ‚úÖ ENHANCED RERANKING: Consensus-based document selection for better accuracy
                docs_to_rerank = broad_search_results  # RERANK ALL DOCUMENTS
                logger.info(f"üéØ ENHANCED RERANKING - Analyzing {len(broad_search_results)} candidates for consensus")
                
                if len(broad_search_results) >= 5:
                    # ‚úÖ NEW METHOD: Consensus-based document selection (more robust)
                    consensus_document = self.reranker_service.get_consensus_document(
                        query=query,
                        documents=docs_to_rerank,
                        top_k=5,  # Analyze top 5 candidates
                        consensus_threshold=0.6,  # 3/5 = 60%
                        min_rerank_score=-0.5  # Adjusted for legal documents
                    )
                    
                    if consensus_document:
                        nucleus_chunks = [consensus_document]
                        logger.info(f"‚úÖ CONSENSUS FOUND: Selected document based on chunk agreement")
                    else:
                        # Fallback to traditional single best document
                        logger.warning("‚ùå NO CONSENSUS: Falling back to traditional single best document")
                        nucleus_chunks = self.reranker_service.rerank_documents(
                            query=query,
                            documents=docs_to_rerank,
                            top_k=1,
                            router_confidence=routing_result.get('confidence', 0.0),
                            router_confidence_level=routing_result.get('confidence_level', 'low')
                        )
                else:
                    # Not enough candidates for consensus analysis
                    logger.info(f"INSUFFICIENT CANDIDATES ({len(broad_search_results)}) - Using traditional reranking")
                    nucleus_chunks = self.reranker_service.rerank_documents(
                        query=query,
                        documents=docs_to_rerank,
                        top_k=1,  # CH·ªà 1 nucleus chunk cao nh·∫•t - s·∫Ω expand to√†n b·ªô document ch·ª©a chunk n√†y
                        router_confidence=routing_result.get('confidence', 0.0),
                        router_confidence_level=routing_result.get('confidence_level', 'low')
                    )
                
                # Unload reranker sau khi ho√†n th√†nh ƒë·ªÉ gi·∫£i ph√≥ng VRAM
                if hasattr(self.reranker_service, 'unload_model'):
                    self.reranker_service.unload_model()
                
                # üö® INTELLIGENT CONFIDENCE CHECK - Ki·ªÉm tra COMBINED confidence tr∆∞·ªõc khi g·ªçi LLM
                router_confidence = routing_result.get('confidence', 0.0)
                best_score = nucleus_chunks[0].get('rerank_score', 0) if nucleus_chunks and len(nucleus_chunks) > 0 else 0.0
                
                # Calculate combined confidence score
                combined_confidence = (router_confidence * 0.4 + best_score * 0.6)  # Reranker c√≥ tr·ªçng s·ªë cao h∆°n
                logger.info(f"üéØ Combined Confidence: {combined_confidence:.4f} (Router: {router_confidence:.4f}, Rerank: {best_score:.4f})")
                
                # SMART CLARIFICATION THRESHOLD - Tr√°nh c√¢u tr·∫£ l·ªùi sai l·ªách
                CLARIFICATION_THRESHOLD = 0.3  # ƒêi·ªÅu ch·ªânh threshold n√†y theo c·∫ßn thi·∫øt
                
                if combined_confidence < CLARIFICATION_THRESHOLD:
                    logger.warning(f"üö® COMBINED CONFIDENCE QU√Å TH·∫§P ({combined_confidence:.4f} < {CLARIFICATION_THRESHOLD}) - K√≠ch ho·∫°t Smart Clarification")
                    
                    return self._generate_smart_clarification(routing_result, query, session_id, start_time)
                
                if nucleus_chunks and len(nucleus_chunks) > 0:
                    logger.info(f"Best rerank score: {best_score:.4f}")
                    logger.info("üéØ PURE RERANKER MODE - No protective logic, full expansion strategy")
            
                logger.info(f"Selected {len(nucleus_chunks)} nucleus chunk with rerank-based strategy")
            else:
                nucleus_chunks = broad_search_results[:1]  # Fallback: l·∫•y chunk t·ªët nh·∫•t theo vector similarity
                
            # Step 5: INTELLIGENT Context Expansion - ∆Øu ti√™n nucleus chunk + context li√™n quan
            expanded_context = None
            logger.info("üéØ INTELLIGENT CONTEXT EXPANSION - ∆Øu ti√™n nucleus chunk t·ª´ reranker")
            self.metrics["context_expansions"] += 1
            
            # üß† SMART OPTIMIZATION: ∆Øu ti√™n nucleus chunk + context li√™n quan thay v√¨ c·∫Øt ng·∫´u nhi√™n
            # Logic: Lu√¥n gi·ªØ nguy√™n nucleus chunk + th√™m context xung quanh n·∫øu c√≤n ch·ªó
            # Step 5: Context Expansion - THI·∫æT K·∫æ G·ªêC: FULL DOCUMENT
            logger.info("Context expansion: Loading TO√ÄN B·ªò DOCUMENT ƒë·ªÉ ƒë·∫£m b·∫£o ng·ªØ c·∫£nh ph√°p lu·∫≠t ƒë·∫ßy ƒë·ªß")
            
            expanded_context = self.context_expansion_service.expand_context_with_nucleus(
                nucleus_chunks=nucleus_chunks
            )
            
            context_text = self._build_context_from_expanded(expanded_context)
            logger.info(f"Context expanded: {expanded_context['total_length']} chars from {len(expanded_context.get('source_documents', []))} documents")
            
            # Phase 2: LLM Generation - Load LLM cho generation phase
            logger.info("üîÑ PHASE 2: LLM Generation (GPU) - Loading LLM for final answer...")
            
            # Step 6: Generate Answer (GPU LLM)
            if not session:
                return {
                    "type": "error",
                    "error": "Session not found",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            answer = self._generate_answer_with_context(
                query=query,
                context=context_text,
                session=session
            )
            
            # Update session history
            session.query_history.append({
                "query": query,
                "answer": answer,
                "timestamp": time.time(),
                "nucleus_chunks_count": len(nucleus_chunks),
                "context_length": len(context_text)
            })
            
            # Keep only last 5 queries in session (gi·∫£m t·ª´ 10 ƒë·ªÉ ti·∫øt ki·ªám memory)
            if len(session.query_history) > 5:
                session.query_history = session.query_history[-5:]
            
            # üî• Update session state for Stateful Router
            # Ch·ªâ update state khi routing th√†nh c√¥ng v·ªõi confidence ƒë·ªß t·ªët (0.78+)
            if routing_result and routing_result.get('confidence', 0) >= 0.78:
                target_collection = routing_result.get('target_collection')
                if target_collection:
                    rag_content = {
                        "context_text": context_text,
                        "nucleus_chunks": nucleus_chunks,
                        "expanded_context": expanded_context,
                        "collections": best_collections
                    }
                    session.update_successful_routing(
                        collection=target_collection, 
                        confidence=routing_result.get('confidence', 0),
                        filters=routing_result.get('inferred_filters', {}),  # üî• NEW: L∆∞u filters
                        rag_content=rag_content
                    )
                    logger.info(f"üî• Updated session state: {target_collection} (confidence: {routing_result.get('confidence', 0):.3f})")
                
            processing_time = time.time() - start_time
            self.metrics["avg_response_time"] = (
                (self.metrics["avg_response_time"] * (self.metrics["total_queries"] - 1) + processing_time) 
                / self.metrics["total_queries"]
            )
            
            return {
                "type": "answer",
                "answer": answer,
                "context_info": {
                    "nucleus_chunks": len(nucleus_chunks),
                    "context_length": len(context_text),
                    "source_collections": list(set(chunk.get("collection", "") for chunk in nucleus_chunks)),
                    "source_documents": list(expanded_context.get("source_documents", [])) if expanded_context else []
                },
                "context_details": {
                    "total_length": expanded_context.get("total_length", len(context_text)) if expanded_context else len(context_text),
                    "expansion_strategy": expanded_context.get("expansion_strategy", "unknown") if expanded_context else "no_expansion",
                    "source_documents": expanded_context.get("source_documents", []) if expanded_context else [],
                    "nucleus_chunks_count": len(nucleus_chunks)
                },
                "session_id": session_id,
                "processing_time": processing_time,
                "routing_info": {
                    "best_collections": best_collections,
                    "target_collection": routing_result.get('target_collection'),
                    "confidence": float(routing_result.get('confidence', 0.0)),
                    "original_confidence": float(routing_result.get('original_confidence', 0.0)) if routing_result.get('original_confidence') is not None else None,
                    "was_overridden": routing_result.get('was_overridden', False),
                    "inferred_filters": routing_result.get('inferred_filters', {}),
                    "confidence_level": routing_result.get('confidence_level', 'unknown'),
                    "status": routing_result.get('status', 'routed')
                }
            }
            
        except Exception as e:
            logger.error(f"Error in enhanced query: {e}")
            return {
                "type": "error",
                "error": str(e),
                "session_id": session_id,
                "processing_time": time.time() - start_time
            }
            
    def handle_clarification(
        self,
        session_id: str,
        selected_option: Dict[str, Any],  # üîß CHANGE: Nh·∫≠n full option object thay v√¨ string
        original_query: str
    ) -> Dict[str, Any]:
        """
        MULTI-TURN CONVERSATION: X·ª≠ l√Ω ph·∫£n h·ªìi clarification v·ªõi nhi·ªÅu giai ƒëo·∫°n
        - Giai ƒëo·∫°n 2: proceed_with_collection ‚Üí Generate document suggestions  
        - Giai ƒëo·∫°n 2.5: proceed_with_document ‚Üí Generate question suggestions within document
        - Giai ƒëo·∫°n 3: proceed_with_question ‚Üí Run RAG with clarified query
        """
        start_time = time.time()  # üîß ADD: Track processing time
        session = self.get_session(session_id)
        if not session:
            return {
                "type": "error", 
                "error": f"Session {session_id} not found",
                "session_id": session_id,
                "processing_time": 0.0
            }
            
        action = selected_option.get('action')
        collection = selected_option.get('collection')
        
        if action == 'proceed_with_collection' and collection:
            # üéØ GIAI ƒêO·∫†N 2: User ch·ªçn collection, hi·ªÉn th·ªã documents ƒë·ªÉ ch·ªçn
            logger.info(f"üéØ Clarification Step 2: User selected collection '{collection}'. Showing documents.")
            
            try:
                # L·∫•y danh s√°ch documents trong collection n√†y t·ª´ smart_router
                collection_questions = self.smart_router.get_example_questions_for_collection(collection)
                
                # Extract unique documents from questions
                collection_documents = {}
                for question in collection_questions:
                    source = question.get('source', '')
                    if source:
                        # Clean up source path to get document name
                        doc_name = source.replace('.json', '').split('/')[-1]
                        if '. ' in doc_name:
                            doc_name = doc_name.split('. ', 1)[1]  # Remove numbering
                        
                        if doc_name not in collection_documents:
                            collection_documents[doc_name] = {
                                "filename": source,
                                "title": doc_name,
                                "description": f"T√†i li·ªáu v·ªÅ {doc_name}",
                                "question_count": 0
                            }
                        collection_documents[doc_name]["question_count"] += 1
                
                if not collection_documents:
                    logger.warning(f"‚ö†Ô∏è No documents found in collection '{collection}'")
                    return {
                        "answer": f"Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o trong '{collection}'. Vui l√≤ng th·ª≠ l·∫°i.",
                        "type": "error",
                        "session_id": session_id,
                        "processing_time": time.time() - start_time
                    }
                
                # Convert to list and limit to top 8 documents
                document_list = list(collection_documents.values())
                document_list = sorted(document_list, key=lambda x: x["question_count"], reverse=True)[:8]
                
                # T·∫°o suggestions cho document selection
                document_suggestions = []
                for i, doc in enumerate(document_list):
                    document_suggestions.append({
                        "id": str(i + 1),
                        "title": doc['title'],
                        "description": f"T√†i li·ªáu: {doc['title']} ({doc['question_count']} c√¢u h·ªèi)",
                        "action": "proceed_with_document",
                        "collection": collection,
                        "document_filename": doc['filename'],
                        "document_title": doc['title']
                    })
                
                # T·∫°o clarification response cho document selection
                clarification_response = {
                    "message": f"B·∫°n ƒë√£ ch·ªçn '{collection}'. H√£y ch·ªçn t√†i li·ªáu c·ª• th·ªÉ:",
                    "options": document_suggestions,
                    "show_manual_input": True,
                    "manual_input_placeholder": "Ho·∫∑c nh·∫≠p c√¢u h·ªèi c·ª• th·ªÉ c·ªßa b·∫°n...",
                    "context": "document_selection",
                    "metadata": {
                        "collection": collection,
                        "available_documents": document_list,
                        "stage": "document_selection"
                    }
                }
                
                # Update session state
                session.metadata["routing_state"] = {
                    "collection": collection,
                    "available_documents": document_list,
                    "stage": "document_selection"
                }
                self.chat_sessions[session_id] = session
                
                return {
                    "answer": clarification_response["message"],
                    "clarification": clarification_response,
                    "collection": collection,
                    "documents": document_list,
                    "type": "clarification_needed",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            except Exception as e:
                logger.error(f"‚ùå Error in document selection: {e}")
                return {
                    "answer": f"C√≥ l·ªói khi t·∫£i danh s√°ch t√†i li·ªáu trong '{collection}'. Vui l√≤ng th·ª≠ l·∫°i.",
                    "type": "error",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
        
        if action == 'proceed_with_document' and collection:
            # üéØ GIAI ƒêO·∫†N 2.5: User ch·ªçn document, generate question suggestions trong document ƒë√≥
            document_filename = selected_option.get('document_filename')
            document_title = selected_option.get('document_title')
            
            logger.info(f"üéØ Clarification Step 2.5: User selected document '{document_title}' in collection '{collection}'. Generating question suggestions.")
            
            try:
                # L·∫•y t·∫•t c·∫£ questions trong collection v√† filter theo document
                collection_questions = self.smart_router.get_example_questions_for_collection(collection)
                
                # Filter questions by document source
                document_questions = []
                for question in collection_questions:
                    if question.get('source') and document_filename in question.get('source', ''):
                        document_questions.append(question)
                
                if not document_questions:
                    logger.warning(f"‚ö†Ô∏è No questions found for document {document_title}")
                    # Fallback: Use all collection questions
                    document_questions = collection_questions[:5]
                
                # Create suggestions from document questions
                suggestions = []
                for i, q in enumerate(document_questions[:5]):
                    question_text = q.get('text', str(q)) if isinstance(q, dict) else str(q)
                    suggestions.append({
                        "id": str(i + 1),
                        "title": question_text,
                        "description": f"C√¢u h·ªèi v·ªÅ {document_title}",
                        "action": "proceed_with_question",
                        "collection": collection,
                        "document_filename": document_filename,
                        "document_title": document_title,
                        "question_text": question_text,
                        "source_file": q.get('source', '') if isinstance(q, dict) else '',
                        "category": q.get('category', 'general') if isinstance(q, dict) else 'general'
                    })
                
                # Add manual input option
                suggestions.append({
                    "id": str(len(suggestions) + 1),
                    "title": "C√¢u h·ªèi kh√°c...",
                    "description": f"T√¥i mu·ªën h·ªèi v·ªÅ v·∫•n ƒë·ªÅ kh√°c trong {document_title}",
                    "action": "manual_input",
                    "collection": collection,
                    "document_filename": document_filename,
                    "document_title": document_title
                })
                
                collection_display = self.smart_router.collection_mappings.get(collection, {}).get('display_name', collection.replace('_', ' ').title())
                
                clarification_response = {
                    "message": f"B·∫°n ƒë√£ ch·ªçn t√†i li·ªáu '{document_title}'. H√£y ch·ªçn c√¢u h·ªèi ph√π h·ª£p:",
                    "options": suggestions,
                    "show_manual_input": True,
                    "manual_input_placeholder": f"Ho·∫∑c nh·∫≠p c√¢u h·ªèi c·ª• th·ªÉ v·ªÅ {document_title}...",
                    "context": "question_selection",
                    "metadata": {
                        "collection": collection,
                        "document_filename": document_filename,
                        "document_title": document_title,
                        "stage": "question_selection"
                    }
                }
                
                # Update session state
                session.metadata["routing_state"] = {
                    "collection": collection,
                    "document_filename": document_filename,
                    "document_title": document_title,
                    "stage": "question_selection"
                }
                self.chat_sessions[session_id] = session
                
                return {
                    "answer": clarification_response["message"],
                    "clarification": clarification_response,
                    "collection": collection,
                    "document_title": document_title,
                    "type": "clarification_needed",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
            except Exception as e:
                logger.error(f"‚ùå Error in document question generation: {e}")
                return {
                    "answer": f"C√≥ l·ªói khi t·∫£i c√¢u h·ªèi cho '{document_title}'. Vui l√≤ng th·ª≠ l·∫°i.",
                    "type": "error",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
                
        elif action == 'proceed_with_question':
            # üéØ GIAI ƒêO·∫†N 3 ‚Üí 4: User ch·ªçn c√¢u h·ªèi c·ª• th·ªÉ, ch·∫°y RAG
            question_text = selected_option.get('question_text')
            document_title = selected_option.get('document_title')  # üî• NEW: Get exact document title
            source_file = selected_option.get('source_file')  # üî• NEW: For debugging
            
            if question_text and collection:
                logger.info(f"üöÄ Clarification Step 3‚Üí4: User selected question '{question_text}' in collection '{collection}'.")
                if document_title:
                    logger.info(f"üéØ Target document: '{document_title}' (source: {source_file})")
                
                # Ch·∫°y RAG v·ªõi c√¢u h·ªèi ƒê√É ƒê∆Ø·ª¢C L√ÄM R√ï v√† collection ƒê√É CH·ªà ƒê·ªäNH
                return self.enhanced_query(
                    query=question_text,  # üî• D√πng c√¢u h·ªèi c·ª• th·ªÉ, kh√¥ng ph·∫£i original query m∆° h·ªì
                    session_id=session_id,
                    forced_collection=collection,  # üî• Force routing to selected collection
                    forced_document_title=document_title  # üî• NEW: Force exact document filtering
                )
            else:
                return {
                    "type": "error",
                    "error": "Missing question_text or collection for proceed_with_question action",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time
                }
            
        elif action == 'manual_input':
            # üîß IMPROVED: Manual input v·ªõi context preservation
            logger.info(f"üîÑ Manual input requested by user. Preserving valuable context.")
            
            # ‚úÖ SMART CONTEXT PRESERVATION: Gi·ªØ context c√≥ gi√° tr·ªã thay v√¨ clear all
            original_routing = session.metadata.get('original_routing_context', {})
            selected_collection = selected_option.get('collection')  # Collection user ƒë√£ ch·ªçn
            selected_document = selected_option.get('document_filename')  # Document user ƒë√£ ch·ªçn (if any)
            document_title = selected_option.get('document_title')  # Document title (if any)
            
            # Determine context to preserve based on conversation stage
            if selected_document and selected_collection:
                # Case 2: User ƒë√£ ch·ªçn document ‚Üí Preserve document-level context
                logger.info(f"üîÑ CASE 2: Preserving document context: {document_title} in {selected_collection}")
                session.last_successful_collection = selected_collection
                session.last_successful_confidence = original_routing.get('confidence', 0.7)
                session.last_successful_timestamp = time.time()
                session.last_successful_filters = original_routing.get('inferred_filters', {})
                
                # Also preserve document-specific context
                session.metadata['preserved_document'] = {
                    'filename': selected_document,
                    'title': document_title,
                    'collection': selected_collection
                }
                
                # Clear only metadata v·ªÅ clarification process
                session.metadata.pop('original_routing_context', None)
                session.metadata.pop('original_query', None)
                
                return {
                    "type": "manual_input_request",
                    "message": f"Vui l√≤ng nh·∫≠p c√¢u h·ªèi c·ª• th·ªÉ v·ªÅ '{document_title}'. T√¥i s·∫Ω t√¨m ki·∫øm trong t√†i li·ªáu n√†y.",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time,
                    "context_preserved": True,
                    "preserved_collection": selected_collection,
                    "preserved_document": document_title
                }
                
            elif selected_collection:
                # Case 1: User ƒë√£ ch·ªçn collection ‚Üí Preserve collection context
                logger.info(f"üîÑ CASE 1: Preserving collection context: {selected_collection}")
                session.last_successful_collection = selected_collection
                session.last_successful_confidence = original_routing.get('confidence', 0.7)
                session.last_successful_timestamp = time.time()
                session.last_successful_filters = original_routing.get('inferred_filters', {})
                
                # Clear only metadata v·ªÅ clarification process
                session.metadata.pop('original_routing_context', None)
                session.metadata.pop('original_query', None)
                
                return {
                    "type": "manual_input_request",
                    "message": f"Vui l√≤ng nh·∫≠p l·∫°i c√¢u h·ªèi c·ª• th·ªÉ h∆°n v·ªÅ '{selected_collection}'. T√¥i s·∫Ω t√¨m ki·∫øm trong lƒ©nh v·ª±c n√†y.",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time,
                    "context_preserved": True,
                    "preserved_collection": selected_collection
                }
            else:
                # Kh√¥ng c√≥ collection context ‚Üí Clear session (fallback)
                logger.info(f"üîÑ No collection context to preserve, clearing session state.")
                session.clear_routing_state()
                session.metadata.clear()
                
                return {
                    "type": "manual_input_request",
                    "message": "Vui l√≤ng nh·∫≠p l·∫°i c√¢u h·ªèi c·ª• th·ªÉ h∆°n. T√¥i s·∫Ω t√¨m ki·∫øm trong ng·ªØ c·∫£nh ph√π h·ª£p.",
                    "session_id": session_id,
                    "processing_time": time.time() - start_time,
                    "context_preserved": False
                }
            
            # ‚úÖ Update session access time  
            session.last_accessed = time.time()
            
        else:
            # Invalid action
            return {
                "type": "error",
                "error": f"Invalid clarification action: {action}",
                "session_id": session_id, 
                "processing_time": 0.0
            }
        
    def _build_context_from_expanded(self, expanded_context: Dict[str, Any]) -> str:
        """Build context string t·ª´ expanded context"""
        context_parts = []
        
        for doc_content in expanded_context.get("expanded_content", []):
            source = doc_content.get("source", "N/A")
            text = doc_content.get("text", "")
            chunk_count = doc_content.get("chunk_count", 0)
            
            context_parts.append(f"=== T√†i li·ªáu: {source} ({chunk_count} ƒëo·∫°n) ===\n{text}")
            
        return "\n\n".join(context_parts)
        
    def _generate_answer_with_context(
        self,
        query: str,
        context: str,
        session: OptimizedChatSession
    ) -> str:
        """Generate answer v·ªõi context v√† session history s·ª≠ d·ª•ng ChatML format"""
        
        # CHU·∫®N B·ªä CHAT HISTORY C√ì C·∫§U TR√öC cho ChatML template
        chat_history_structured = []
        if len(session.query_history) > 0:
            # üöÄ PERFORMANCE OPTIMIZATION: Ch·ªâ l·∫•y 1 l∆∞·ª£t h·ªèi-ƒë√°p g·∫ßn nh·∫•t ƒë·ªÉ gi·∫£m prompt length
            recent_queries = session.query_history[-1:]  # Only last 1 query thay v√¨ 3
            logger.info(f"‚ö° Chat history: {len(recent_queries)} entries (optimized for speed)")
            
            for item in recent_queries:
                chat_history_structured.append({"role": "user", "content": item['query']})
                # R√∫t g·ªçn answer ƒë·ªÉ tr√°nh context overflow
                answer_preview = item['answer'][:100] + "..." if len(item['answer']) > 100 else item['answer']
                chat_history_structured.append({"role": "assistant", "content": answer_preview})
            
        # ALWAYS use FULL system prompt - No conservative strategy
        system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam.

üö® QUY T·∫ÆC B·∫ÆT BU·ªòC - KH√îNG ƒê∆Ø·ª¢C VI PH·∫†M:
1. CH·ªà tr·∫£ l·ªùi d·ª±a CH√çNH X√ÅC tr√™n th√¥ng tin C√ì TRONG t√†i li·ªáu
2. Tr·∫£ l·ªùi NG·∫ÆN G·ªåN (t·ªëi ƒëa 9-10 c√¢u)
3. KH√îNG t·ª± s√°ng t·∫°o th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu
4. N·∫øu th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu, h√£y tr·∫£ l·ªùi: "T√†i li·ªáu kh√¥ng ƒë·ªÅ c·∫≠p ƒë·∫øn v·∫•n ƒë·ªÅ n√†y."

üéØ C√ÅC LO·∫†I TH√îNG TIN QUAN TR·ªåNG C·∫¶N CH√ö √ù:
- PH√ç/L·ªÜ PH√ç: T√¨m "fee_text", "fee_vnd" - n√™u r√µ mi·ªÖn ph√≠ ho·∫∑c s·ªë ti·ªÅn c·ª• th·ªÉ
- TH·ªúI GIAN: T√¨m "processing_time_text" - n√™u r√µ th·ªùi gian x·ª≠ l√Ω
- C∆† QUAN: T√¨m "executing_agency" - n√™u r√µ n∆°i th·ª±c hi·ªán th·ªß t·ª•c  
- FORM M·∫™U: T√¨m "has_form" - n√™u c√≥/kh√¥ng c√≥ form m·∫´u
- ƒêI·ªÄU KI·ªÜN: T√¨m "requirements_conditions" - n√™u ƒëi·ªÅu ki·ªán c·∫ßn ƒë√°p ·ª©ng
- M√É TH·ª¶ T·ª§C: T√¨m "code" - m√£ quy tr√¨nh

ƒê·ªäNH D·∫†NG TR·∫¢ L·ªúI:
- C√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c
- ∆Øu ti√™n th√¥ng tin user h·ªèi nh∆∞ng c√≥ th·ªÉ b·ªï sung th√¥ng tin li√™n quan
- D·∫´n ch·ª©ng t·ª´ t√†i li·ªáu n·∫øu c√≥"""
        
        logger.info(f"üìù Using ChatML format with structured chat history: {len(chat_history_structured)} messages")
        
        # üî• TOKEN MANAGEMENT - Ki·ªÉm so√°t ƒë·ªô d√†i ƒë·ªÉ tr√°nh context overflow
        from app.core.config import settings
        
        # ∆Ø·ªõc t√≠nh token ƒë∆°n gi·∫£n (1 token ‚âà 3-4 k√Ω t·ª± ti·∫øng Vi·ªát)
        # T√≠nh to√°n cho ChatML format v·ªõi c√°c token ƒë·∫∑c bi·ªát
        chat_history_text = "\n".join([f"{item['role']}: {item['content']}" for item in chat_history_structured])
        estimated_tokens = len(system_prompt + context + query + chat_history_text + "<|im_start|><|im_end|>") // 3
        max_context_tokens = settings.n_ctx - 500  # ƒê·ªÉ l·∫°i 500 token cho response
        
        if estimated_tokens > max_context_tokens:
            # C·∫Øt b·ªõt context ƒë·ªÉ fit trong gi·ªõi h·∫°n
            logger.warning(f"üö® Context overflow detected: {estimated_tokens} tokens > {max_context_tokens} max")
            
            # T√≠nh to√°n space c√≤n l·∫°i cho context
            fixed_parts_length = len(system_prompt + chat_history_text + query + "<|im_start|><|im_end|>")
            remaining_space = (max_context_tokens * 3) - fixed_parts_length
            
            if remaining_space > 500:  # ƒê·∫£m b·∫£o c√≥ √≠t nh·∫•t 500 k√Ω t·ª± cho context
                context = context[:remaining_space] + "\n\n[...TH√îNG TIN ƒê√É ƒê∆Ø·ª¢C R√öT G·ªåN ƒê·ªÇ TR√ÅNH QU√Å T·∫¢I...]"
                logger.info(f"‚úÇÔ∏è Context truncated to {len(context)} chars")
            else:
                # N·∫øu kh√¥ng ƒë·ªß ch·ªó, b·ªè chat history
                chat_history_structured = []
                context = context[:max_context_tokens * 3 // 2] + "\n\n[...R√öT G·ªåN...]"
                logger.warning("‚ö†Ô∏è Removed chat history due to extreme context overflow")
        
        logger.info(f"üìù Final context length: {len(context)} chars (~{len(context)//3} tokens)")

        try:
            response_data = self.llm_service.generate_response(
                user_query=query,
                context=context,
                max_tokens=settings.max_tokens,
                temperature=settings.temperature,
                system_prompt=system_prompt,
                chat_history=chat_history_structured  # üî• THAM S·ªê M·ªöI cho ChatML
            )
            
            # Extract response text from dict
            if isinstance(response_data, dict) and "response" in response_data:
                return response_data["response"].strip()
            elif isinstance(response_data, str):
                return response_data.strip()
            else:
                return str(response_data).strip()
            
        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            return f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t·∫°o c√¢u tr·∫£ l·ªùi: {e}"
            
    def get_health_status(self) -> Dict[str, Any]:
        """Tr·∫°ng th√°i health c·ªßa service"""
        try:
            collections = self.vectordb_service.list_collections()
            total_documents = 0
            
            for collection_info in collections:
                try:
                    collection = self.vectordb_service.get_collection(collection_info["name"])
                    count = collection.count()
                    total_documents += count
                except:
                    continue
                    
            return {
                "status": "healthy",
                "total_collections": len(collections),
                "total_documents": total_documents,
                "llm_loaded": self.llm_service.model is not None,
                "reranker_loaded": self.reranker_service.model is not None,
                "embedding_device": "CPU (VRAM optimized)",
                "llm_device": "GPU",
                "reranker_device": "GPU",
                "active_sessions": len(self.chat_sessions),
                "metrics": self.metrics,
                "router_stats": self.smart_router.get_collection_info(),
                "context_expansion": {
                    "total_chunks_cached": len(self.context_expansion_service.document_metadata_cache),
                    **self.context_expansion_service.get_stats()
                }
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
            
    def cleanup_old_sessions(self, max_age_hours: int = 24):
        """D·ªçn d·∫πp sessions c≈©"""
        current_time = time.time()
        cutoff_time = current_time - (max_age_hours * 3600)
        
        old_sessions = [
            session_id for session_id, session in self.chat_sessions.items()
            if session.last_accessed < cutoff_time
        ]
        
        for session_id in old_sessions:
            del self.chat_sessions[session_id]
            
        if old_sessions:
            logger.info(f"Cleaned up {len(old_sessions)} old sessions")
            
        return len(old_sessions)

    # API Compatibility Methods
    def query(self, question: Optional[str] = None, query: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """Compatibility method cho API routes"""
        # H·ªó tr·ª£ c·∫£ 'question' v√† 'query' parameters
        query_text = question or query
        if not query_text:
            raise ValueError("Either 'question' or 'query' parameter is required")
        return self.enhanced_query(query_text, **kwargs)
    
    @property
    def query_router(self):
        """Compatibility property cho API routes"""
        # T·∫°o wrapper v·ªõi explain_routing method
        class RouterWrapper:
            def __init__(self, smart_router):
                self.smart_router = smart_router
                # Copy t·∫•t c·∫£ methods t·ª´ smart_router
                for attr in dir(smart_router):
                    if not attr.startswith('_') and callable(getattr(smart_router, attr)):
                        setattr(self, attr, getattr(smart_router, attr))
            
            def explain_routing(self, question: str) -> Dict[str, Any]:
                """Explain routing decision cho question"""
                try:
                    # S·ª≠ d·ª•ng smart router ƒë·ªÉ classify
                    route_result = self.smart_router.route_query(question)
                    return {
                        'question': question,
                        'route': route_result.get('route_name', 'general'),
                        'confidence': route_result.get('confidence', 0.0),
                        'reasoning': route_result.get('reasoning', 'No reasoning available'),
                        'suggested_collections': route_result.get('suggested_collections', [])
                    }
                except Exception as e:
                    logger.error(f"Error explaining routing: {e}")
                    return {
                        'question': question,
                        'route': 'general',
                        'confidence': 0.0,
                        'reasoning': f'Error: {str(e)}',
                        'suggested_collections': []
                    }
        
        return RouterWrapper(self.smart_router)

    def build_index(self, collection_name: Optional[str] = None, force_rebuild: bool = False, **kwargs) -> Dict[str, Any]:
        """Build index cho collection ho·∫∑c t·∫•t c·∫£ collections"""
        try:
            if collection_name:
                # Build specific collection
                if self.vectordb_service.collection_exists(collection_name):
                    stats = self.vectordb_service.get_collection_stats(collection_name)
                    return {
                        'status': 'success',
                        'collections_processed': 1,
                        'collection_name': collection_name,
                        'message': f'Collection {collection_name} already exists and ready',
                        'document_count': stats.get('document_count', 0)
                    }
                else:
                    return {
                        'status': 'error',
                        'collections_processed': 0,
                        'collection_name': collection_name,
                        'error': f'Collection {collection_name} does not exist',
                        'suggestion': 'Run python tools/2_build_vectordb_final.py to build collections'
                    }
            else:
                # Build all collections - return info about existing ones
                collections = self.vectordb_service.list_collections()
                return {
                    'status': 'success',
                    'collections_processed': len(collections),
                    'message': f'Found {len(collections)} existing collections',
                    'collections': [col['name'] for col in collections],
                    'total_documents': sum(col.get('document_count', 0) for col in collections),
                    'suggestion': 'Use python tools/2_build_vectordb_final.py to build new collections from documents'
                }
        except Exception as e:
            logger.error(f"Error in build_index: {e}")
            return {
                'status': 'error',
                'collections_processed': 0,
                'error': str(e)
            }
    
    def _generate_smart_clarification(self, routing_result: Dict[str, Any], query: str, session_id: str, start_time: float) -> Dict[str, Any]:
        """T·∫°o clarification th√¥ng minh d·ª±a tr√™n confidence level"""
        try:
            # G·ªçi Smart Clarification Service ƒë·ªÉ t·∫°o clarification th√¥ng minh
            clarification_service = SmartClarificationService()
            clarification_response = clarification_service.generate_clarification(
                query=query,
                confidence=routing_result.get('confidence', 0.0),
                routing_result=routing_result
            )
            
            # Merge clarification response with required fields
            processing_time = time.time() - start_time
            
            # Get the main response from clarification service
            response = clarification_response.copy()
            
            # Add required fields that API expects
            response.update({
                "session_id": session_id,
                "processing_time": processing_time,
                "routing_info": {
                    "target_collection": routing_result.get('target_collection'),
                    "router_confidence": routing_result.get('confidence', 0.0),
                    "status": "smart_clarification"
                }
            })
            
            # üîß STORE ROUTING CONTEXT: Save original routing info to session for Step 2‚Üí3 similarity matching
            session = self.get_session(session_id)
            if session:
                session.metadata['original_routing_context'] = routing_result
                session.metadata['original_query'] = query
                logger.info(f"üíæ Stored original routing context for session {session_id}")
            
            return convert_numpy_types(response)
            
        except Exception as e:
            logger.error(f"Error generating smart clarification: {e}")
            processing_time = time.time() - start_time
            
            fallback_response = {
                "type": "clarification_needed",
                "confidence": routing_result.get('confidence', 0.0),
                "clarification": {
                    "message": "Xin l·ªói, t√¥i kh√¥ng r√µ √Ω ƒë·ªãnh c·ªßa c√¢u h·ªèi. B·∫°n c√≥ th·ªÉ di·ªÖn ƒë·∫°t r√µ h∆°n kh√¥ng?",
                    "options": [
                        {
                            'id': 'retry',
                            'title': "H√£y di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi",
                            'description': "T√¥i s·∫Ω c·ªë g·∫Øng hi·ªÉu r√µ h∆°n",
                            'action': 'manual_input'
                        }
                    ],
                    "style": "fallback"
                },
                "session_id": session_id,
                "processing_time": processing_time,
                "routing_info": {
                    "target_collection": routing_result.get('target_collection'),
                    "router_confidence": routing_result.get('confidence', 0.0),
                    "status": "smart_clarification_error",
                    "error": str(e)
                }
            }
            return convert_numpy_types(fallback_response)
    
    def _activate_vector_backup_strategy(self, routing_result: Dict[str, Any], query: str, session_id: str, start_time: float) -> Dict[str, Any]:
        """K√≠ch ho·∫°t Vector Backup Strategy khi Smart Router ho√†n to√†n th·∫•t b·∫°i"""
        try:
            logger.info("üö® Activating Vector Backup Strategy - searching across all collections")
            
            # Th·ª±c hi·ªán vector search tr·ª±c ti·∫øp tr√™n t·∫•t c·∫£ collections ƒë·ªÉ t√¨m topics li√™n quan
            all_collections = self.vectordb_service.list_collections()
            backup_results = []
            
            for collection_info in all_collections[:3]:  # Limit to top 3 collections for performance
                collection_name = collection_info["name"]
                try:
                    collection = self.vectordb_service.get_collection(collection_name)
                    search_results = self.vectordb_service.search_in_collection(
                        collection_name,
                        query,
                        top_k=2,  # Ch·ªâ l·∫•y top 2 results per collection
                        similarity_threshold=0.3,
                        where_filter={}
                    )
                    
                    if search_results:
                        best_result = search_results[0]
                        backup_results.append({
                            'collection': collection_name,
                            'score': best_result.get('similarity', best_result.get('score', 0)),
                            'content': best_result.get('content', best_result.get('document', ''))[:200] + "...",
                            'metadata': best_result.get('metadata', {}),
                            'source': best_result.get('metadata', {}).get('source', 'N/A')
                        })
                        
                except Exception as e:
                    logger.warning(f"Error searching collection {collection_name}: {e}")
                    continue
            
            # Sort by score v√† t·∫°o suggestions
            backup_results.sort(key=lambda x: x['score'], reverse=True)
            
            options = []
            for i, result in enumerate(backup_results[:3], 1):
                # Tr√≠ch xu·∫•t title t·ª´ metadata n·∫øu c√≥
                metadata = result.get('metadata', {})
                title = metadata.get('document_title', metadata.get('title', f"Th·ªß t·ª•c {result['collection']}"))
                
                option = {
                    'id': str(i),
                    'title': title,
                    'description': f"ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng: {result['score']:.2f} - {result['content']}",
                    'collection': result['collection'],
                    'backup_score': result['score']
                }
                options.append(option)
            
            clarification_msg = "T√¥i kh√¥ng t√¨m th·∫•y c√¢u h·ªèi m·∫´u ph√π h·ª£p, nh∆∞ng d·ª±a tr√™n t√¨m ki·∫øm trong d·ªØ li·ªáu, c√¢u h·ªèi c·ªßa b·∫°n c√≥ th·ªÉ li√™n quan ƒë·∫øn:"
            
            if not options:
                clarification_msg = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p. B·∫°n c√≥ th·ªÉ th·ª≠ v·ªõi t·ª´ kh√≥a kh√°c kh√¥ng?"
            
            logger.info(f"Vector backup strategy found {len(options)} potential matches")
            
            return {
                "type": "clarification_needed",
                "status": "vector_backup",
                "confidence": routing_result.get('confidence', 0.0),
                "clarification": clarification_msg,
                "options": options,
                "backup_results": len(backup_results),
                "session_id": session_id,
                "processing_time": time.time() - start_time,
                "strategy": "vector_backup"
            }
            
        except Exception as e:
            logger.error(f"Error in vector backup strategy: {e}")
            return {
                "type": "clarification_needed",
                "status": "fallback_error", 
                "clarification": "Xin l·ªói, c√≥ l·ªói h·ªá th·ªëng khi x·ª≠ l√Ω c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i sau.",
                "session_id": session_id,
                "processing_time": time.time() - start_time
            }
    
    @property  
    def document_processor(self):
        """Compatibility property cho API routes"""
        import os
        
        class DocumentProcessorCompat:
            def get_available_collections(self, documents_dir):
                """L·∫•y danh s√°ch collections c√≥ th·ªÉ t·∫°o t·ª´ documents"""
                try:
                    if not os.path.exists(documents_dir):
                        return []
                    
                    collections = []
                    for item in os.listdir(documents_dir):
                        item_path = os.path.join(documents_dir, item)
                        if os.path.isdir(item_path):
                            # ƒê·∫øm s·ªë files PDF trong th∆∞ m·ª•c
                            pdf_count = len([f for f in os.listdir(item_path) 
                                           if f.lower().endswith('.pdf')])
                            if pdf_count > 0:
                                collections.append({
                                    'name': item,
                                    'path': item_path,
                                    'document_count': pdf_count
                                })
                    return collections
                except Exception as e:
                    logger.error(f"Error getting available collections: {e}")
                    return []
        
        return DocumentProcessorCompat()
