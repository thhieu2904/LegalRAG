"""
Enhanced Context Expansion Service
Sá»­ dá»¥ng "Nucleus Chunk" strategy Ä‘á»ƒ má»Ÿ rá»™ng ngá»¯ cáº£nh hiá»‡u quáº£
"""

import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Set
import json

logger = logging.getLogger(__name__)

class EnhancedContextExpansionService:
    """Service má»Ÿ rá»™ng ngá»¯ cáº£nh vá»›i Nucleus Chunk strategy"""
    
    def __init__(self, vectordb_service, documents_dir: str):
        self.vectordb_service = vectordb_service
        self.documents_dir = Path(documents_dir)
        
        # Cache metadata cá»§a documents
        self.document_metadata_cache = {}
        self._build_document_metadata_cache()
    
    def _build_document_metadata_cache(self):
        """XÃ¢y dá»±ng cache metadata Ä‘á»ƒ map chunk -> document"""
        try:
            # Láº¥y táº¥t cáº£ collections
            collections = self.vectordb_service.list_collections()
            
            for collection_info in collections:
                collection_name = collection_info["name"]
                collection = self.vectordb_service.get_collection(collection_name)
                
                # Láº¥y táº¥t cáº£ documents trong collection
                try:
                    results = collection.get()
                    
                    for i, chunk_id in enumerate(results["ids"]):
                        metadata = results.get("metadatas", [{}])[i] if i < len(results.get("metadatas", [])) else {}
                        
                        if metadata and "source" in metadata:
                            source_file = metadata["source"]
                            
                            # Build cache entry
                            self.document_metadata_cache[chunk_id] = {
                                "source_file": source_file,
                                "collection": collection_name,
                                "metadata": metadata,
                                "chunk_index": metadata.get("chunk_index", 0)
                            }
                            
                except Exception as e:
                    logger.warning(f"Could not process collection {collection_name}: {e}")
                    
            logger.info(f"Built metadata cache for {len(self.document_metadata_cache)} chunks")
            
        except Exception as e:
            logger.error(f"Error building document metadata cache: {e}")
    
    def expand_context_with_nucleus(
        self,
        nucleus_chunks: List[Dict[str, Any]], 
        max_context_length: int = 3000,
        include_full_document: bool = True
    ) -> Dict[str, Any]:
        """
        Má»Ÿ rá»™ng ngá»¯ cáº£nh dá»±a trÃªn nucleus chunks - STRATEGY: 1 CHUNK â†’ TOÃ€N Bá»˜ DOCUMENT
        
        Flow tá»‘i Æ°u:
        1. Láº¥y 1 nucleus chunk vá»›i rerank score cao nháº¥t
        2. TÃ¬m source file JSON chá»©a chunk Ä‘Ã³  
        3. Load toÃ n bá»™ ná»™i dung document tá»« file JSON gá»‘c
        4. Return full document content thay vÃ¬ chá»‰ 1 chunk
        
        Args:
            nucleus_chunks: List chunks Ä‘Ã£ rerank (thÆ°á»ng chá»‰ 1 chunk cao nháº¥t)
            max_context_length: Äá»™ dÃ i context tá»‘i Ä‘a (kÃ½ tá»±)
            include_full_document: True = láº¥y toÃ n bá»™ document, False = chá»‰ chunks liá»n ká»
            
        Returns:
            Expanded context vá»›i toÃ n bá»™ document content vÃ  metadata
        """
        try:
            expanded_context = {
                "nucleus_chunks": nucleus_chunks,
                "expanded_content": [],
                "source_documents": [],
                "total_length": 0,
                "expansion_strategy": "single_nucleus_full_document"
            }
            
            # CHá»ˆ Xá»¬ LÃ 1 NUCLEUS CHUNK Äáº¦U TIÃŠN (chunk cÃ³ rerank score cao nháº¥t)
            if not nucleus_chunks:
                logger.warning("No nucleus chunks provided")
                return expanded_context
                
            nucleus_chunk = nucleus_chunks[0]  # Láº¥y chunk cao nháº¥t sau rerank
            logger.info(f"Processing nucleus chunk with ID: {nucleus_chunk.get('id', 'N/A')}")
            
            # TÃ¬m source file JSON tá»« nucleus chunk metadata
            source_file = None
            
            # Thá»­ nhiá»u cÃ¡ch Ä‘á»ƒ tÃ¬m source file
            if "source" in nucleus_chunk and "file_path" in nucleus_chunk["source"]:
                source_file = nucleus_chunk["source"]["file_path"]
            elif "metadata" in nucleus_chunk and "source" in nucleus_chunk["metadata"]:
                source_file = nucleus_chunk["metadata"]["source"].get("file_path")
            
            if not source_file:
                logger.warning("Could not find source file for nucleus chunk")
                return expanded_context
                
            logger.info(f"Found source file: {source_file}")
            logger.info("Loading FULL DOCUMENT content (not just chunks)")
            
            # QUAN TRá»ŒNG: Load toÃ n bá»™ document gá»‘c tá»« file JSON thay vÃ¬ chá»‰ láº¥y chunks
            full_document_content = self._load_full_document(source_file)
            
            if full_document_content:
                # Giá»›i háº¡n context length
                if len(full_document_content) > max_context_length:
                    # Truncate nhÆ°ng giá»¯ pháº§n Ä‘áº§u vÃ  thÃ´ng tin quan trá»ng
                    full_document_content = full_document_content[:max_context_length] + "..."
                
                expanded_context["expanded_content"] = [{
                    "text": full_document_content,
                    "source": source_file,
                    "document_title": nucleus_chunk.get("source", {}).get("document_title", ""),
                    "type": "full_document"
                }]
                expanded_context["source_documents"] = [source_file]
                expanded_context["total_length"] = len(full_document_content)
                
                logger.info(f"Expanded context: {len(full_document_content)} chars from 1 document")
            else:
                logger.warning("Could not load full document content")
            
            return expanded_context
            
        except Exception as e:
            logger.error(f"Error in context expansion: {e}")
            # Fallback: return nucleus chunks as-is
            return {
                "nucleus_chunks": nucleus_chunks,
                "expanded_content": [{"text": chunk.get("content", ""), "source": "fallback", "type": "chunk_fallback"} for chunk in nucleus_chunks],
                "source_documents": [],
                "total_length": sum(len(chunk.get("content", "")) for chunk in nucleus_chunks),
                "expansion_strategy": "fallback"
            }
    
    def _load_full_document(self, file_path: str) -> str:
        """
        Load ná»™i dung document cÃ³ chá»n lá»c theo cÃ¢u há»i Ä‘á»ƒ trÃ¡nh overload LLM
        STRATEGY: Thay vÃ¬ load toÃ n bá»™ document, chá»‰ load nhá»¯ng pháº§n liÃªn quan
        """
        try:
            import json
            from pathlib import Path
            
            if not Path(file_path).exists():
                logger.warning(f"Source file not found: {file_path}")
                return ""
                
            logger.info(f"Loading selective document content from: {file_path}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            # Build selective document content - Æ¯U TIÃŠN THÃ”NG TIN QUAN TRá»ŒNG
            metadata = json_data.get('metadata', {})
            content_chunks = json_data.get('content_chunks', [])
            
            # Táº¡o content vá»›i thÃ´ng tin TÃ“M Táº®T vÃ  TRá»ŒNG TÃ‚M
            essential_parts = []
            
            # HEADER - ThÃ´ng tin cá»‘t lÃµi
            if metadata.get('title'):
                essential_parts.append(f"ðŸ“‹ TIÃŠU Äá»€: {metadata['title']}")
            
            if metadata.get('executing_agency'):
                essential_parts.append(f"ðŸ¢ CÆ  QUAN THá»°C HIá»†N: {metadata['executing_agency']}")
                
            if metadata.get('processing_time_text'):
                essential_parts.append(f"â° THá»œI GIAN Xá»¬ LÃ: {metadata['processing_time_text']}")
            
            # QUAN TRá»ŒNG NHáº¤T: ThÃ´ng tin vá» PHÃ/Lá»† PHÃ Ä‘Æ°á»£c Æ°u tiÃªn hÃ ng Ä‘áº§u
            if metadata.get('fee_text'):
                essential_parts.append(f"ðŸ’° THÃ”NG TIN PHÃ/Lá»† PHÃ:")
                essential_parts.append(f"   {metadata['fee_text']}")
                
            essential_parts.append("=" * 60)
            
            # CONTENT CHUNKS - Chá»‰ láº¥y nhá»¯ng pháº§n Cá»T LÃ•I, bá» qua chi tiáº¿t khÃ´ng cáº§n thiáº¿t
            priority_keywords = ['phÃ­', 'lá»‡ phÃ­', 'miá»…n', 'tiá»n', 'giáº¥y tá»', 'há»“ sÆ¡', 'thá»§ tá»¥c']
            
            for chunk in content_chunks:
                section_title = chunk.get('section_title', '')
                content = chunk.get('content', '')
                
                # Æ¯u tiÃªn cÃ¡c section vá» phÃ­, giáº¥y tá» cáº§n thiáº¿t
                if any(keyword in section_title.lower() for keyword in priority_keywords) or \
                   any(keyword in content.lower() for keyword in priority_keywords):
                    
                    essential_parts.append(f"\nðŸ“„ {section_title}:")
                    essential_parts.append("-" * 40)
                    
                    # RÃºt gá»n content, chá»‰ giá»¯ thÃ´ng tin quan trá»ng
                    if len(content) > 500:
                        # TÃ¡ch thÃ nh cÃ¢u vÃ  chá»‰ giá»¯ nhá»¯ng cÃ¢u cÃ³ tá»« khÃ³a quan trá»ng
                        sentences = content.split('.')
                        important_sentences = []
                        
                        for sentence in sentences:
                            if any(keyword in sentence.lower() for keyword in priority_keywords):
                                important_sentences.append(sentence.strip())
                                
                        if important_sentences:
                            essential_parts.append('\n'.join(important_sentences[:3]))  # Top 3 sentences
                        else:
                            essential_parts.append(content[:500] + "...")
                    else:
                        essential_parts.append(content.strip())
            
            # Táº¡o final content - NGáº®N Gá»ŒN vÃ  TRá»ŒNG TÃ‚M
            final_content = "\n".join(essential_parts)
            
            # Giá»›i háº¡n Ä‘á»™ dÃ i tá»‘i Ä‘a 2000 chars Ä‘á»ƒ LLM khÃ´ng bá»‹ overwhelmed
            if len(final_content) > 2000:
                final_content = final_content[:2000] + "\n\n[...Ná»™i dung Ä‘Æ°á»£c rÃºt gá»n Ä‘á»ƒ táº­p trung vÃ o thÃ´ng tin quan trá»ng...]"
            
            logger.info(f"Loaded selective document: {len(final_content)} characters (optimized for LLM focus)")
            
            return final_content
            
        except Exception as e:
            logger.error(f"Error loading selective document {file_path}: {e}")
            return ""
    
    def _get_all_chunks_from_document(self, source_file: str) -> List[Dict[str, Any]]:
        """Láº¥y táº¥t cáº£ chunks tá»« má»™t document"""
        document_chunks = []
        
        for chunk_id, metadata in self.document_metadata_cache.items():
            if metadata["source_file"] == source_file:
                # Láº¥y chunk content tá»« vector database
                try:
                    collection = self.vectordb_service.get_collection(metadata["collection"])
                    result = collection.get(ids=[chunk_id])
                    
                    if result["documents"]:
                        document_chunks.append({
                            "id": chunk_id,
                            "content": result["documents"][0],
                            "metadata": metadata,
                            "chunk_index": metadata["chunk_index"]
                        })
                        
                except Exception as e:
                    logger.warning(f"Could not retrieve chunk {chunk_id}: {e}")
        
        # Sáº¯p xáº¿p theo chunk_index
        document_chunks.sort(key=lambda x: x["chunk_index"])
        
        return document_chunks
    
    def _get_surrounding_chunks(self, source_file: str, nucleus_chunks: List[Dict[str, Any]], window_size: int = 2) -> List[Dict[str, Any]]:
        """Láº¥y cÃ¡c chunks xung quanh nucleus chunks"""
        # TÃ¬m nucleus chunk indices trong document nÃ y
        nucleus_indices = set()
        for nucleus_chunk in nucleus_chunks:
            chunk_id = nucleus_chunk.get("id", "")
            if chunk_id in self.document_metadata_cache:
                metadata = self.document_metadata_cache[chunk_id]
                if metadata["source_file"] == source_file:
                    nucleus_indices.add(metadata["chunk_index"])
        
        if not nucleus_indices:
            return []
        
        # XÃ¡c Ä‘á»‹nh range Ä‘á»ƒ láº¥y surrounding chunks
        min_index = min(nucleus_indices) - window_size
        max_index = max(nucleus_indices) + window_size
        
        # Láº¥y chunks trong range
        surrounding_chunks = []
        for chunk_id, metadata in self.document_metadata_cache.items():
            if metadata["source_file"] == source_file:
                chunk_idx = metadata["chunk_index"]
                if min_index <= chunk_idx <= max_index:
                    try:
                        collection = self.vectordb_service.get_collection(metadata["collection"])
                        result = collection.get(ids=[chunk_id])
                        
                        if result["documents"]:
                            surrounding_chunks.append({
                                "id": chunk_id,
                                "content": result["documents"][0],
                                "metadata": metadata,
                                "chunk_index": chunk_idx
                            })
                            
                    except Exception as e:
                        logger.warning(f"Could not retrieve chunk {chunk_id}: {e}")
        
        # Sáº¯p xáº¿p theo chunk_index
        surrounding_chunks.sort(key=lambda x: x["chunk_index"])
        
        return surrounding_chunks
    
    def _merge_document_chunks(self, chunks: List[Dict[str, Any]], source_file: str) -> Dict[str, Any]:
        """Merge cÃ¡c chunks thÃ nh má»™t document context"""
        if not chunks:
            return {}
        
        merged_text = "\n\n".join([chunk["content"] for chunk in chunks])
        
        return {
            "text": merged_text,
            "source": source_file,
            "chunk_count": len(chunks),
            "chunk_indices": [chunk["chunk_index"] for chunk in chunks],
            "total_chars": len(merged_text)
        }
    
    def get_document_summary(self, source_file: str) -> Dict[str, Any]:
        """Láº¥y thÃ´ng tin tÃ³m táº¯t vá» má»™t document"""
        chunks = self._get_all_chunks_from_document(source_file)
        
        if not chunks:
            return {"error": f"No chunks found for {source_file}"}
        
        return {
            "source_file": source_file,
            "total_chunks": len(chunks),
            "total_length": sum(len(chunk["content"]) for chunk in chunks),
            "chunk_indices": [chunk["chunk_index"] for chunk in chunks],
            "collections": list(set(self.document_metadata_cache[chunk["id"]]["collection"] for chunk in chunks))
        }
    
    def rebuild_metadata_cache(self):
        """Rebuild metadata cache (sau khi cÃ³ documents má»›i)"""
        self.document_metadata_cache.clear()
        self._build_document_metadata_cache()
        
    def get_stats(self) -> Dict[str, Any]:
        """Thá»‘ng kÃª context expansion service"""
        source_files = set()
        collections = set()
        
        for metadata in self.document_metadata_cache.values():
            source_files.add(metadata["source_file"])
            collections.add(metadata["collection"])
        
        return {
            "total_chunks": len(self.document_metadata_cache),
            "total_documents": len(source_files),
            "total_collections": len(collections),
            "documents": list(source_files),
            "collections": list(collections)
        }
