"""
Enhanced Context Expansion Service
S·ª≠ d·ª•ng "Nucleus Chunk" strategy ƒë·ªÉ m·ªü r·ªông ng·ªØ c·∫£nh hi·ªáu qu·∫£
"""

import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Set
import json

logger = logging.getLogger(__name__)

class EnhancedContextExpansionService:
    """Service m·ªü r·ªông ng·ªØ c·∫£nh v·ªõi Nucleus Chunk strategy"""
    
    def __init__(self, vectordb_service, documents_dir: str):
        self.vectordb_service = vectordb_service
        self.documents_dir = Path(documents_dir)
        
        # Cache metadata c·ªßa documents
        self.document_metadata_cache = {}
        self._build_document_metadata_cache()
    
    def _build_document_metadata_cache(self):
        """X√¢y d·ª±ng cache metadata ƒë·ªÉ map chunk -> document"""
        try:
            # L·∫•y t·∫•t c·∫£ collections
            collections = self.vectordb_service.list_collections()
            
            for collection_info in collections:
                collection_name = collection_info["name"]
                collection = self.vectordb_service.get_collection(collection_name)
                
                # L·∫•y t·∫•t c·∫£ documents trong collection
                try:
                    results = collection.get()
                    
                    for i, chunk_id in enumerate(results["ids"]):
                        metadata = results.get("metadatas", [{}])[i] if i < len(results.get("metadatas", [])) else {}
                        
                        if metadata and "source" in metadata:
                            source_file = metadata["source"]
                            
                            # Build cache entry
                            self.document_metadata_cache[chunk_id] = {
                                "source_file": source_file,
                                "collection": collection_name,
                                "metadata": metadata,
                                "chunk_index": metadata.get("chunk_index", 0)
                            }
                            
                except Exception as e:
                    logger.warning(f"Could not process collection {collection_name}: {e}")
                    
            logger.info(f"Built metadata cache for {len(self.document_metadata_cache)} chunks")
            
        except Exception as e:
            logger.error(f"Error building document metadata cache: {e}")
    
    def expand_context_with_nucleus(
        self,
        nucleus_chunks: List[Dict[str, Any]], 
        max_context_length: int = 3000,
        include_full_document: bool = True
    ) -> Dict[str, Any]:
        """
        M·ªü r·ªông ng·ªØ c·∫£nh d·ª±a tr√™n nucleus chunks - STRATEGY: 1 CHUNK ‚Üí TO√ÄN B·ªò DOCUMENT
        
        Flow t·ªëi ∆∞u:
        1. L·∫•y 1 nucleus chunk v·ªõi rerank score cao nh·∫•t
        2. T√¨m source file JSON ch·ª©a chunk ƒë√≥  
        3. Load to√†n b·ªô n·ªôi dung document t·ª´ file JSON g·ªëc
        4. Return full document content thay v√¨ ch·ªâ 1 chunk
        
        Args:
            nucleus_chunks: List chunks ƒë√£ rerank (th∆∞·ªùng ch·ªâ 1 chunk cao nh·∫•t)
            max_context_length: ƒê·ªô d√†i context t·ªëi ƒëa (k√Ω t·ª±)
            include_full_document: True = l·∫•y to√†n b·ªô document, False = ch·ªâ chunks li·ªÅn k·ªÅ
            
        Returns:
            Expanded context v·ªõi to√†n b·ªô document content v√† metadata
        """
        try:
            expanded_context = {
                "nucleus_chunks": nucleus_chunks,
                "expanded_content": [],
                "source_documents": [],
                "total_length": 0,
                "expansion_strategy": "single_nucleus_full_document"
            }
            
            # CH·ªà X·ª¨ L√ù 1 NUCLEUS CHUNK ƒê·∫¶U TI√äN (chunk c√≥ rerank score cao nh·∫•t)
            if not nucleus_chunks:
                logger.warning("No nucleus chunks provided")
                return expanded_context
                
            nucleus_chunk = nucleus_chunks[0]  # L·∫•y chunk cao nh·∫•t sau rerank
            logger.info(f"Processing nucleus chunk with ID: {nucleus_chunk.get('id', 'N/A')}")
            
            # T√¨m source file JSON t·ª´ nucleus chunk metadata
            source_file = None
            
            # Th·ª≠ nhi·ªÅu c√°ch ƒë·ªÉ t√¨m source file
            if "source" in nucleus_chunk and "file_path" in nucleus_chunk["source"]:
                source_file = nucleus_chunk["source"]["file_path"]
            elif "metadata" in nucleus_chunk and "source" in nucleus_chunk["metadata"]:
                source_file = nucleus_chunk["metadata"]["source"].get("file_path")
            
            if not source_file:
                logger.warning("Could not find source file for nucleus chunk")
                return expanded_context
                
            logger.info(f"Found source file: {source_file}")
            logger.info("Loading FULL DOCUMENT content (not just chunks)")
            
            # QUAN TR·ªåNG: Load to√†n b·ªô document g·ªëc t·ª´ file JSON thay v√¨ ch·ªâ l·∫•y chunks
            full_document_content = self._load_full_document(source_file)
            
            if full_document_content:
                # Gi·ªõi h·∫°n context length
                if len(full_document_content) > max_context_length:
                    # Truncate nh∆∞ng gi·ªØ ph·∫ßn ƒë·∫ßu v√† th√¥ng tin quan tr·ªçng
                    full_document_content = full_document_content[:max_context_length] + "..."
                
                expanded_context["expanded_content"] = [{
                    "text": full_document_content,
                    "source": source_file,
                    "document_title": nucleus_chunk.get("source", {}).get("document_title", ""),
                    "type": "full_document"
                }]
                expanded_context["source_documents"] = [source_file]
                expanded_context["total_length"] = len(full_document_content)
                
                logger.info(f"Expanded context: {len(full_document_content)} chars from 1 document")
            else:
                logger.warning("Could not load full document content")
            
            return expanded_context
            
        except Exception as e:
            logger.error(f"Error in context expansion: {e}")
            # Fallback: return nucleus chunks as-is
            return {
                "nucleus_chunks": nucleus_chunks,
                "expanded_content": [{"text": chunk.get("content", ""), "source": "fallback", "type": "chunk_fallback"} for chunk in nucleus_chunks],
                "source_documents": [],
                "total_length": sum(len(chunk.get("content", "")) for chunk in nucleus_chunks),
                "expansion_strategy": "fallback"
            }
    
    def _load_full_document(self, file_path: str) -> str:
        """
        Load n·ªôi dung document c√≥ ch·ªçn l·ªçc theo c√¢u h·ªèi ƒë·ªÉ tr√°nh overload LLM
        STRATEGY: Thay v√¨ load to√†n b·ªô document, ch·ªâ load nh·ªØng ph·∫ßn li√™n quan
        """
        try:
            import json
            from pathlib import Path
            
            if not Path(file_path).exists():
                logger.warning(f"Source file not found: {file_path}")
                return ""
                
            logger.info(f"Loading selective document content from: {file_path}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            # Build selective document content - ∆ØU TI√äN TH√îNG TIN QUAN TR·ªåNG
            metadata = json_data.get('metadata', {})
            content_chunks = json_data.get('content_chunks', [])
            
            # T·∫°o content v·ªõi th√¥ng tin T√ìM T·∫ÆT v√† TR·ªåNG T√ÇM
            essential_parts = []
            
            # HEADER - Th√¥ng tin c·ªët l√µi
            if metadata.get('title'):
                essential_parts.append(f"üìã TI√äU ƒê·ªÄ: {metadata['title']}")
            
            if metadata.get('executing_agency'):
                essential_parts.append(f"üè¢ C∆† QUAN TH·ª∞C HI·ªÜN: {metadata['executing_agency']}")
                
            if metadata.get('processing_time_text'):
                essential_parts.append(f"‚è∞ TH·ªúI GIAN X·ª¨ L√ù: {metadata['processing_time_text']}")
            
            # QUAN TR·ªåNG NH·∫§T: Th√¥ng tin v·ªÅ PH√ç/L·ªÜ PH√ç ƒë∆∞·ª£c ∆∞u ti√™n h√†ng ƒë·∫ßu
            if metadata.get('fee_text'):
                essential_parts.append(f"üí∞ TH√îNG TIN PH√ç/L·ªÜ PH√ç:")
                essential_parts.append(f"   {metadata['fee_text']}")
                
            essential_parts.append("=" * 60)
            
            # CONTENT CHUNKS - Ch·ªâ l·∫•y nh·ªØng ph·∫ßn C·ªêT L√ïI, b·ªè qua chi ti·∫øt kh√¥ng c·∫ßn thi·∫øt
            priority_keywords = ['ph√≠', 'l·ªá ph√≠', 'mi·ªÖn', 'ti·ªÅn', 'gi·∫•y t·ªù', 'h·ªì s∆°', 'th·ªß t·ª•c']
            
            for chunk in content_chunks:
                section_title = chunk.get('section_title', '')
                content = chunk.get('content', '')
                
                # ∆Øu ti√™n c√°c section v·ªÅ ph√≠, gi·∫•y t·ªù c·∫ßn thi·∫øt
                if any(keyword in section_title.lower() for keyword in priority_keywords) or \
                   any(keyword in content.lower() for keyword in priority_keywords):
                    
                    essential_parts.append(f"\nüìÑ {section_title}:")
                    essential_parts.append("-" * 40)
                    
                    # R√∫t g·ªçn content, ch·ªâ gi·ªØ th√¥ng tin quan tr·ªçng
                    if len(content) > 500:
                        # T√°ch th√†nh c√¢u v√† ch·ªâ gi·ªØ nh·ªØng c√¢u c√≥ t·ª´ kh√≥a quan tr·ªçng
                        sentences = content.split('.')
                        important_sentences = []
                        
                        for sentence in sentences:
                            if any(keyword in sentence.lower() for keyword in priority_keywords):
                                important_sentences.append(sentence.strip())
                                
                        if important_sentences:
                            essential_parts.append('\n'.join(important_sentences[:3]))  # Top 3 sentences
                        else:
                            essential_parts.append(content[:500] + "...")
                    else:
                        essential_parts.append(content.strip())
            
            # T·∫°o final content - NG·∫ÆN G·ªåN v√† TR·ªåNG T√ÇM
            final_content = "\n".join(essential_parts)
            
            # Gi·ªõi h·∫°n ƒë·ªô d√†i t·ªëi ƒëa 2000 chars ƒë·ªÉ LLM kh√¥ng b·ªã overwhelmed
            if len(final_content) > 2000:
                final_content = final_content[:2000] + "\n\n[...N·ªôi dung ƒë∆∞·ª£c r√∫t g·ªçn ƒë·ªÉ t·∫≠p trung v√†o th√¥ng tin quan tr·ªçng...]"
            
            logger.info(f"Loaded selective document: {len(final_content)} characters (optimized for LLM focus)")
            
            return final_content
            
        except Exception as e:
            logger.error(f"Error loading selective document {file_path}: {e}")
            return ""
    
    def _get_all_chunks_from_document(self, source_file: str) -> List[Dict[str, Any]]:
        """L·∫•y t·∫•t c·∫£ chunks t·ª´ m·ªôt document"""
        document_chunks = []
        
        for chunk_id, metadata in self.document_metadata_cache.items():
            if metadata["source_file"] == source_file:
                # L·∫•y chunk content t·ª´ vector database
                try:
                    collection = self.vectordb_service.get_collection(metadata["collection"])
                    result = collection.get(ids=[chunk_id])
                    
                    if result["documents"]:
                        document_chunks.append({
                            "id": chunk_id,
                            "content": result["documents"][0],
                            "metadata": metadata,
                            "chunk_index": metadata["chunk_index"]
                        })
                        
                except Exception as e:
                    logger.warning(f"Could not retrieve chunk {chunk_id}: {e}")
        
        # S·∫Øp x·∫øp theo chunk_index
        document_chunks.sort(key=lambda x: x["chunk_index"])
        
        return document_chunks
    
    def _get_surrounding_chunks(self, source_file: str, nucleus_chunks: List[Dict[str, Any]], window_size: int = 2) -> List[Dict[str, Any]]:
        """L·∫•y c√°c chunks xung quanh nucleus chunks"""
        # T√¨m nucleus chunk indices trong document n√†y
        nucleus_indices = set()
        for nucleus_chunk in nucleus_chunks:
            chunk_id = nucleus_chunk.get("id", "")
            if chunk_id in self.document_metadata_cache:
                metadata = self.document_metadata_cache[chunk_id]
                if metadata["source_file"] == source_file:
                    nucleus_indices.add(metadata["chunk_index"])
        
        if not nucleus_indices:
            return []
        
        # X√°c ƒë·ªãnh range ƒë·ªÉ l·∫•y surrounding chunks
        min_index = min(nucleus_indices) - window_size
        max_index = max(nucleus_indices) + window_size
        
        # L·∫•y chunks trong range
        surrounding_chunks = []
        for chunk_id, metadata in self.document_metadata_cache.items():
            if metadata["source_file"] == source_file:
                chunk_idx = metadata["chunk_index"]
                if min_index <= chunk_idx <= max_index:
                    try:
                        collection = self.vectordb_service.get_collection(metadata["collection"])
                        result = collection.get(ids=[chunk_id])
                        
                        if result["documents"]:
                            surrounding_chunks.append({
                                "id": chunk_id,
                                "content": result["documents"][0],
                                "metadata": metadata,
                                "chunk_index": chunk_idx
                            })
                            
                    except Exception as e:
                        logger.warning(f"Could not retrieve chunk {chunk_id}: {e}")
        
        # S·∫Øp x·∫øp theo chunk_index
        surrounding_chunks.sort(key=lambda x: x["chunk_index"])
        
        return surrounding_chunks
    
    def _merge_document_chunks(self, chunks: List[Dict[str, Any]], source_file: str) -> Dict[str, Any]:
        """Merge c√°c chunks th√†nh m·ªôt document context"""
        if not chunks:
            return {}
        
        merged_text = "\n\n".join([chunk["content"] for chunk in chunks])
        
        return {
            "text": merged_text,
            "source": source_file,
            "chunk_count": len(chunks),
            "chunk_indices": [chunk["chunk_index"] for chunk in chunks],
            "total_chars": len(merged_text)
        }
    
    def get_document_summary(self, source_file: str) -> Dict[str, Any]:
        """L·∫•y th√¥ng tin t√≥m t·∫Øt v·ªÅ m·ªôt document"""
        chunks = self._get_all_chunks_from_document(source_file)
        
        if not chunks:
            return {"error": f"No chunks found for {source_file}"}
        
        return {
            "source_file": source_file,
            "total_chunks": len(chunks),
            "total_length": sum(len(chunk["content"]) for chunk in chunks),
            "chunk_indices": [chunk["chunk_index"] for chunk in chunks],
            "collections": list(set(self.document_metadata_cache[chunk["id"]]["collection"] for chunk in chunks))
        }
    
    def rebuild_metadata_cache(self):
        """Rebuild metadata cache (sau khi c√≥ documents m·ªõi)"""
        self.document_metadata_cache.clear()
        self._build_document_metadata_cache()
        
    def get_stats(self) -> Dict[str, Any]:
        """Th·ªëng k√™ context expansion service"""
        source_files = set()
        collections = set()
        
        for metadata in self.document_metadata_cache.values():
            source_files.add(metadata["source_file"])
            collections.add(metadata["collection"])
        
        return {
            "total_chunks": len(self.document_metadata_cache),
            "total_documents": len(source_files),
            "total_collections": len(collections),
            "documents": list(source_files),
            "collections": list(collections)
        }
