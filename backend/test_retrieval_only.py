"""
Test chá»‰ vá»›i vector search + rerank, khÃ´ng cÃ³ LLM generation
"""

import os
import sys
import time
from pathlib import Path

# Add backend path
backend_path = Path(__file__).parent
sys.path.append(str(backend_path))

def test_without_llm_generation():
    """Test chá»‰ tá»›i bÆ°á»›c rerank, khÃ´ng generate answer"""
    
    try:
        print("ğŸš« TESTING WITHOUT LLM GENERATION")
        print("=" * 50)
        
        from app.core.config import settings
        from app.services.vector_database import VectorDBService
        from app.services.result_reranker import RerankerService
        
        print(f"âœ… Config - BROAD_SEARCH_K: {settings.broad_search_k}")
        
        # Initialize services
        print("ğŸ”§ Initializing services...")
        start_init = time.time()
        
        vectordb_service = VectorDBService()
        reranker_service = RerankerService()
        
        init_time = time.time() - start_init
        print(f"âœ… Services initialized: {init_time:.2f}s")
        
        # Test full retrieval pipeline without LLM
        query = "Ä‘Äƒng kÃ½ khai sinh cÃ³ tá»‘n phÃ­ khÃ´ng"
        
        print(f"\nğŸ¯ RETRIEVAL PIPELINE: {query}")
        print("-" * 40)
        
        start_query = time.time()
        
        # Step 1: Vector search
        print("ğŸ” Step 1: Vector search...")
        search_start = time.time()
        
        search_results = vectordb_service.search_in_collection(
            collection_name="ho_tich_cap_xa",
            query=query,
            top_k=settings.broad_search_k,
            similarity_threshold=settings.similarity_threshold
        )
        
        search_time = time.time() - search_start
        print(f"   â±ï¸ Vector search: {search_time:.2f}s ({len(search_results)} docs)")
        
        # Step 2: Reranking
        print("ğŸ”¥ Step 2: Reranking...")
        rerank_start = time.time()
        
        reranked_results = reranker_service.rerank_documents(
            query=query,
            documents=search_results,
            top_k=1
        )
        
        rerank_time = time.time() - rerank_start
        print(f"   â±ï¸ Reranking: {rerank_time:.2f}s")
        
        total_retrieval_time = time.time() - start_query
        
        print(f"\nğŸ“Š RETRIEVAL RESULTS:")
        print(f"  â±ï¸ Total retrieval time: {total_retrieval_time:.2f}s")
        print(f"  ğŸ“Š Search: {search_time:.2f}s")
        print(f"  ğŸ”¥ Rerank: {rerank_time:.2f}s")
        print(f"  ğŸ“„ Best result score: {reranked_results[0].get('rerank_score', 0):.4f}")
        print(f"  ğŸ“ Best content length: {len(reranked_results[0].get('content', ''))} chars")
        
        # Performance comparison
        print(f"\nğŸ” ANALYSIS:")
        print(f"  Retrieval only: {total_retrieval_time:.2f}s")
        print(f"  Full RAG pipeline: ~18.16s")
        print(f"  LLM + Context overhead: ~{18.16 - total_retrieval_time:.2f}s")
        
        if total_retrieval_time < 3:
            print(f"  âœ… Retrieval is fast - problem is in LLM/Context stage")
        else:
            print(f"  âš ï¸ Retrieval is also slow")
            
        return total_retrieval_time
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    timing = test_without_llm_generation()
    
    if timing:
        print(f"\nğŸ¯ DIAGNOSIS:")
        if timing < 2:
            print(f"âœ… Retrieval pipeline is optimized")
            print(f"ğŸ” Focus on optimizing:")
            print(f"   - Context building")
            print(f"   - LLM prompt construction") 
            print(f"   - LLM generation with long context")
        else:
            print(f"âš ï¸ Retrieval pipeline needs optimization too")
