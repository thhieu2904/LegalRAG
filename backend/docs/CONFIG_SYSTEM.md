# LegalRAG Configuration System

## ğŸ“‹ Tá»•ng quan

Há»‡ thá»‘ng cáº¥u hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c **tÃ¡ch biá»‡t hoÃ n toÃ n khá»i logic code**, sá»­ dá»¥ng pattern Node.js vá»›i file `.env` Ä‘á»ƒ quáº£n lÃ½ táº¥t cáº£ settings.

## ğŸ¯ Cáº¥u hÃ¬nh cuá»‘i cÃ¹ng Ä‘Æ°á»£c xÃ¡c nháº­n

### Models & Performance

- **Embedding Model**: `AITeamVN/Vietnamese_Embedding_v2` (1024 dimensions)
- **Reranker Model**: `AITeamVN/Vietnamese_Reranker` (CrossEncoder)
- **LLM Model**: `PhoGPT-4B-Chat-q4_k_m.gguf` (4-bit quantized)
- **Context Length**: `8192` tokens (~6000 Vietnamese words)
- **CPU Threads**: `6` threads (optimized for Intel 12700H - 6P cores)
- **Temperature**: `0.1` (maximum accuracy, minimal creativity for legal text)

### RAG Process Configuration

- **Broad Search K**: `30` documents (initial retrieval)
- **Similarity Threshold**: `0.3` (permissive for recall)
- **Context Expansion**: `1` adjacent chunk
- **Reranking**: `Enabled` (Vietnamese-specific reranking)
- **Query Routing**: `Enabled` (intelligent collection selection)

### All Models Local

- âœ… Models cached in `data/models/hf_cache/`
- âœ… Offline mode enabled (`HF_HUB_OFFLINE=1`)
- âœ… No internet downloads during runtime
- âœ… Local model detection working

## ğŸ”§ Usage Examples

### Before (Hard-coded)

```python
# âŒ Hard-coded values scattered throughout code
vectordb.search_in_collection("collection", "query", top_k=5, similarity_threshold=0.3)
llm.generate_response(query, context, temperature=0.7, max_tokens=2048)
router.route_query(query, top_k=2)
```

### After (Config-based)

```python
# âœ… Clean code using config defaults
vectordb.search_in_collection("collection", "query")  # Uses config defaults
llm.generate_response(query, context)  # Uses config defaults
router.route_query(query)  # Uses config defaults

# âœ… Override when needed
vectordb.search_in_collection("collection", "query", top_k=10)  # Override specific param
```

## ğŸ“ Configuration Management

### Single Source of Truth

```bash
# File: .env
TEMPERATURE=0.1                    # Used in: LLMService.generate_response()
DEFAULT_SEARCH_TOP_K=5             # Used in: VectorDBService.search_in_collection()
QUERY_ROUTER_TOP_K=2               # Used in: QueryRouter.route_query()
EMBEDDING_MODEL_NAME=AITeamVN/Vietnamese_Embedding_v2  # Used in: VectorDBService
```

### Verified Usage

- âœ… All 30+ config values have explicit usage comments
- âœ… All services tested and confirmed using config values
- âœ… No orphaned/unused config values
- âœ… No hardcoded magic numbers remaining in code

## ğŸ¯ Benefits Achieved

### 1. Easy Maintenance (nhÆ° Node.js)

```bash
# Change behavior by editing .env only
TEMPERATURE=0.2        # More creative responses
DEFAULT_SEARCH_TOP_K=10  # More search results
N_THREADS=8            # Use more CPU cores
```

### 2. Environment-based Configuration

```bash
# Development
DEBUG=true
TEMPERATURE=0.1

# Production
DEBUG=false
TEMPERATURE=0.05
```

### 3. No Code Changes Required

- Tune performance: Edit `.env` â†’ Restart
- Change models: Edit `.env` â†’ Restart
- Adjust search parameters: Edit `.env` â†’ Restart

## ğŸ” Verification Status

### âœ… All Tests Passed

- **Config Loading**: All 30+ values loaded correctly
- **Service Integration**: All services use config defaults
- **Path Resolution**: All file paths computed correctly
- **Model Loading**: All models load from local cache
- **Logic Separation**: Zero hardcoded values in business logic

### âœ… Architecture Benefits

- ğŸ¯ Single source of truth (`.env` file)
- ğŸ”§ Easy maintenance and tuning
- ğŸ“¦ Environment-based deployment
- ğŸš€ No code changes for config adjustments
- ğŸ’¡ Developer-friendly like Node.js projects

## ğŸ“ Next Steps

**System is production-ready!**

To adjust any behavior:

1. Edit `.env` file
2. Restart application
3. Changes take effect immediately

**No more digging through code files to change settings!** ğŸ‰
