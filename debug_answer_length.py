#!/usr/bin/env python3
"""
DEBUG ANSWER LENGTH: 0 ISSUE - DEEP INVESTIGATION
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

from app.services.language_model import LLMService
from app.core.config import settings
import json

def debug_llm_generation():
    """Debug tr·ª±c ti·∫øp LLM generation"""
    
    print("üîç DEEP DEBUG: LLM GENERATION ISSUE")
    print("=" * 60)
    
    # T·∫°o m·ªôt LLMService instance
    llm_service = LLMService()
    
    # Test v·ªõi prompt ƒë∆°n gi·∫£n
    simple_context = "Th√¥ng tin: ƒêƒÉng k√Ω k·∫øt h√¥n mi·ªÖn l·ªá ph√≠."
    simple_query = "L·ªá ph√≠ l√† bao nhi√™u?"
    simple_system = "B·∫°n l√† tr·ª£ l√Ω ph√°p lu·∫≠t. Tr·∫£ l·ªùi ng·∫Øn g·ªçn."
    
    print("üìù Testing with simple inputs:")
    print(f"   Context: {simple_context}")
    print(f"   Query: {simple_query}")
    print(f"   System: {simple_system}")
    
    # T·∫°o prompt
    formatted_prompt = llm_service._format_prompt(
        system_prompt=simple_system,
        user_query=simple_query,
        context=simple_context,
        chat_history=None
    )
    
    print(f"\nüìã Generated ChatML prompt:")
    print("-" * 40)
    print(formatted_prompt)
    print("-" * 40)
    
    # Ph√¢n t√≠ch prompt
    prompt_length = len(formatted_prompt)
    estimated_tokens = prompt_length // 3
    
    print(f"\nüìä Prompt Analysis:")
    print(f"   Length: {prompt_length} chars")
    print(f"   Estimated tokens: {estimated_tokens}")
    print(f"   Context window: {settings.n_ctx}")
    print(f"   Max tokens: {settings.max_tokens}")
    
    # T√≠nh to√°n space
    safety_buffer = 256
    available_space = settings.n_ctx - estimated_tokens - safety_buffer
    MINIMUM_RESPONSE_TOKENS = 64
    
    print(f"   Available space: {available_space}")
    print(f"   Minimum required: {MINIMUM_RESPONSE_TOKENS}")
    
    if available_space <= MINIMUM_RESPONSE_TOKENS:
        print(f"   üö® INSUFFICIENT SPACE - would return default message")
        return
    
    dynamic_max_tokens = min(settings.max_tokens, available_space)
    if dynamic_max_tokens < MINIMUM_RESPONSE_TOKENS:
        dynamic_max_tokens = MINIMUM_RESPONSE_TOKENS
    
    print(f"   Dynamic max tokens: {dynamic_max_tokens}")
    
    # Ki·ªÉm tra model c√≥ load kh√¥ng
    print(f"\nü§ñ Model Status:")
    print(f"   Model loaded: {llm_service.is_model_loaded()}")
    print(f"   Model path exists: {llm_service.model_path.exists()}")
    
    if not llm_service.model_path.exists():
        print(f"   ‚ùå MODEL FILE NOT FOUND!")
        print(f"   Path: {llm_service.model_path}")
        return
    
    # Test generate n·∫øu c√≥ th·ªÉ
    try:
        print(f"\nüß™ Testing LLM generation...")
        print(f"   Loading model (if not loaded)...")
        
        # Ensure model is loaded
        llm_service.ensure_loaded()
        
        if not llm_service.model:
            print(f"   ‚ùå MODEL FAILED TO LOAD!")
            return
            
        print(f"   ‚úÖ Model loaded successfully")
        
        # Test v·ªõi parameters ƒë∆°n gi·∫£n
        test_response = llm_service.model(
            formatted_prompt,
            max_tokens=100,  # S·ªë nh·ªè ƒë·ªÉ test
            temperature=0.1,
            stop=["<|im_end|>"],
            echo=False,
            stream=False
        )
        
        print(f"\nüìã Raw model response:")
        print(f"   Type: {type(test_response)}")
        
        if isinstance(test_response, dict):
            print(f"   Keys: {list(test_response.keys())}")
            
            if 'choices' in test_response:
                choices = test_response['choices']
                print(f"   Choices count: {len(choices)}")
                
                if len(choices) > 0:
                    first_choice = choices[0]
                    text = first_choice.get('text', '')
                    print(f"   Generated text length: {len(text)}")
                    print(f"   Generated text: '{text}'")
                    
                    if len(text.strip()) == 0:
                        print(f"   üö® EMPTY TEXT GENERATED!")
                        print(f"   Choice details: {first_choice}")
                else:
                    print(f"   ‚ùå NO CHOICES IN RESPONSE!")
            else:
                print(f"   ‚ùå NO 'choices' KEY IN RESPONSE!")
                print(f"   Response: {test_response}")
        else:
            print(f"   Response: {test_response}")
            
    except Exception as e:
        print(f"   ‚ùå Error during generation: {e}")
        import traceback
        traceback.print_exc()

def test_context_expansion_issue():
    """Test context expansion c√≥ th·ªÉ g√¢y v·∫•n ƒë·ªÅ"""
    
    print(f"\nüîç TESTING CONTEXT EXPANSION ISSUE")
    print("=" * 60)
    
    # Simple test v·ªõi context r·ªóng
    print("üìù Test 1: Empty context")
    llm_service = LLMService()
    
    empty_prompt = llm_service._format_prompt(
        system_prompt="Tr·∫£ l·ªùi ng·∫Øn g·ªçn.",
        user_query="Xin ch√†o",
        context="",
        chat_history=None
    )
    
    print(f"   Empty context prompt length: {len(empty_prompt)}")
    print(f"   Prompt:\n{empty_prompt}")
    
    # Test v·ªõi context c√≥ n·ªôi dung
    print(f"\nüìù Test 2: With context")
    
    with_context_prompt = llm_service._format_prompt(
        system_prompt="Tr·∫£ l·ªùi ng·∫Øn g·ªçn.",
        user_query="L·ªá ph√≠ l√† bao nhi√™u?",
        context="ƒêƒÉng k√Ω k·∫øt h√¥n mi·ªÖn l·ªá ph√≠.",
        chat_history=None
    )
    
    print(f"   With context prompt length: {len(with_context_prompt)}")
    print(f"   Prompt:\n{with_context_prompt}")

def main():
    """Ch·∫°y debug to√†n di·ªán"""
    
    print("üöÄ COMPREHENSIVE DEBUG: ANSWER LENGTH: 0 ISSUE")
    print("=" * 70)
    
    try:
        debug_llm_generation()
        test_context_expansion_issue()
    except Exception as e:
        print(f"‚ùå Debug failed: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\n" + "=" * 70)
    print("üéØ DEBUG COMPLETED")
    print("Check the output above for clues about the Answer length: 0 issue")

if __name__ == "__main__":
    main()
