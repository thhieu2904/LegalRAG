#!/usr/bin/env python3
"""
Test Enhanced Prompt for Multiple Questions Generation V2
==========================================================
"""

import sys
from pathlib import Path
import json
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

# Add app to Python path
sys.path.append(str(Path(__file__).parent.parent))

try:
    from tools.generate_router_with_llm_v2 import SmartRouterLLMGenerator
except ImportError as e:
    logger.error(f"‚ùå Cannot import SmartRouterLLMGenerator: {e}")
    sys.exit(1)

def test_enhanced_prompt():
    """Test the enhanced prompt with a sample document."""
    print("üß™ Testing Enhanced Prompt for Multiple Questions Generation V2")
    print("=" * 70)
    
    # Sample document for testing
    sample_doc = {
        "metadata": {
            "title": "ƒêƒÉng k√Ω khai sinh",
            "code": "DK_KS_001",
            "applicant_type": ["C√° nh√¢n", "T·ªï ch·ª©c"],
            "executing_agency": "UBND c·∫•p ph∆∞·ªùng/x√£",
            "processing_time_text": "Ngay khi nh·∫≠n ƒë·ªß h·ªì s∆° h·ª£p l·ªá",
            "fee_text": "Mi·ªÖn ph√≠",
            "submission_method": ["Tr·ª±c ti·∫øp", "Tr·ª±c tuy·∫øn"],
            "result_delivery": ["Tr·ª±c ti·∫øp", "B∆∞u ch√≠nh"]
        },
        "content_chunks": [
            {
                "section_title": "Th√†nh ph·∫ßn h·ªì s∆°",
                "content": "ƒê∆°n ƒëƒÉng k√Ω khai sinh theo m·∫´u, Gi·∫•y ch·ª©ng sinh ho·∫∑c gi·∫•y ra vi·ªán, Gi·∫•y t·ªù t√πy th√¢n c·ªßa cha m·∫π, Gi·∫•y ch·ª©ng nh·∫≠n k·∫øt h√¥n c·ªßa cha m·∫π..."
            }
        ]
    }
    
    try:
        # Initialize generator
        generator = SmartRouterLLMGenerator()
        
        # Generate summary for prompt
        document_summary = generator._summarize_document_for_prompt(sample_doc)
        
        # Create the actual prompt
        user_query = f"""NHI·ªÜM V·ª§: T·∫°o ch√≠nh x√°c 10 c√¢u h·ªèi v·ªÅ th·ªß t·ª•c "{sample_doc['metadata']['title']}"

TH√îNG TIN: {document_summary}

Y√äU C·∫¶U: T·∫°o 10 c√¢u h·ªèi ng·∫Øn g·ªçn, m·ªói c√¢u m·ªôt d√≤ng, ƒë√°nh s·ªë t·ª´ 1-10. M·ªói c√¢u ph·∫£i kh√°c nhau ho√†n to√†n.

V√ç D·ª§ FORMAT:
1. Th·ªß t·ª•c n√†y l√† g√¨?
2. Ai c√≥ th·ªÉ l√†m?
3. C·∫ßn gi·∫•y t·ªù g√¨?
4. Chi ph√≠ bao nhi√™u?
5. L√†m ·ªü ƒë√¢u?
6. M·∫•t bao l√¢u?
7. L√†m online ƒë∆∞·ª£c kh√¥ng?
8. Nh·∫≠n k·∫øt qu·∫£ nh∆∞ th·∫ø n√†o?
9. C√≥ ƒëi·ªÅu ki·ªán g√¨ ƒë·∫∑c bi·ªát?
10. L∆∞u √Ω g√¨ khi l√†m?

B·∫ÆT ƒê·∫¶U T·∫†O 10 C√ÇU H·ªéI:"""

        system_prompt = "T·∫°o ch√≠nh x√°c 10 c√¢u h·ªèi ng·∫Øn. ƒê√°nh s·ªë 1-10. Kh√¥ng gi·∫£i th√≠ch th√™m."
        
        print("üìù USER QUERY:")
        print(user_query)
        print("\nüß† SYSTEM PROMPT:")
        print(system_prompt)
        
        # Call LLM
        print(f"\nü§ñ LLM RESPONSE:")
        response_data = generator.llm_service.generate_response(
            user_query=user_query,
            max_tokens=500,
            temperature=0.3,
            system_prompt=system_prompt
        )
        
        response_text = response_data.get('response', '')
        print(response_text)
        
        # Extract questions
        print("\n" + "=" * 70)
        result = generator._extract_questions_from_llm_response(response_text, "ƒêƒÉng k√Ω khai sinh")
        
        if result:
            print("‚úÖ QUESTIONS EXTRACTED SUCCESSFULLY:")
            print(f"üìä Total questions: {1 + len(result.get('question_variants', []))}")
            print(f"üéØ Main question: {result.get('main_question', 'N/A')}")
            print(f"üî¢ Variants: {len(result.get('question_variants', []))}")
            
            print(f"\nüìã FULL RESULT:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        else:
            print("‚ùå FAILED TO EXTRACT QUESTIONS")
            
    except Exception as e:
        logger.error(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_enhanced_prompt()
